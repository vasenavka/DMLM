2024-09-28 18:22:48,811:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:22:48,812:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:22:48,812:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:22:48,812:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:24:48,300:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:24:48,300:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:24:48,300:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:24:48,300:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:39:10,528:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:39:10,529:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:39:10,529:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:39:10,529:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:40:51,568:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:40:51,568:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:40:51,568:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:40:51,568:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:41:13,571:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:41:13,571:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:41:13,571:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:41:13,571:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-28 18:41:15,525:INFO:PyCaret ClassificationExperiment
2024-09-28 18:41:15,525:INFO:Logging name: clf-default-name
2024-09-28 18:41:15,525:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-28 18:41:15,525:INFO:version 3.3.2
2024-09-28 18:41:15,525:INFO:Initializing setup()
2024-09-28 18:41:15,525:INFO:self.USI: 4399
2024-09-28 18:41:15,525:INFO:self._variable_keys: {'X', 'pipeline', 'y_train', 'y_test', 'seed', 'log_plots_param', 'gpu_n_jobs_param', 'y', 'fold_shuffle_param', 'exp_id', 'exp_name_log', 'USI', 'target_param', 'memory', 'logging_param', 'n_jobs_param', 'data', 'gpu_param', 'html_param', '_available_plots', 'X_test', 'X_train', 'fold_generator', 'is_multiclass', 'idx', 'fix_imbalance', 'fold_groups_param', '_ml_usecase'}
2024-09-28 18:41:15,525:INFO:Checking environment
2024-09-28 18:41:15,525:INFO:python_version: 3.11.6
2024-09-28 18:41:15,525:INFO:python_build: ('tags/v3.11.6:8b6ee5b', 'Oct  2 2023 14:57:12')
2024-09-28 18:41:15,525:INFO:machine: AMD64
2024-09-28 18:41:15,533:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-28 18:41:15,535:INFO:Memory: svmem(total=17113423872, available=5616402432, percent=67.2, used=11497021440, free=5616402432)
2024-09-28 18:41:15,535:INFO:Physical Core: 6
2024-09-28 18:41:15,535:INFO:Logical Core: 12
2024-09-28 18:41:15,535:INFO:Checking libraries
2024-09-28 18:41:15,535:INFO:System:
2024-09-28 18:41:15,535:INFO:    python: 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]
2024-09-28 18:41:15,535:INFO:executable: C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Scripts\python.exe
2024-09-28 18:41:15,535:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-28 18:41:15,535:INFO:PyCaret required dependencies:
2024-09-28 18:41:15,555:INFO:                 pip: 23.2.1
2024-09-28 18:41:15,555:INFO:          setuptools: 68.2.0
2024-09-28 18:41:15,555:INFO:             pycaret: 3.3.2
2024-09-28 18:41:15,555:INFO:             IPython: 8.27.0
2024-09-28 18:41:15,555:INFO:          ipywidgets: 8.1.5
2024-09-28 18:41:15,555:INFO:                tqdm: 4.66.5
2024-09-28 18:41:15,555:INFO:               numpy: 1.26.4
2024-09-28 18:41:15,555:INFO:              pandas: 2.1.4
2024-09-28 18:41:15,555:INFO:              jinja2: 3.1.4
2024-09-28 18:41:15,555:INFO:               scipy: 1.11.4
2024-09-28 18:41:15,555:INFO:              joblib: 1.3.2
2024-09-28 18:41:15,555:INFO:             sklearn: 1.4.2
2024-09-28 18:41:15,555:INFO:                pyod: 2.0.2
2024-09-28 18:41:15,555:INFO:            imblearn: 0.12.3
2024-09-28 18:41:15,555:INFO:   category_encoders: 2.6.3
2024-09-28 18:41:15,555:INFO:            lightgbm: 4.5.0
2024-09-28 18:41:15,555:INFO:               numba: 0.60.0
2024-09-28 18:41:15,555:INFO:            requests: 2.32.3
2024-09-28 18:41:15,555:INFO:          matplotlib: 3.7.5
2024-09-28 18:41:15,556:INFO:          scikitplot: 0.3.7
2024-09-28 18:41:15,556:INFO:         yellowbrick: 1.5
2024-09-28 18:41:15,556:INFO:              plotly: 5.24.1
2024-09-28 18:41:15,556:INFO:    plotly-resampler: Not installed
2024-09-28 18:41:15,556:INFO:             kaleido: 0.2.1
2024-09-28 18:41:15,556:INFO:           schemdraw: 0.15
2024-09-28 18:41:15,556:INFO:         statsmodels: 0.14.3
2024-09-28 18:41:15,556:INFO:              sktime: 0.26.0
2024-09-28 18:41:15,556:INFO:               tbats: 1.1.3
2024-09-28 18:41:15,556:INFO:            pmdarima: 2.0.4
2024-09-28 18:41:15,556:INFO:              psutil: 6.0.0
2024-09-28 18:41:15,556:INFO:          markupsafe: 2.1.5
2024-09-28 18:41:15,556:INFO:             pickle5: Not installed
2024-09-28 18:41:15,556:INFO:         cloudpickle: 3.0.0
2024-09-28 18:41:15,556:INFO:         deprecation: 2.1.0
2024-09-28 18:41:15,556:INFO:              xxhash: 3.5.0
2024-09-28 18:41:15,556:INFO:           wurlitzer: Not installed
2024-09-28 18:41:15,556:INFO:PyCaret optional dependencies:
2024-09-28 18:41:15,571:INFO:                shap: Not installed
2024-09-28 18:41:15,571:INFO:           interpret: Not installed
2024-09-28 18:41:15,571:INFO:                umap: Not installed
2024-09-28 18:41:15,571:INFO:     ydata_profiling: Not installed
2024-09-28 18:41:15,571:INFO:  explainerdashboard: Not installed
2024-09-28 18:41:15,571:INFO:             autoviz: Not installed
2024-09-28 18:41:15,571:INFO:           fairlearn: Not installed
2024-09-28 18:41:15,571:INFO:          deepchecks: Not installed
2024-09-28 18:41:15,571:INFO:             xgboost: Not installed
2024-09-28 18:41:15,571:INFO:            catboost: Not installed
2024-09-28 18:41:15,571:INFO:              kmodes: Not installed
2024-09-28 18:41:15,571:INFO:             mlxtend: Not installed
2024-09-28 18:41:15,571:INFO:       statsforecast: Not installed
2024-09-28 18:41:15,571:INFO:        tune_sklearn: Not installed
2024-09-28 18:41:15,571:INFO:                 ray: Not installed
2024-09-28 18:41:15,571:INFO:            hyperopt: Not installed
2024-09-28 18:41:15,572:INFO:              optuna: Not installed
2024-09-28 18:41:15,572:INFO:               skopt: Not installed
2024-09-28 18:41:15,572:INFO:              mlflow: 2.16.2
2024-09-28 18:41:15,572:INFO:              gradio: Not installed
2024-09-28 18:41:15,572:INFO:             fastapi: Not installed
2024-09-28 18:41:15,572:INFO:             uvicorn: Not installed
2024-09-28 18:41:15,572:INFO:              m2cgen: Not installed
2024-09-28 18:41:15,572:INFO:           evidently: Not installed
2024-09-28 18:41:15,572:INFO:               fugue: Not installed
2024-09-28 18:41:15,572:INFO:           streamlit: Not installed
2024-09-28 18:41:15,572:INFO:             prophet: Not installed
2024-09-28 18:41:15,572:INFO:None
2024-09-28 18:41:15,572:INFO:Set up data.
2024-09-28 18:41:15,780:INFO:Set up folding strategy.
2024-09-28 18:41:15,780:INFO:Set up train/test split.
2024-09-28 18:41:15,979:INFO:Set up index.
2024-09-28 18:41:15,983:INFO:Assigning column types.
2024-09-28 18:41:16,173:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-28 18:41:16,212:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-28 18:41:16,215:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-28 18:41:16,243:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,244:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,282:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-28 18:41:16,282:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-28 18:41:16,306:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,307:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,307:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-28 18:41:16,345:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-28 18:41:16,369:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,369:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,409:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-28 18:41:16,432:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,432:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,433:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-28 18:41:16,495:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,495:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,558:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,558:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:16,561:INFO:Preparing preprocessing pipeline...
2024-09-28 18:41:16,584:INFO:Set up simple imputation.
2024-09-28 18:41:17,034:INFO:Finished creating preprocessing pipeline.
2024-09-28 18:41:17,041:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-28 18:41:17,041:INFO:Creating final display dataframe.
2024-09-28 18:41:18,759:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape      (3156, 1001)
4        Transformed data shape      (3156, 1001)
5   Transformed train set shape      (2209, 1001)
6    Transformed test set shape       (947, 1001)
7              Numeric features              1000
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              4399
2024-09-28 18:41:18,824:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:18,824:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:18,887:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:18,888:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-28 18:41:18,889:INFO:setup() successfully completed in 3.37s...............
2024-09-28 18:41:18,889:INFO:Initializing compare_models()
2024-09-28 18:41:18,889:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-28 18:41:18,889:INFO:Checking exceptions
2024-09-28 18:41:18,995:INFO:Preparing display monitor
2024-09-28 18:41:18,998:INFO:Initializing Logistic Regression
2024-09-28 18:41:18,998:INFO:Total runtime is 0.0 minutes
2024-09-28 18:41:18,998:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:18,998:INFO:Initializing create_model()
2024-09-28 18:41:18,998:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:18,998:INFO:Checking exceptions
2024-09-28 18:41:18,998:INFO:Importing libraries
2024-09-28 18:41:18,998:INFO:Copying training dataset
2024-09-28 18:41:19,225:INFO:Defining folds
2024-09-28 18:41:19,226:INFO:Declaring metric variables
2024-09-28 18:41:19,226:INFO:Importing untrained model
2024-09-28 18:41:19,226:INFO:Logistic Regression Imported successfully
2024-09-28 18:41:19,226:INFO:Starting cross validation
2024-09-28 18:41:19,228:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:41:24,098:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,397:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,555:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,674:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,725:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,762:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,837:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,858:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,958:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,958:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:24,973:INFO:Calculating mean and std
2024-09-28 18:41:24,974:INFO:Creating metrics dataframe
2024-09-28 18:41:24,975:INFO:Uploading results into container
2024-09-28 18:41:24,976:INFO:Uploading model into container now
2024-09-28 18:41:24,976:INFO:_master_model_container: 1
2024-09-28 18:41:24,976:INFO:_display_container: 2
2024-09-28 18:41:24,976:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-28 18:41:24,976:INFO:create_model() successfully completed......................................
2024-09-28 18:41:25,045:INFO:SubProcess create_model() end ==================================
2024-09-28 18:41:25,046:INFO:Creating metrics dataframe
2024-09-28 18:41:25,047:INFO:Initializing K Neighbors Classifier
2024-09-28 18:41:25,048:INFO:Total runtime is 0.10083694458007812 minutes
2024-09-28 18:41:25,048:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:25,048:INFO:Initializing create_model()
2024-09-28 18:41:25,048:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:25,048:INFO:Checking exceptions
2024-09-28 18:41:25,048:INFO:Importing libraries
2024-09-28 18:41:25,048:INFO:Copying training dataset
2024-09-28 18:41:25,213:INFO:Defining folds
2024-09-28 18:41:25,214:INFO:Declaring metric variables
2024-09-28 18:41:25,214:INFO:Importing untrained model
2024-09-28 18:41:25,214:INFO:K Neighbors Classifier Imported successfully
2024-09-28 18:41:25,214:INFO:Starting cross validation
2024-09-28 18:41:25,216:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:41:26,939:INFO:Calculating mean and std
2024-09-28 18:41:26,940:INFO:Creating metrics dataframe
2024-09-28 18:41:26,941:INFO:Uploading results into container
2024-09-28 18:41:26,942:INFO:Uploading model into container now
2024-09-28 18:41:26,942:INFO:_master_model_container: 2
2024-09-28 18:41:26,942:INFO:_display_container: 2
2024-09-28 18:41:26,942:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-28 18:41:26,942:INFO:create_model() successfully completed......................................
2024-09-28 18:41:27,008:INFO:SubProcess create_model() end ==================================
2024-09-28 18:41:27,008:INFO:Creating metrics dataframe
2024-09-28 18:41:27,010:INFO:Initializing Naive Bayes
2024-09-28 18:41:27,010:INFO:Total runtime is 0.13353660106658935 minutes
2024-09-28 18:41:27,011:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:27,011:INFO:Initializing create_model()
2024-09-28 18:41:27,011:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:27,011:INFO:Checking exceptions
2024-09-28 18:41:27,011:INFO:Importing libraries
2024-09-28 18:41:27,011:INFO:Copying training dataset
2024-09-28 18:41:27,183:INFO:Defining folds
2024-09-28 18:41:27,183:INFO:Declaring metric variables
2024-09-28 18:41:27,184:INFO:Importing untrained model
2024-09-28 18:41:27,184:INFO:Naive Bayes Imported successfully
2024-09-28 18:41:27,184:INFO:Starting cross validation
2024-09-28 18:41:27,186:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:41:27,987:INFO:Calculating mean and std
2024-09-28 18:41:27,987:INFO:Creating metrics dataframe
2024-09-28 18:41:27,989:INFO:Uploading results into container
2024-09-28 18:41:27,989:INFO:Uploading model into container now
2024-09-28 18:41:27,989:INFO:_master_model_container: 3
2024-09-28 18:41:27,989:INFO:_display_container: 2
2024-09-28 18:41:27,990:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-28 18:41:27,990:INFO:create_model() successfully completed......................................
2024-09-28 18:41:28,051:INFO:SubProcess create_model() end ==================================
2024-09-28 18:41:28,051:INFO:Creating metrics dataframe
2024-09-28 18:41:28,053:INFO:Initializing Decision Tree Classifier
2024-09-28 18:41:28,053:INFO:Total runtime is 0.150912078221639 minutes
2024-09-28 18:41:28,054:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:28,054:INFO:Initializing create_model()
2024-09-28 18:41:28,054:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:28,054:INFO:Checking exceptions
2024-09-28 18:41:28,054:INFO:Importing libraries
2024-09-28 18:41:28,054:INFO:Copying training dataset
2024-09-28 18:41:28,225:INFO:Defining folds
2024-09-28 18:41:28,225:INFO:Declaring metric variables
2024-09-28 18:41:28,225:INFO:Importing untrained model
2024-09-28 18:41:28,225:INFO:Decision Tree Classifier Imported successfully
2024-09-28 18:41:28,225:INFO:Starting cross validation
2024-09-28 18:41:28,227:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:41:29,536:INFO:Calculating mean and std
2024-09-28 18:41:29,536:INFO:Creating metrics dataframe
2024-09-28 18:41:29,538:INFO:Uploading results into container
2024-09-28 18:41:29,538:INFO:Uploading model into container now
2024-09-28 18:41:29,538:INFO:_master_model_container: 4
2024-09-28 18:41:29,538:INFO:_display_container: 2
2024-09-28 18:41:29,539:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-28 18:41:29,539:INFO:create_model() successfully completed......................................
2024-09-28 18:41:29,600:INFO:SubProcess create_model() end ==================================
2024-09-28 18:41:29,600:INFO:Creating metrics dataframe
2024-09-28 18:41:29,602:INFO:Initializing SVM - Linear Kernel
2024-09-28 18:41:29,602:INFO:Total runtime is 0.1767350435256958 minutes
2024-09-28 18:41:29,602:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:29,602:INFO:Initializing create_model()
2024-09-28 18:41:29,602:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:29,602:INFO:Checking exceptions
2024-09-28 18:41:29,602:INFO:Importing libraries
2024-09-28 18:41:29,603:INFO:Copying training dataset
2024-09-28 18:41:29,772:INFO:Defining folds
2024-09-28 18:41:29,772:INFO:Declaring metric variables
2024-09-28 18:41:29,772:INFO:Importing untrained model
2024-09-28 18:41:29,772:INFO:SVM - Linear Kernel Imported successfully
2024-09-28 18:41:29,772:INFO:Starting cross validation
2024-09-28 18:41:29,775:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:41:30,309:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,345:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,395:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,413:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,613:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,644:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,672:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,681:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,753:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,838:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:30,855:INFO:Calculating mean and std
2024-09-28 18:41:30,855:INFO:Creating metrics dataframe
2024-09-28 18:41:30,857:INFO:Uploading results into container
2024-09-28 18:41:30,857:INFO:Uploading model into container now
2024-09-28 18:41:30,857:INFO:_master_model_container: 5
2024-09-28 18:41:30,857:INFO:_display_container: 2
2024-09-28 18:41:30,858:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-28 18:41:30,858:INFO:create_model() successfully completed......................................
2024-09-28 18:41:30,919:INFO:SubProcess create_model() end ==================================
2024-09-28 18:41:30,919:INFO:Creating metrics dataframe
2024-09-28 18:41:30,921:INFO:Initializing Ridge Classifier
2024-09-28 18:41:30,921:INFO:Total runtime is 0.19872477054595947 minutes
2024-09-28 18:41:30,921:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:30,921:INFO:Initializing create_model()
2024-09-28 18:41:30,921:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:30,922:INFO:Checking exceptions
2024-09-28 18:41:30,922:INFO:Importing libraries
2024-09-28 18:41:30,922:INFO:Copying training dataset
2024-09-28 18:41:31,101:INFO:Defining folds
2024-09-28 18:41:31,101:INFO:Declaring metric variables
2024-09-28 18:41:31,101:INFO:Importing untrained model
2024-09-28 18:41:31,102:INFO:Ridge Classifier Imported successfully
2024-09-28 18:41:31,102:INFO:Starting cross validation
2024-09-28 18:41:31,106:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:41:31,444:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:31,533:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:31,575:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:31,724:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:31,724:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:31,832:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:31,868:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:31,878:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:31,955:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:32,010:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:32,027:INFO:Calculating mean and std
2024-09-28 18:41:32,027:INFO:Creating metrics dataframe
2024-09-28 18:41:32,029:INFO:Uploading results into container
2024-09-28 18:41:32,029:INFO:Uploading model into container now
2024-09-28 18:41:32,029:INFO:_master_model_container: 6
2024-09-28 18:41:32,029:INFO:_display_container: 2
2024-09-28 18:41:32,029:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-28 18:41:32,030:INFO:create_model() successfully completed......................................
2024-09-28 18:41:32,088:INFO:SubProcess create_model() end ==================================
2024-09-28 18:41:32,088:INFO:Creating metrics dataframe
2024-09-28 18:41:32,090:INFO:Initializing Random Forest Classifier
2024-09-28 18:41:32,090:INFO:Total runtime is 0.21820346117019654 minutes
2024-09-28 18:41:32,090:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:32,091:INFO:Initializing create_model()
2024-09-28 18:41:32,091:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:32,091:INFO:Checking exceptions
2024-09-28 18:41:32,091:INFO:Importing libraries
2024-09-28 18:41:32,091:INFO:Copying training dataset
2024-09-28 18:41:32,257:INFO:Defining folds
2024-09-28 18:41:32,257:INFO:Declaring metric variables
2024-09-28 18:41:32,257:INFO:Importing untrained model
2024-09-28 18:41:32,257:INFO:Random Forest Classifier Imported successfully
2024-09-28 18:41:32,258:INFO:Starting cross validation
2024-09-28 18:41:32,260:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:41:34,600:INFO:Calculating mean and std
2024-09-28 18:41:34,600:INFO:Creating metrics dataframe
2024-09-28 18:41:34,602:INFO:Uploading results into container
2024-09-28 18:41:34,602:INFO:Uploading model into container now
2024-09-28 18:41:34,602:INFO:_master_model_container: 7
2024-09-28 18:41:34,602:INFO:_display_container: 2
2024-09-28 18:41:34,603:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-28 18:41:34,603:INFO:create_model() successfully completed......................................
2024-09-28 18:41:34,664:INFO:SubProcess create_model() end ==================================
2024-09-28 18:41:34,664:INFO:Creating metrics dataframe
2024-09-28 18:41:34,666:INFO:Initializing Quadratic Discriminant Analysis
2024-09-28 18:41:34,666:INFO:Total runtime is 0.26114283402760824 minutes
2024-09-28 18:41:34,667:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:34,667:INFO:Initializing create_model()
2024-09-28 18:41:34,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:34,667:INFO:Checking exceptions
2024-09-28 18:41:34,667:INFO:Importing libraries
2024-09-28 18:41:34,667:INFO:Copying training dataset
2024-09-28 18:41:34,835:INFO:Defining folds
2024-09-28 18:41:34,835:INFO:Declaring metric variables
2024-09-28 18:41:34,835:INFO:Importing untrained model
2024-09-28 18:41:34,835:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-28 18:41:34,835:INFO:Starting cross validation
2024-09-28 18:41:34,837:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:41:35,207:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:35,297:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:35,357:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:35,460:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:35,596:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:35,756:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:35,971:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:35,994:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:36,105:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:36,152:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,253:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,358:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,387:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-28 18:41:36,484:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,504:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,645:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,713:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,721:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,773:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,851:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:36,862:INFO:Calculating mean and std
2024-09-28 18:41:36,862:INFO:Creating metrics dataframe
2024-09-28 18:41:36,864:INFO:Uploading results into container
2024-09-28 18:41:36,865:INFO:Uploading model into container now
2024-09-28 18:41:36,865:INFO:_master_model_container: 8
2024-09-28 18:41:36,865:INFO:_display_container: 2
2024-09-28 18:41:36,865:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-28 18:41:36,865:INFO:create_model() successfully completed......................................
2024-09-28 18:41:36,926:INFO:SubProcess create_model() end ==================================
2024-09-28 18:41:36,926:INFO:Creating metrics dataframe
2024-09-28 18:41:36,928:INFO:Initializing Ada Boost Classifier
2024-09-28 18:41:36,929:INFO:Total runtime is 0.29883952538172404 minutes
2024-09-28 18:41:36,929:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:36,929:INFO:Initializing create_model()
2024-09-28 18:41:36,929:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:36,929:INFO:Checking exceptions
2024-09-28 18:41:36,929:INFO:Importing libraries
2024-09-28 18:41:36,929:INFO:Copying training dataset
2024-09-28 18:41:37,097:INFO:Defining folds
2024-09-28 18:41:37,097:INFO:Declaring metric variables
2024-09-28 18:41:37,097:INFO:Importing untrained model
2024-09-28 18:41:37,097:INFO:Ada Boost Classifier Imported successfully
2024-09-28 18:41:37,098:INFO:Starting cross validation
2024-09-28 18:41:37,100:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:41:37,324:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:37,375:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:37,506:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:37,561:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:37,643:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:37,711:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:37,769:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:37,829:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:37,896:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:37,904:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-28 18:41:39,407:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,488:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,541:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,680:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,686:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,792:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,794:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,804:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,863:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,876:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:41:39,886:INFO:Calculating mean and std
2024-09-28 18:41:39,886:INFO:Creating metrics dataframe
2024-09-28 18:41:39,888:INFO:Uploading results into container
2024-09-28 18:41:39,888:INFO:Uploading model into container now
2024-09-28 18:41:39,888:INFO:_master_model_container: 9
2024-09-28 18:41:39,888:INFO:_display_container: 2
2024-09-28 18:41:39,888:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-28 18:41:39,888:INFO:create_model() successfully completed......................................
2024-09-28 18:41:39,950:INFO:SubProcess create_model() end ==================================
2024-09-28 18:41:39,950:INFO:Creating metrics dataframe
2024-09-28 18:41:39,952:INFO:Initializing Gradient Boosting Classifier
2024-09-28 18:41:39,952:INFO:Total runtime is 0.3492313981056213 minutes
2024-09-28 18:41:39,952:INFO:SubProcess create_model() called ==================================
2024-09-28 18:41:39,952:INFO:Initializing create_model()
2024-09-28 18:41:39,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:41:39,952:INFO:Checking exceptions
2024-09-28 18:41:39,952:INFO:Importing libraries
2024-09-28 18:41:39,952:INFO:Copying training dataset
2024-09-28 18:41:40,117:INFO:Defining folds
2024-09-28 18:41:40,117:INFO:Declaring metric variables
2024-09-28 18:41:40,117:INFO:Importing untrained model
2024-09-28 18:41:40,117:INFO:Gradient Boosting Classifier Imported successfully
2024-09-28 18:41:40,117:INFO:Starting cross validation
2024-09-28 18:41:40,120:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:42:18,154:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,177:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,220:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,420:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,483:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,598:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,615:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,630:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,669:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,811:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:18,821:INFO:Calculating mean and std
2024-09-28 18:42:18,821:INFO:Creating metrics dataframe
2024-09-28 18:42:18,823:INFO:Uploading results into container
2024-09-28 18:42:18,823:INFO:Uploading model into container now
2024-09-28 18:42:18,823:INFO:_master_model_container: 10
2024-09-28 18:42:18,824:INFO:_display_container: 2
2024-09-28 18:42:18,824:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-28 18:42:18,824:INFO:create_model() successfully completed......................................
2024-09-28 18:42:18,890:INFO:SubProcess create_model() end ==================================
2024-09-28 18:42:18,890:INFO:Creating metrics dataframe
2024-09-28 18:42:18,892:INFO:Initializing Linear Discriminant Analysis
2024-09-28 18:42:18,892:INFO:Total runtime is 0.998230238755544 minutes
2024-09-28 18:42:18,892:INFO:SubProcess create_model() called ==================================
2024-09-28 18:42:18,893:INFO:Initializing create_model()
2024-09-28 18:42:18,893:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:42:18,893:INFO:Checking exceptions
2024-09-28 18:42:18,893:INFO:Importing libraries
2024-09-28 18:42:18,893:INFO:Copying training dataset
2024-09-28 18:42:19,073:INFO:Defining folds
2024-09-28 18:42:19,073:INFO:Declaring metric variables
2024-09-28 18:42:19,073:INFO:Importing untrained model
2024-09-28 18:42:19,074:INFO:Linear Discriminant Analysis Imported successfully
2024-09-28 18:42:19,074:INFO:Starting cross validation
2024-09-28 18:42:19,076:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:42:20,763:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:21,351:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:21,431:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:21,612:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:21,684:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:21,825:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:21,890:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:21,914:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:21,929:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:21,996:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-28 18:42:22,012:INFO:Calculating mean and std
2024-09-28 18:42:22,012:INFO:Creating metrics dataframe
2024-09-28 18:42:22,014:INFO:Uploading results into container
2024-09-28 18:42:22,014:INFO:Uploading model into container now
2024-09-28 18:42:22,015:INFO:_master_model_container: 11
2024-09-28 18:42:22,015:INFO:_display_container: 2
2024-09-28 18:42:22,015:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-28 18:42:22,015:INFO:create_model() successfully completed......................................
2024-09-28 18:42:22,084:INFO:SubProcess create_model() end ==================================
2024-09-28 18:42:22,084:INFO:Creating metrics dataframe
2024-09-28 18:42:22,086:INFO:Initializing Extra Trees Classifier
2024-09-28 18:42:22,086:INFO:Total runtime is 1.0514630635579427 minutes
2024-09-28 18:42:22,086:INFO:SubProcess create_model() called ==================================
2024-09-28 18:42:22,086:INFO:Initializing create_model()
2024-09-28 18:42:22,087:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:42:22,087:INFO:Checking exceptions
2024-09-28 18:42:22,087:INFO:Importing libraries
2024-09-28 18:42:22,087:INFO:Copying training dataset
2024-09-28 18:42:22,269:INFO:Defining folds
2024-09-28 18:42:22,270:INFO:Declaring metric variables
2024-09-28 18:42:22,270:INFO:Importing untrained model
2024-09-28 18:42:22,270:INFO:Extra Trees Classifier Imported successfully
2024-09-28 18:42:22,270:INFO:Starting cross validation
2024-09-28 18:42:22,272:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:42:25,122:INFO:Calculating mean and std
2024-09-28 18:42:25,122:INFO:Creating metrics dataframe
2024-09-28 18:42:25,124:INFO:Uploading results into container
2024-09-28 18:42:25,124:INFO:Uploading model into container now
2024-09-28 18:42:25,125:INFO:_master_model_container: 12
2024-09-28 18:42:25,125:INFO:_display_container: 2
2024-09-28 18:42:25,125:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-28 18:42:25,125:INFO:create_model() successfully completed......................................
2024-09-28 18:42:25,199:INFO:SubProcess create_model() end ==================================
2024-09-28 18:42:25,199:INFO:Creating metrics dataframe
2024-09-28 18:42:25,202:INFO:Initializing Light Gradient Boosting Machine
2024-09-28 18:42:25,202:INFO:Total runtime is 1.103399109840393 minutes
2024-09-28 18:42:25,202:INFO:SubProcess create_model() called ==================================
2024-09-28 18:42:25,202:INFO:Initializing create_model()
2024-09-28 18:42:25,202:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:42:25,202:INFO:Checking exceptions
2024-09-28 18:42:25,202:INFO:Importing libraries
2024-09-28 18:42:25,202:INFO:Copying training dataset
2024-09-28 18:42:25,379:INFO:Defining folds
2024-09-28 18:42:25,379:INFO:Declaring metric variables
2024-09-28 18:42:25,380:INFO:Importing untrained model
2024-09-28 18:42:25,380:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-28 18:42:25,380:INFO:Starting cross validation
2024-09-28 18:42:25,382:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:42:54,007:INFO:Calculating mean and std
2024-09-28 18:42:54,008:INFO:Creating metrics dataframe
2024-09-28 18:42:54,010:INFO:Uploading results into container
2024-09-28 18:42:54,010:INFO:Uploading model into container now
2024-09-28 18:42:54,010:INFO:_master_model_container: 13
2024-09-28 18:42:54,010:INFO:_display_container: 2
2024-09-28 18:42:54,011:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-28 18:42:54,011:INFO:create_model() successfully completed......................................
2024-09-28 18:42:54,097:INFO:SubProcess create_model() end ==================================
2024-09-28 18:42:54,097:INFO:Creating metrics dataframe
2024-09-28 18:42:54,099:INFO:Initializing Dummy Classifier
2024-09-28 18:42:54,099:INFO:Total runtime is 1.5850149432818095 minutes
2024-09-28 18:42:54,100:INFO:SubProcess create_model() called ==================================
2024-09-28 18:42:54,100:INFO:Initializing create_model()
2024-09-28 18:42:54,100:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000178E0571E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:42:54,100:INFO:Checking exceptions
2024-09-28 18:42:54,100:INFO:Importing libraries
2024-09-28 18:42:54,100:INFO:Copying training dataset
2024-09-28 18:42:54,279:INFO:Defining folds
2024-09-28 18:42:54,279:INFO:Declaring metric variables
2024-09-28 18:42:54,280:INFO:Importing untrained model
2024-09-28 18:42:54,280:INFO:Dummy Classifier Imported successfully
2024-09-28 18:42:54,280:INFO:Starting cross validation
2024-09-28 18:42:54,282:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-28 18:42:54,648:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:54,681:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:54,809:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:54,879:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:54,962:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:55,050:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:55,106:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:55,159:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:55,208:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:55,213:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-28 18:42:55,223:INFO:Calculating mean and std
2024-09-28 18:42:55,223:INFO:Creating metrics dataframe
2024-09-28 18:42:55,225:INFO:Uploading results into container
2024-09-28 18:42:55,225:INFO:Uploading model into container now
2024-09-28 18:42:55,225:INFO:_master_model_container: 14
2024-09-28 18:42:55,225:INFO:_display_container: 2
2024-09-28 18:42:55,226:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-28 18:42:55,226:INFO:create_model() successfully completed......................................
2024-09-28 18:42:55,293:INFO:SubProcess create_model() end ==================================
2024-09-28 18:42:55,293:INFO:Creating metrics dataframe
2024-09-28 18:42:55,295:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning:

Styler.applymap has been deprecated. Use Styler.map instead.


2024-09-28 18:42:55,296:INFO:Initializing create_model()
2024-09-28 18:42:55,297:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:42:55,297:INFO:Checking exceptions
2024-09-28 18:42:55,297:INFO:Importing libraries
2024-09-28 18:42:55,297:INFO:Copying training dataset
2024-09-28 18:42:55,470:INFO:Defining folds
2024-09-28 18:42:55,470:INFO:Declaring metric variables
2024-09-28 18:42:55,470:INFO:Importing untrained model
2024-09-28 18:42:55,470:INFO:Declaring custom model
2024-09-28 18:42:55,471:INFO:Extra Trees Classifier Imported successfully
2024-09-28 18:42:55,473:INFO:Cross validation set to False
2024-09-28 18:42:55,474:INFO:Fitting Model
2024-09-28 18:42:55,842:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-28 18:42:55,843:INFO:create_model() successfully completed......................................
2024-09-28 18:42:55,924:INFO:_master_model_container: 14
2024-09-28 18:42:55,924:INFO:_display_container: 2
2024-09-28 18:42:55,925:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-28 18:42:55,925:INFO:compare_models() successfully completed......................................
2024-09-28 18:42:55,925:INFO:Initializing finalize_model()
2024-09-28 18:42:55,925:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-28 18:42:55,925:INFO:Finalizing ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-28 18:42:56,064:INFO:Initializing create_model()
2024-09-28 18:42:56,064:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-28 18:42:56,064:INFO:Checking exceptions
2024-09-28 18:42:56,065:INFO:Importing libraries
2024-09-28 18:42:56,065:INFO:Copying training dataset
2024-09-28 18:42:56,081:INFO:Defining folds
2024-09-28 18:42:56,081:INFO:Declaring metric variables
2024-09-28 18:42:56,081:INFO:Importing untrained model
2024-09-28 18:42:56,081:INFO:Declaring custom model
2024-09-28 18:42:56,082:INFO:Extra Trees Classifier Imported successfully
2024-09-28 18:42:56,084:INFO:Cross validation set to False
2024-09-28 18:42:56,085:INFO:Fitting Model
2024-09-28 18:42:56,603:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-28 18:42:56,603:INFO:create_model() successfully completed......................................
2024-09-28 18:42:56,671:INFO:_master_model_container: 14
2024-09-28 18:42:56,671:INFO:_display_container: 2
2024-09-28 18:42:56,679:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-28 18:42:56,679:INFO:finalize_model() successfully completed......................................
2024-09-28 18:42:56,755:INFO:Initializing predict_model()
2024-09-28 18:42:56,755:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000178A67B47C0>)
2024-09-28 18:42:56,756:INFO:Checking exceptions
2024-09-28 18:42:56,756:INFO:Preloading libraries
2024-09-28 18:42:56,756:INFO:Set up data.
2024-09-28 18:42:56,913:INFO:Set up index.
2024-09-28 18:42:57,107:INFO:Initializing plot_model()
2024-09-28 18:42:57,107:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000178DF1AB950>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-28 18:42:57,107:INFO:Checking exceptions
2024-09-28 18:42:57,200:INFO:Preloading libraries
2024-09-28 18:42:57,225:INFO:Copying training dataset
2024-09-28 18:42:57,226:INFO:Plot type: feature
2024-09-28 18:42:57,226:WARNING:No coef_ found. Trying feature_importances_
2024-09-28 18:42:58,220:INFO:Visual Rendered Successfully
2024-09-28 18:42:58,289:INFO:plot_model() successfully completed......................................
2024-09-29 15:07:49,045:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:07:49,045:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:07:49,045:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:07:49,045:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:08:12,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:08:12,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:08:12,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:08:12,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:08:14,607:INFO:PyCaret ClassificationExperiment
2024-09-29 15:08:14,607:INFO:Logging name: clf-default-name
2024-09-29 15:08:14,607:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-29 15:08:14,607:INFO:version 3.3.2
2024-09-29 15:08:14,607:INFO:Initializing setup()
2024-09-29 15:08:14,607:INFO:self.USI: 72f8
2024-09-29 15:08:14,607:INFO:self._variable_keys: {'target_param', 'fold_shuffle_param', 'idx', 'gpu_n_jobs_param', 'X_train', 'exp_id', 'exp_name_log', 'html_param', 'X_test', 'pipeline', 'y_train', '_ml_usecase', 'seed', 'X', 'fold_generator', 'logging_param', 'y', 'fix_imbalance', 'log_plots_param', 'y_test', '_available_plots', 'fold_groups_param', 'n_jobs_param', 'gpu_param', 'is_multiclass', 'memory', 'data', 'USI'}
2024-09-29 15:08:14,607:INFO:Checking environment
2024-09-29 15:08:14,607:INFO:python_version: 3.11.6
2024-09-29 15:08:14,607:INFO:python_build: ('tags/v3.11.6:8b6ee5b', 'Oct  2 2023 14:57:12')
2024-09-29 15:08:14,607:INFO:machine: AMD64
2024-09-29 15:08:14,614:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-29 15:08:14,616:INFO:Memory: svmem(total=17113423872, available=9739882496, percent=43.1, used=7373541376, free=9739882496)
2024-09-29 15:08:14,616:INFO:Physical Core: 6
2024-09-29 15:08:14,616:INFO:Logical Core: 12
2024-09-29 15:08:14,616:INFO:Checking libraries
2024-09-29 15:08:14,616:INFO:System:
2024-09-29 15:08:14,616:INFO:    python: 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]
2024-09-29 15:08:14,616:INFO:executable: C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Scripts\python.exe
2024-09-29 15:08:14,617:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-29 15:08:14,617:INFO:PyCaret required dependencies:
2024-09-29 15:08:14,677:INFO:                 pip: 23.2.1
2024-09-29 15:08:14,677:INFO:          setuptools: 68.2.0
2024-09-29 15:08:14,677:INFO:             pycaret: 3.3.2
2024-09-29 15:08:14,677:INFO:             IPython: 8.27.0
2024-09-29 15:08:14,677:INFO:          ipywidgets: 8.1.5
2024-09-29 15:08:14,677:INFO:                tqdm: 4.66.5
2024-09-29 15:08:14,677:INFO:               numpy: 1.26.4
2024-09-29 15:08:14,677:INFO:              pandas: 2.1.4
2024-09-29 15:08:14,677:INFO:              jinja2: 3.1.4
2024-09-29 15:08:14,677:INFO:               scipy: 1.11.4
2024-09-29 15:08:14,677:INFO:              joblib: 1.3.2
2024-09-29 15:08:14,677:INFO:             sklearn: 1.4.2
2024-09-29 15:08:14,677:INFO:                pyod: 2.0.2
2024-09-29 15:08:14,677:INFO:            imblearn: 0.12.3
2024-09-29 15:08:14,677:INFO:   category_encoders: 2.6.3
2024-09-29 15:08:14,677:INFO:            lightgbm: 4.5.0
2024-09-29 15:08:14,677:INFO:               numba: 0.60.0
2024-09-29 15:08:14,677:INFO:            requests: 2.32.3
2024-09-29 15:08:14,677:INFO:          matplotlib: 3.7.5
2024-09-29 15:08:14,677:INFO:          scikitplot: 0.3.7
2024-09-29 15:08:14,677:INFO:         yellowbrick: 1.5
2024-09-29 15:08:14,677:INFO:              plotly: 5.24.1
2024-09-29 15:08:14,678:INFO:    plotly-resampler: Not installed
2024-09-29 15:08:14,678:INFO:             kaleido: 0.2.1
2024-09-29 15:08:14,678:INFO:           schemdraw: 0.15
2024-09-29 15:08:14,678:INFO:         statsmodels: 0.14.3
2024-09-29 15:08:14,678:INFO:              sktime: 0.26.0
2024-09-29 15:08:14,678:INFO:               tbats: 1.1.3
2024-09-29 15:08:14,678:INFO:            pmdarima: 2.0.4
2024-09-29 15:08:14,678:INFO:              psutil: 6.0.0
2024-09-29 15:08:14,678:INFO:          markupsafe: 2.1.5
2024-09-29 15:08:14,678:INFO:             pickle5: Not installed
2024-09-29 15:08:14,678:INFO:         cloudpickle: 3.0.0
2024-09-29 15:08:14,678:INFO:         deprecation: 2.1.0
2024-09-29 15:08:14,678:INFO:              xxhash: 3.5.0
2024-09-29 15:08:14,678:INFO:           wurlitzer: Not installed
2024-09-29 15:08:14,678:INFO:PyCaret optional dependencies:
2024-09-29 15:08:14,693:INFO:                shap: Not installed
2024-09-29 15:08:14,693:INFO:           interpret: Not installed
2024-09-29 15:08:14,693:INFO:                umap: Not installed
2024-09-29 15:08:14,693:INFO:     ydata_profiling: Not installed
2024-09-29 15:08:14,693:INFO:  explainerdashboard: Not installed
2024-09-29 15:08:14,693:INFO:             autoviz: Not installed
2024-09-29 15:08:14,693:INFO:           fairlearn: Not installed
2024-09-29 15:08:14,694:INFO:          deepchecks: Not installed
2024-09-29 15:08:14,694:INFO:             xgboost: Not installed
2024-09-29 15:08:14,694:INFO:            catboost: Not installed
2024-09-29 15:08:14,694:INFO:              kmodes: Not installed
2024-09-29 15:08:14,694:INFO:             mlxtend: Not installed
2024-09-29 15:08:14,694:INFO:       statsforecast: Not installed
2024-09-29 15:08:14,694:INFO:        tune_sklearn: Not installed
2024-09-29 15:08:14,694:INFO:                 ray: Not installed
2024-09-29 15:08:14,694:INFO:            hyperopt: Not installed
2024-09-29 15:08:14,694:INFO:              optuna: Not installed
2024-09-29 15:08:14,694:INFO:               skopt: Not installed
2024-09-29 15:08:14,694:INFO:              mlflow: 2.16.2
2024-09-29 15:08:14,694:INFO:              gradio: Not installed
2024-09-29 15:08:14,694:INFO:             fastapi: Not installed
2024-09-29 15:08:14,694:INFO:             uvicorn: Not installed
2024-09-29 15:08:14,694:INFO:              m2cgen: Not installed
2024-09-29 15:08:14,694:INFO:           evidently: Not installed
2024-09-29 15:08:14,694:INFO:               fugue: Not installed
2024-09-29 15:08:14,694:INFO:           streamlit: Not installed
2024-09-29 15:08:14,694:INFO:             prophet: Not installed
2024-09-29 15:08:14,694:INFO:None
2024-09-29 15:08:14,694:INFO:Set up data.
2024-09-29 15:08:14,902:INFO:Set up folding strategy.
2024-09-29 15:08:14,902:INFO:Set up train/test split.
2024-09-29 15:08:15,098:INFO:Set up index.
2024-09-29 15:08:15,102:INFO:Assigning column types.
2024-09-29 15:08:15,284:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-29 15:08:15,324:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-29 15:08:15,327:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:08:15,359:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,359:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,398:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-29 15:08:15,399:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:08:15,423:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,423:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,424:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-29 15:08:15,465:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:08:15,490:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,490:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,530:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:08:15,554:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,554:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,554:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-29 15:08:15,618:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,618:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,683:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,683:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:15,684:INFO:Preparing preprocessing pipeline...
2024-09-29 15:08:15,710:INFO:Set up simple imputation.
2024-09-29 15:08:16,166:INFO:Finished creating preprocessing pipeline.
2024-09-29 15:08:16,174:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-29 15:08:16,174:INFO:Creating final display dataframe.
2024-09-29 15:08:17,217:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape      (3156, 1001)
4        Transformed data shape      (3156, 1001)
5   Transformed train set shape      (2209, 1001)
6    Transformed test set shape       (947, 1001)
7              Numeric features              1000
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              72f8
2024-09-29 15:08:17,284:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:17,284:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:17,348:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:17,348:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:08:17,349:INFO:setup() successfully completed in 2.75s...............
2024-09-29 15:08:17,349:INFO:Initializing compare_models()
2024-09-29 15:08:17,349:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-29 15:08:17,349:INFO:Checking exceptions
2024-09-29 15:08:17,523:INFO:Preparing display monitor
2024-09-29 15:08:17,525:INFO:Initializing Logistic Regression
2024-09-29 15:08:17,526:INFO:Total runtime is 1.6649564107259113e-05 minutes
2024-09-29 15:08:17,526:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:17,526:INFO:Initializing create_model()
2024-09-29 15:08:17,526:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:17,526:INFO:Checking exceptions
2024-09-29 15:08:17,526:INFO:Importing libraries
2024-09-29 15:08:17,526:INFO:Copying training dataset
2024-09-29 15:08:17,692:INFO:Defining folds
2024-09-29 15:08:17,692:INFO:Declaring metric variables
2024-09-29 15:08:17,692:INFO:Importing untrained model
2024-09-29 15:08:17,693:INFO:Logistic Regression Imported successfully
2024-09-29 15:08:17,693:INFO:Starting cross validation
2024-09-29 15:08:17,695:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:08:22,527:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:22,747:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:22,826:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:22,994:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:23,003:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:23,037:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:23,070:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:23,133:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:23,220:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:23,241:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:23,251:INFO:Calculating mean and std
2024-09-29 15:08:23,252:INFO:Creating metrics dataframe
2024-09-29 15:08:23,253:INFO:Uploading results into container
2024-09-29 15:08:23,254:INFO:Uploading model into container now
2024-09-29 15:08:23,254:INFO:_master_model_container: 1
2024-09-29 15:08:23,254:INFO:_display_container: 2
2024-09-29 15:08:23,255:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-29 15:08:23,255:INFO:create_model() successfully completed......................................
2024-09-29 15:08:23,327:INFO:SubProcess create_model() end ==================================
2024-09-29 15:08:23,327:INFO:Creating metrics dataframe
2024-09-29 15:08:23,329:INFO:Initializing K Neighbors Classifier
2024-09-29 15:08:23,329:INFO:Total runtime is 0.09673652251561482 minutes
2024-09-29 15:08:23,329:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:23,329:INFO:Initializing create_model()
2024-09-29 15:08:23,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:23,329:INFO:Checking exceptions
2024-09-29 15:08:23,329:INFO:Importing libraries
2024-09-29 15:08:23,330:INFO:Copying training dataset
2024-09-29 15:08:23,493:INFO:Defining folds
2024-09-29 15:08:23,493:INFO:Declaring metric variables
2024-09-29 15:08:23,494:INFO:Importing untrained model
2024-09-29 15:08:23,494:INFO:K Neighbors Classifier Imported successfully
2024-09-29 15:08:23,494:INFO:Starting cross validation
2024-09-29 15:08:23,496:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:08:25,264:INFO:Calculating mean and std
2024-09-29 15:08:25,264:INFO:Creating metrics dataframe
2024-09-29 15:08:25,266:INFO:Uploading results into container
2024-09-29 15:08:25,266:INFO:Uploading model into container now
2024-09-29 15:08:25,267:INFO:_master_model_container: 2
2024-09-29 15:08:25,267:INFO:_display_container: 2
2024-09-29 15:08:25,267:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-29 15:08:25,267:INFO:create_model() successfully completed......................................
2024-09-29 15:08:25,332:INFO:SubProcess create_model() end ==================================
2024-09-29 15:08:25,332:INFO:Creating metrics dataframe
2024-09-29 15:08:25,334:INFO:Initializing Naive Bayes
2024-09-29 15:08:25,334:INFO:Total runtime is 0.1301474650700887 minutes
2024-09-29 15:08:25,334:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:25,334:INFO:Initializing create_model()
2024-09-29 15:08:25,335:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:25,335:INFO:Checking exceptions
2024-09-29 15:08:25,335:INFO:Importing libraries
2024-09-29 15:08:25,335:INFO:Copying training dataset
2024-09-29 15:08:25,501:INFO:Defining folds
2024-09-29 15:08:25,502:INFO:Declaring metric variables
2024-09-29 15:08:25,502:INFO:Importing untrained model
2024-09-29 15:08:25,502:INFO:Naive Bayes Imported successfully
2024-09-29 15:08:25,502:INFO:Starting cross validation
2024-09-29 15:08:25,504:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:08:26,317:INFO:Calculating mean and std
2024-09-29 15:08:26,317:INFO:Creating metrics dataframe
2024-09-29 15:08:26,319:INFO:Uploading results into container
2024-09-29 15:08:26,319:INFO:Uploading model into container now
2024-09-29 15:08:26,319:INFO:_master_model_container: 3
2024-09-29 15:08:26,319:INFO:_display_container: 2
2024-09-29 15:08:26,320:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-29 15:08:26,320:INFO:create_model() successfully completed......................................
2024-09-29 15:08:26,380:INFO:SubProcess create_model() end ==================================
2024-09-29 15:08:26,381:INFO:Creating metrics dataframe
2024-09-29 15:08:26,383:INFO:Initializing Decision Tree Classifier
2024-09-29 15:08:26,383:INFO:Total runtime is 0.14762854973475137 minutes
2024-09-29 15:08:26,383:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:26,383:INFO:Initializing create_model()
2024-09-29 15:08:26,383:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:26,383:INFO:Checking exceptions
2024-09-29 15:08:26,383:INFO:Importing libraries
2024-09-29 15:08:26,383:INFO:Copying training dataset
2024-09-29 15:08:26,549:INFO:Defining folds
2024-09-29 15:08:26,549:INFO:Declaring metric variables
2024-09-29 15:08:26,550:INFO:Importing untrained model
2024-09-29 15:08:26,550:INFO:Decision Tree Classifier Imported successfully
2024-09-29 15:08:26,550:INFO:Starting cross validation
2024-09-29 15:08:26,552:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:08:27,851:INFO:Calculating mean and std
2024-09-29 15:08:27,851:INFO:Creating metrics dataframe
2024-09-29 15:08:27,853:INFO:Uploading results into container
2024-09-29 15:08:27,853:INFO:Uploading model into container now
2024-09-29 15:08:27,853:INFO:_master_model_container: 4
2024-09-29 15:08:27,853:INFO:_display_container: 2
2024-09-29 15:08:27,854:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-29 15:08:27,854:INFO:create_model() successfully completed......................................
2024-09-29 15:08:27,913:INFO:SubProcess create_model() end ==================================
2024-09-29 15:08:27,913:INFO:Creating metrics dataframe
2024-09-29 15:08:27,915:INFO:Initializing SVM - Linear Kernel
2024-09-29 15:08:27,915:INFO:Total runtime is 0.1731626550356547 minutes
2024-09-29 15:08:27,916:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:27,916:INFO:Initializing create_model()
2024-09-29 15:08:27,916:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:27,916:INFO:Checking exceptions
2024-09-29 15:08:27,916:INFO:Importing libraries
2024-09-29 15:08:27,916:INFO:Copying training dataset
2024-09-29 15:08:28,081:INFO:Defining folds
2024-09-29 15:08:28,081:INFO:Declaring metric variables
2024-09-29 15:08:28,081:INFO:Importing untrained model
2024-09-29 15:08:28,082:INFO:SVM - Linear Kernel Imported successfully
2024-09-29 15:08:28,082:INFO:Starting cross validation
2024-09-29 15:08:28,084:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:08:28,626:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:28,692:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:28,707:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:28,753:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:28,941:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:28,953:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:28,961:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:28,961:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:29,060:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:29,120:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:29,132:INFO:Calculating mean and std
2024-09-29 15:08:29,132:INFO:Creating metrics dataframe
2024-09-29 15:08:29,134:INFO:Uploading results into container
2024-09-29 15:08:29,134:INFO:Uploading model into container now
2024-09-29 15:08:29,134:INFO:_master_model_container: 5
2024-09-29 15:08:29,134:INFO:_display_container: 2
2024-09-29 15:08:29,135:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-29 15:08:29,135:INFO:create_model() successfully completed......................................
2024-09-29 15:08:29,193:INFO:SubProcess create_model() end ==================================
2024-09-29 15:08:29,193:INFO:Creating metrics dataframe
2024-09-29 15:08:29,195:INFO:Initializing Ridge Classifier
2024-09-29 15:08:29,195:INFO:Total runtime is 0.19449933767318725 minutes
2024-09-29 15:08:29,195:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:29,196:INFO:Initializing create_model()
2024-09-29 15:08:29,196:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:29,196:INFO:Checking exceptions
2024-09-29 15:08:29,196:INFO:Importing libraries
2024-09-29 15:08:29,196:INFO:Copying training dataset
2024-09-29 15:08:29,359:INFO:Defining folds
2024-09-29 15:08:29,360:INFO:Declaring metric variables
2024-09-29 15:08:29,360:INFO:Importing untrained model
2024-09-29 15:08:29,360:INFO:Ridge Classifier Imported successfully
2024-09-29 15:08:29,360:INFO:Starting cross validation
2024-09-29 15:08:29,362:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:08:29,699:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:29,756:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:29,805:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:29,944:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:29,984:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:30,036:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:30,049:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:30,153:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:30,161:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:30,187:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:30,199:INFO:Calculating mean and std
2024-09-29 15:08:30,199:INFO:Creating metrics dataframe
2024-09-29 15:08:30,201:INFO:Uploading results into container
2024-09-29 15:08:30,201:INFO:Uploading model into container now
2024-09-29 15:08:30,201:INFO:_master_model_container: 6
2024-09-29 15:08:30,201:INFO:_display_container: 2
2024-09-29 15:08:30,202:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-29 15:08:30,202:INFO:create_model() successfully completed......................................
2024-09-29 15:08:30,260:INFO:SubProcess create_model() end ==================================
2024-09-29 15:08:30,260:INFO:Creating metrics dataframe
2024-09-29 15:08:30,262:INFO:Initializing Random Forest Classifier
2024-09-29 15:08:30,262:INFO:Total runtime is 0.21228455305099486 minutes
2024-09-29 15:08:30,262:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:30,262:INFO:Initializing create_model()
2024-09-29 15:08:30,262:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:30,262:INFO:Checking exceptions
2024-09-29 15:08:30,262:INFO:Importing libraries
2024-09-29 15:08:30,263:INFO:Copying training dataset
2024-09-29 15:08:30,427:INFO:Defining folds
2024-09-29 15:08:30,427:INFO:Declaring metric variables
2024-09-29 15:08:30,427:INFO:Importing untrained model
2024-09-29 15:08:30,427:INFO:Random Forest Classifier Imported successfully
2024-09-29 15:08:30,427:INFO:Starting cross validation
2024-09-29 15:08:30,430:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:08:32,708:INFO:Calculating mean and std
2024-09-29 15:08:32,708:INFO:Creating metrics dataframe
2024-09-29 15:08:32,710:INFO:Uploading results into container
2024-09-29 15:08:32,710:INFO:Uploading model into container now
2024-09-29 15:08:32,710:INFO:_master_model_container: 7
2024-09-29 15:08:32,711:INFO:_display_container: 2
2024-09-29 15:08:32,711:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-29 15:08:32,711:INFO:create_model() successfully completed......................................
2024-09-29 15:08:32,771:INFO:SubProcess create_model() end ==================================
2024-09-29 15:08:32,771:INFO:Creating metrics dataframe
2024-09-29 15:08:32,773:INFO:Initializing Quadratic Discriminant Analysis
2024-09-29 15:08:32,773:INFO:Total runtime is 0.2541271130243937 minutes
2024-09-29 15:08:32,773:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:32,773:INFO:Initializing create_model()
2024-09-29 15:08:32,774:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:32,774:INFO:Checking exceptions
2024-09-29 15:08:32,774:INFO:Importing libraries
2024-09-29 15:08:32,774:INFO:Copying training dataset
2024-09-29 15:08:32,939:INFO:Defining folds
2024-09-29 15:08:32,939:INFO:Declaring metric variables
2024-09-29 15:08:32,939:INFO:Importing untrained model
2024-09-29 15:08:32,940:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-29 15:08:32,940:INFO:Starting cross validation
2024-09-29 15:08:32,942:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:08:33,400:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:33,454:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:33,464:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:33,565:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:33,639:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:33,736:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:33,947:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:33,998:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:34,049:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:34,099:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:08:34,332:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,416:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,468:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,505:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,608:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,659:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,732:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,769:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,787:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,797:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:34,814:INFO:Calculating mean and std
2024-09-29 15:08:34,814:INFO:Creating metrics dataframe
2024-09-29 15:08:34,816:INFO:Uploading results into container
2024-09-29 15:08:34,816:INFO:Uploading model into container now
2024-09-29 15:08:34,816:INFO:_master_model_container: 8
2024-09-29 15:08:34,816:INFO:_display_container: 2
2024-09-29 15:08:34,817:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-29 15:08:34,817:INFO:create_model() successfully completed......................................
2024-09-29 15:08:34,878:INFO:SubProcess create_model() end ==================================
2024-09-29 15:08:34,878:INFO:Creating metrics dataframe
2024-09-29 15:08:34,880:INFO:Initializing Ada Boost Classifier
2024-09-29 15:08:34,880:INFO:Total runtime is 0.2892429312070211 minutes
2024-09-29 15:08:34,880:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:34,880:INFO:Initializing create_model()
2024-09-29 15:08:34,880:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:34,880:INFO:Checking exceptions
2024-09-29 15:08:34,880:INFO:Importing libraries
2024-09-29 15:08:34,881:INFO:Copying training dataset
2024-09-29 15:08:35,044:INFO:Defining folds
2024-09-29 15:08:35,044:INFO:Declaring metric variables
2024-09-29 15:08:35,045:INFO:Importing untrained model
2024-09-29 15:08:35,045:INFO:Ada Boost Classifier Imported successfully
2024-09-29 15:08:35,045:INFO:Starting cross validation
2024-09-29 15:08:35,047:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:08:35,328:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:35,426:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:35,434:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:35,470:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:35,592:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:35,645:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:35,702:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:35,747:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:35,773:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:35,874:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:08:37,417:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,514:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,537:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,584:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,667:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,718:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,757:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,801:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,806:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,884:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:08:37,896:INFO:Calculating mean and std
2024-09-29 15:08:37,896:INFO:Creating metrics dataframe
2024-09-29 15:08:37,897:INFO:Uploading results into container
2024-09-29 15:08:37,897:INFO:Uploading model into container now
2024-09-29 15:08:37,898:INFO:_master_model_container: 9
2024-09-29 15:08:37,898:INFO:_display_container: 2
2024-09-29 15:08:37,898:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-29 15:08:37,898:INFO:create_model() successfully completed......................................
2024-09-29 15:08:37,957:INFO:SubProcess create_model() end ==================================
2024-09-29 15:08:37,958:INFO:Creating metrics dataframe
2024-09-29 15:08:37,960:INFO:Initializing Gradient Boosting Classifier
2024-09-29 15:08:37,960:INFO:Total runtime is 0.3405856927235921 minutes
2024-09-29 15:08:37,960:INFO:SubProcess create_model() called ==================================
2024-09-29 15:08:37,960:INFO:Initializing create_model()
2024-09-29 15:08:37,960:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:08:37,960:INFO:Checking exceptions
2024-09-29 15:08:37,960:INFO:Importing libraries
2024-09-29 15:08:37,960:INFO:Copying training dataset
2024-09-29 15:08:38,128:INFO:Defining folds
2024-09-29 15:08:38,128:INFO:Declaring metric variables
2024-09-29 15:08:38,128:INFO:Importing untrained model
2024-09-29 15:08:38,128:INFO:Gradient Boosting Classifier Imported successfully
2024-09-29 15:08:38,129:INFO:Starting cross validation
2024-09-29 15:08:38,131:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:09:15,859:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,147:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,300:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,402:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,413:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,487:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,518:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,548:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,615:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,659:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:16,677:INFO:Calculating mean and std
2024-09-29 15:09:16,677:INFO:Creating metrics dataframe
2024-09-29 15:09:16,679:INFO:Uploading results into container
2024-09-29 15:09:16,679:INFO:Uploading model into container now
2024-09-29 15:09:16,680:INFO:_master_model_container: 10
2024-09-29 15:09:16,680:INFO:_display_container: 2
2024-09-29 15:09:16,680:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-29 15:09:16,680:INFO:create_model() successfully completed......................................
2024-09-29 15:09:16,742:INFO:SubProcess create_model() end ==================================
2024-09-29 15:09:16,742:INFO:Creating metrics dataframe
2024-09-29 15:09:16,744:INFO:Initializing Linear Discriminant Analysis
2024-09-29 15:09:16,744:INFO:Total runtime is 0.9869736194610597 minutes
2024-09-29 15:09:16,744:INFO:SubProcess create_model() called ==================================
2024-09-29 15:09:16,745:INFO:Initializing create_model()
2024-09-29 15:09:16,745:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:09:16,745:INFO:Checking exceptions
2024-09-29 15:09:16,745:INFO:Importing libraries
2024-09-29 15:09:16,745:INFO:Copying training dataset
2024-09-29 15:09:16,907:INFO:Defining folds
2024-09-29 15:09:16,907:INFO:Declaring metric variables
2024-09-29 15:09:16,907:INFO:Importing untrained model
2024-09-29 15:09:16,907:INFO:Linear Discriminant Analysis Imported successfully
2024-09-29 15:09:16,907:INFO:Starting cross validation
2024-09-29 15:09:16,909:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:09:18,215:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,322:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,583:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,627:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,656:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,768:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,795:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,878:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,914:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,952:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:09:18,967:INFO:Calculating mean and std
2024-09-29 15:09:18,967:INFO:Creating metrics dataframe
2024-09-29 15:09:18,969:INFO:Uploading results into container
2024-09-29 15:09:18,969:INFO:Uploading model into container now
2024-09-29 15:09:18,969:INFO:_master_model_container: 11
2024-09-29 15:09:18,969:INFO:_display_container: 2
2024-09-29 15:09:18,970:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-29 15:09:18,970:INFO:create_model() successfully completed......................................
2024-09-29 15:09:19,030:INFO:SubProcess create_model() end ==================================
2024-09-29 15:09:19,030:INFO:Creating metrics dataframe
2024-09-29 15:09:19,032:INFO:Initializing Extra Trees Classifier
2024-09-29 15:09:19,032:INFO:Total runtime is 1.025117353598277 minutes
2024-09-29 15:09:19,032:INFO:SubProcess create_model() called ==================================
2024-09-29 15:09:19,033:INFO:Initializing create_model()
2024-09-29 15:09:19,033:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:09:19,033:INFO:Checking exceptions
2024-09-29 15:09:19,033:INFO:Importing libraries
2024-09-29 15:09:19,033:INFO:Copying training dataset
2024-09-29 15:09:19,197:INFO:Defining folds
2024-09-29 15:09:19,197:INFO:Declaring metric variables
2024-09-29 15:09:19,197:INFO:Importing untrained model
2024-09-29 15:09:19,198:INFO:Extra Trees Classifier Imported successfully
2024-09-29 15:09:19,198:INFO:Starting cross validation
2024-09-29 15:09:19,200:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:09:21,590:INFO:Calculating mean and std
2024-09-29 15:09:21,590:INFO:Creating metrics dataframe
2024-09-29 15:09:21,592:INFO:Uploading results into container
2024-09-29 15:09:21,592:INFO:Uploading model into container now
2024-09-29 15:09:21,592:INFO:_master_model_container: 12
2024-09-29 15:09:21,592:INFO:_display_container: 2
2024-09-29 15:09:21,593:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:09:21,593:INFO:create_model() successfully completed......................................
2024-09-29 15:09:21,658:INFO:SubProcess create_model() end ==================================
2024-09-29 15:09:21,658:INFO:Creating metrics dataframe
2024-09-29 15:09:21,660:INFO:Initializing Light Gradient Boosting Machine
2024-09-29 15:09:21,661:INFO:Total runtime is 1.0689213275909426 minutes
2024-09-29 15:09:21,661:INFO:SubProcess create_model() called ==================================
2024-09-29 15:09:21,661:INFO:Initializing create_model()
2024-09-29 15:09:21,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:09:21,661:INFO:Checking exceptions
2024-09-29 15:09:21,661:INFO:Importing libraries
2024-09-29 15:09:21,661:INFO:Copying training dataset
2024-09-29 15:09:21,829:INFO:Defining folds
2024-09-29 15:09:21,829:INFO:Declaring metric variables
2024-09-29 15:09:21,829:INFO:Importing untrained model
2024-09-29 15:09:21,829:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-29 15:09:21,829:INFO:Starting cross validation
2024-09-29 15:09:21,832:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:09:44,036:INFO:Calculating mean and std
2024-09-29 15:09:44,036:INFO:Creating metrics dataframe
2024-09-29 15:09:44,039:INFO:Uploading results into container
2024-09-29 15:09:44,039:INFO:Uploading model into container now
2024-09-29 15:09:44,039:INFO:_master_model_container: 13
2024-09-29 15:09:44,039:INFO:_display_container: 2
2024-09-29 15:09:44,040:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-29 15:09:44,040:INFO:create_model() successfully completed......................................
2024-09-29 15:09:44,123:INFO:SubProcess create_model() end ==================================
2024-09-29 15:09:44,123:INFO:Creating metrics dataframe
2024-09-29 15:09:44,125:INFO:Initializing Dummy Classifier
2024-09-29 15:09:44,125:INFO:Total runtime is 1.4433304627736412 minutes
2024-09-29 15:09:44,125:INFO:SubProcess create_model() called ==================================
2024-09-29 15:09:44,126:INFO:Initializing create_model()
2024-09-29 15:09:44,126:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000216175A3E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:09:44,126:INFO:Checking exceptions
2024-09-29 15:09:44,126:INFO:Importing libraries
2024-09-29 15:09:44,126:INFO:Copying training dataset
2024-09-29 15:09:44,290:INFO:Defining folds
2024-09-29 15:09:44,290:INFO:Declaring metric variables
2024-09-29 15:09:44,290:INFO:Importing untrained model
2024-09-29 15:09:44,290:INFO:Dummy Classifier Imported successfully
2024-09-29 15:09:44,290:INFO:Starting cross validation
2024-09-29 15:09:44,292:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:09:44,595:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:44,625:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:44,681:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:44,684:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:44,810:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:44,866:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:44,925:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:44,966:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:45,023:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:45,065:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:09:45,074:INFO:Calculating mean and std
2024-09-29 15:09:45,074:INFO:Creating metrics dataframe
2024-09-29 15:09:45,076:INFO:Uploading results into container
2024-09-29 15:09:45,076:INFO:Uploading model into container now
2024-09-29 15:09:45,076:INFO:_master_model_container: 14
2024-09-29 15:09:45,076:INFO:_display_container: 2
2024-09-29 15:09:45,076:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-29 15:09:45,077:INFO:create_model() successfully completed......................................
2024-09-29 15:09:45,135:INFO:SubProcess create_model() end ==================================
2024-09-29 15:09:45,135:INFO:Creating metrics dataframe
2024-09-29 15:09:45,137:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning:

Styler.applymap has been deprecated. Use Styler.map instead.


2024-09-29 15:09:45,138:INFO:Initializing create_model()
2024-09-29 15:09:45,138:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:09:45,138:INFO:Checking exceptions
2024-09-29 15:09:45,139:INFO:Importing libraries
2024-09-29 15:09:45,139:INFO:Copying training dataset
2024-09-29 15:09:45,301:INFO:Defining folds
2024-09-29 15:09:45,302:INFO:Declaring metric variables
2024-09-29 15:09:45,302:INFO:Importing untrained model
2024-09-29 15:09:45,302:INFO:Declaring custom model
2024-09-29 15:09:45,302:INFO:Extra Trees Classifier Imported successfully
2024-09-29 15:09:45,304:INFO:Cross validation set to False
2024-09-29 15:09:45,304:INFO:Fitting Model
2024-09-29 15:09:45,610:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:09:45,610:INFO:create_model() successfully completed......................................
2024-09-29 15:09:45,682:INFO:_master_model_container: 14
2024-09-29 15:09:45,682:INFO:_display_container: 2
2024-09-29 15:09:45,682:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:09:45,682:INFO:compare_models() successfully completed......................................
2024-09-29 15:09:45,683:INFO:Initializing finalize_model()
2024-09-29 15:09:45,683:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-29 15:09:45,683:INFO:Finalizing ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:09:45,796:INFO:Initializing create_model()
2024-09-29 15:09:45,796:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:09:45,796:INFO:Checking exceptions
2024-09-29 15:09:45,796:INFO:Importing libraries
2024-09-29 15:09:45,796:INFO:Copying training dataset
2024-09-29 15:09:45,812:INFO:Defining folds
2024-09-29 15:09:45,812:INFO:Declaring metric variables
2024-09-29 15:09:45,812:INFO:Importing untrained model
2024-09-29 15:09:45,812:INFO:Declaring custom model
2024-09-29 15:09:45,813:INFO:Extra Trees Classifier Imported successfully
2024-09-29 15:09:45,816:INFO:Cross validation set to False
2024-09-29 15:09:45,816:INFO:Fitting Model
2024-09-29 15:09:46,203:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-29 15:09:46,203:INFO:create_model() successfully completed......................................
2024-09-29 15:09:46,262:INFO:_master_model_container: 14
2024-09-29 15:09:46,262:INFO:_display_container: 2
2024-09-29 15:09:46,269:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-29 15:09:46,269:INFO:finalize_model() successfully completed......................................
2024-09-29 15:09:46,334:INFO:Initializing predict_model()
2024-09-29 15:09:46,335:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002165D5A47C0>)
2024-09-29 15:09:46,335:INFO:Checking exceptions
2024-09-29 15:09:46,335:INFO:Preloading libraries
2024-09-29 15:09:46,335:INFO:Set up data.
2024-09-29 15:09:46,480:INFO:Set up index.
2024-09-29 15:09:46,658:INFO:Initializing plot_model()
2024-09-29 15:09:46,658:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021615B7AFD0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-29 15:09:46,658:INFO:Checking exceptions
2024-09-29 15:09:46,747:INFO:Preloading libraries
2024-09-29 15:09:46,771:INFO:Copying training dataset
2024-09-29 15:09:46,771:INFO:Plot type: feature
2024-09-29 15:09:46,771:WARNING:No coef_ found. Trying feature_importances_
2024-09-29 15:09:47,771:INFO:Visual Rendered Successfully
2024-09-29 15:09:47,837:INFO:plot_model() successfully completed......................................
2024-09-29 15:37:57,079:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:37:57,079:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:37:57,079:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:37:57,079:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:38:00,094:INFO:PyCaret ClassificationExperiment
2024-09-29 15:38:00,094:INFO:Logging name: clf-default-name
2024-09-29 15:38:00,094:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-29 15:38:00,094:INFO:version 3.3.2
2024-09-29 15:38:00,095:INFO:Initializing setup()
2024-09-29 15:38:00,095:INFO:self.USI: 7bb0
2024-09-29 15:38:00,095:INFO:self._variable_keys: {'exp_id', 'memory', 'logging_param', 'y', 'y_train', 'fix_imbalance', 'X_test', 'target_param', 'data', '_ml_usecase', 'n_jobs_param', 'X', 'seed', 'gpu_param', 'idx', 'pipeline', 'fold_groups_param', 'fold_shuffle_param', 'y_test', 'is_multiclass', 'USI', 'log_plots_param', 'gpu_n_jobs_param', 'X_train', 'html_param', 'fold_generator', '_available_plots', 'exp_name_log'}
2024-09-29 15:38:00,095:INFO:Checking environment
2024-09-29 15:38:00,095:INFO:python_version: 3.11.6
2024-09-29 15:38:00,095:INFO:python_build: ('tags/v3.11.6:8b6ee5b', 'Oct  2 2023 14:57:12')
2024-09-29 15:38:00,095:INFO:machine: AMD64
2024-09-29 15:38:00,103:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-29 15:38:00,105:INFO:Memory: svmem(total=17113423872, available=9213046784, percent=46.2, used=7900377088, free=9213046784)
2024-09-29 15:38:00,105:INFO:Physical Core: 6
2024-09-29 15:38:00,105:INFO:Logical Core: 12
2024-09-29 15:38:00,105:INFO:Checking libraries
2024-09-29 15:38:00,105:INFO:System:
2024-09-29 15:38:00,105:INFO:    python: 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]
2024-09-29 15:38:00,105:INFO:executable: C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Scripts\python.exe
2024-09-29 15:38:00,105:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-29 15:38:00,105:INFO:PyCaret required dependencies:
2024-09-29 15:38:00,155:INFO:                 pip: 23.2.1
2024-09-29 15:38:00,155:INFO:          setuptools: 68.2.0
2024-09-29 15:38:00,155:INFO:             pycaret: 3.3.2
2024-09-29 15:38:00,155:INFO:             IPython: 8.27.0
2024-09-29 15:38:00,155:INFO:          ipywidgets: 8.1.5
2024-09-29 15:38:00,155:INFO:                tqdm: 4.66.5
2024-09-29 15:38:00,155:INFO:               numpy: 1.26.4
2024-09-29 15:38:00,156:INFO:              pandas: 2.1.4
2024-09-29 15:38:00,156:INFO:              jinja2: 3.1.4
2024-09-29 15:38:00,156:INFO:               scipy: 1.11.4
2024-09-29 15:38:00,156:INFO:              joblib: 1.3.2
2024-09-29 15:38:00,156:INFO:             sklearn: 1.4.2
2024-09-29 15:38:00,156:INFO:                pyod: 2.0.2
2024-09-29 15:38:00,156:INFO:            imblearn: 0.12.3
2024-09-29 15:38:00,156:INFO:   category_encoders: 2.6.3
2024-09-29 15:38:00,156:INFO:            lightgbm: 4.5.0
2024-09-29 15:38:00,156:INFO:               numba: 0.60.0
2024-09-29 15:38:00,156:INFO:            requests: 2.32.3
2024-09-29 15:38:00,156:INFO:          matplotlib: 3.7.5
2024-09-29 15:38:00,156:INFO:          scikitplot: 0.3.7
2024-09-29 15:38:00,156:INFO:         yellowbrick: 1.5
2024-09-29 15:38:00,156:INFO:              plotly: 5.24.1
2024-09-29 15:38:00,156:INFO:    plotly-resampler: Not installed
2024-09-29 15:38:00,156:INFO:             kaleido: 0.2.1
2024-09-29 15:38:00,156:INFO:           schemdraw: 0.15
2024-09-29 15:38:00,156:INFO:         statsmodels: 0.14.3
2024-09-29 15:38:00,156:INFO:              sktime: 0.26.0
2024-09-29 15:38:00,156:INFO:               tbats: 1.1.3
2024-09-29 15:38:00,156:INFO:            pmdarima: 2.0.4
2024-09-29 15:38:00,156:INFO:              psutil: 6.0.0
2024-09-29 15:38:00,156:INFO:          markupsafe: 2.1.5
2024-09-29 15:38:00,156:INFO:             pickle5: Not installed
2024-09-29 15:38:00,156:INFO:         cloudpickle: 3.0.0
2024-09-29 15:38:00,156:INFO:         deprecation: 2.1.0
2024-09-29 15:38:00,156:INFO:              xxhash: 3.5.0
2024-09-29 15:38:00,156:INFO:           wurlitzer: Not installed
2024-09-29 15:38:00,157:INFO:PyCaret optional dependencies:
2024-09-29 15:38:00,172:INFO:                shap: Not installed
2024-09-29 15:38:00,172:INFO:           interpret: Not installed
2024-09-29 15:38:00,172:INFO:                umap: Not installed
2024-09-29 15:38:00,172:INFO:     ydata_profiling: Not installed
2024-09-29 15:38:00,172:INFO:  explainerdashboard: Not installed
2024-09-29 15:38:00,172:INFO:             autoviz: Not installed
2024-09-29 15:38:00,172:INFO:           fairlearn: Not installed
2024-09-29 15:38:00,172:INFO:          deepchecks: Not installed
2024-09-29 15:38:00,172:INFO:             xgboost: Not installed
2024-09-29 15:38:00,172:INFO:            catboost: Not installed
2024-09-29 15:38:00,172:INFO:              kmodes: Not installed
2024-09-29 15:38:00,172:INFO:             mlxtend: Not installed
2024-09-29 15:38:00,172:INFO:       statsforecast: Not installed
2024-09-29 15:38:00,172:INFO:        tune_sklearn: Not installed
2024-09-29 15:38:00,172:INFO:                 ray: Not installed
2024-09-29 15:38:00,172:INFO:            hyperopt: Not installed
2024-09-29 15:38:00,172:INFO:              optuna: Not installed
2024-09-29 15:38:00,172:INFO:               skopt: Not installed
2024-09-29 15:38:00,173:INFO:              mlflow: 2.16.2
2024-09-29 15:38:00,173:INFO:              gradio: Not installed
2024-09-29 15:38:00,173:INFO:             fastapi: Not installed
2024-09-29 15:38:00,173:INFO:             uvicorn: Not installed
2024-09-29 15:38:00,173:INFO:              m2cgen: Not installed
2024-09-29 15:38:00,173:INFO:           evidently: Not installed
2024-09-29 15:38:00,173:INFO:               fugue: Not installed
2024-09-29 15:38:00,173:INFO:           streamlit: Not installed
2024-09-29 15:38:00,173:INFO:             prophet: Not installed
2024-09-29 15:38:00,173:INFO:None
2024-09-29 15:38:00,173:INFO:Set up data.
2024-09-29 15:38:00,375:INFO:Set up folding strategy.
2024-09-29 15:38:00,375:INFO:Set up train/test split.
2024-09-29 15:38:00,580:INFO:Set up index.
2024-09-29 15:38:00,584:INFO:Assigning column types.
2024-09-29 15:38:00,800:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-29 15:38:00,841:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-29 15:38:00,844:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:38:00,875:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:00,875:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:00,915:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-29 15:38:00,916:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:38:00,941:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:00,941:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:00,941:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-29 15:38:00,982:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:38:01,007:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:01,007:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:01,049:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:38:01,074:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:01,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:01,075:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-29 15:38:01,140:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:01,140:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:01,205:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:01,205:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:01,214:INFO:Preparing preprocessing pipeline...
2024-09-29 15:38:01,238:INFO:Set up simple imputation.
2024-09-29 15:38:01,702:INFO:Finished creating preprocessing pipeline.
2024-09-29 15:38:01,709:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-29 15:38:01,709:INFO:Creating final display dataframe.
2024-09-29 15:38:02,741:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape      (3156, 1001)
4        Transformed data shape      (3156, 1001)
5   Transformed train set shape      (2209, 1001)
6    Transformed test set shape       (947, 1001)
7              Numeric features              1000
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              7bb0
2024-09-29 15:38:02,807:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:02,807:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:02,872:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:02,873:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:38:02,874:INFO:setup() successfully completed in 2.78s...............
2024-09-29 15:38:02,874:INFO:Initializing compare_models()
2024-09-29 15:38:02,874:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-29 15:38:02,874:INFO:Checking exceptions
2024-09-29 15:38:03,046:INFO:Preparing display monitor
2024-09-29 15:38:03,049:INFO:Initializing Logistic Regression
2024-09-29 15:38:03,049:INFO:Total runtime is 0.0 minutes
2024-09-29 15:38:03,049:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:03,050:INFO:Initializing create_model()
2024-09-29 15:38:03,050:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:03,050:INFO:Checking exceptions
2024-09-29 15:38:03,050:INFO:Importing libraries
2024-09-29 15:38:03,050:INFO:Copying training dataset
2024-09-29 15:38:03,222:INFO:Defining folds
2024-09-29 15:38:03,222:INFO:Declaring metric variables
2024-09-29 15:38:03,222:INFO:Importing untrained model
2024-09-29 15:38:03,223:INFO:Logistic Regression Imported successfully
2024-09-29 15:38:03,223:INFO:Starting cross validation
2024-09-29 15:38:03,225:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:38:08,287:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:08,443:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:08,577:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:08,718:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:08,732:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:08,805:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:08,864:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:08,904:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:08,979:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:08,988:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:09,005:INFO:Calculating mean and std
2024-09-29 15:38:09,006:INFO:Creating metrics dataframe
2024-09-29 15:38:09,007:INFO:Uploading results into container
2024-09-29 15:38:09,008:INFO:Uploading model into container now
2024-09-29 15:38:09,008:INFO:_master_model_container: 1
2024-09-29 15:38:09,008:INFO:_display_container: 2
2024-09-29 15:38:09,008:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-29 15:38:09,008:INFO:create_model() successfully completed......................................
2024-09-29 15:38:09,076:INFO:SubProcess create_model() end ==================================
2024-09-29 15:38:09,076:INFO:Creating metrics dataframe
2024-09-29 15:38:09,078:INFO:Initializing K Neighbors Classifier
2024-09-29 15:38:09,078:INFO:Total runtime is 0.10048692623774211 minutes
2024-09-29 15:38:09,078:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:09,078:INFO:Initializing create_model()
2024-09-29 15:38:09,078:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:09,078:INFO:Checking exceptions
2024-09-29 15:38:09,078:INFO:Importing libraries
2024-09-29 15:38:09,078:INFO:Copying training dataset
2024-09-29 15:38:09,241:INFO:Defining folds
2024-09-29 15:38:09,241:INFO:Declaring metric variables
2024-09-29 15:38:09,241:INFO:Importing untrained model
2024-09-29 15:38:09,241:INFO:K Neighbors Classifier Imported successfully
2024-09-29 15:38:09,242:INFO:Starting cross validation
2024-09-29 15:38:09,244:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:38:10,994:INFO:Calculating mean and std
2024-09-29 15:38:10,995:INFO:Creating metrics dataframe
2024-09-29 15:38:10,997:INFO:Uploading results into container
2024-09-29 15:38:10,997:INFO:Uploading model into container now
2024-09-29 15:38:10,998:INFO:_master_model_container: 2
2024-09-29 15:38:10,998:INFO:_display_container: 2
2024-09-29 15:38:10,998:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-29 15:38:10,998:INFO:create_model() successfully completed......................................
2024-09-29 15:38:11,068:INFO:SubProcess create_model() end ==================================
2024-09-29 15:38:11,068:INFO:Creating metrics dataframe
2024-09-29 15:38:11,070:INFO:Initializing Naive Bayes
2024-09-29 15:38:11,071:INFO:Total runtime is 0.13369783163070678 minutes
2024-09-29 15:38:11,071:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:11,071:INFO:Initializing create_model()
2024-09-29 15:38:11,071:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:11,071:INFO:Checking exceptions
2024-09-29 15:38:11,071:INFO:Importing libraries
2024-09-29 15:38:11,071:INFO:Copying training dataset
2024-09-29 15:38:11,248:INFO:Defining folds
2024-09-29 15:38:11,248:INFO:Declaring metric variables
2024-09-29 15:38:11,248:INFO:Importing untrained model
2024-09-29 15:38:11,248:INFO:Naive Bayes Imported successfully
2024-09-29 15:38:11,248:INFO:Starting cross validation
2024-09-29 15:38:11,251:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:38:12,102:INFO:Calculating mean and std
2024-09-29 15:38:12,156:INFO:Creating metrics dataframe
2024-09-29 15:38:12,157:INFO:Uploading results into container
2024-09-29 15:38:12,158:INFO:Uploading model into container now
2024-09-29 15:38:12,158:INFO:_master_model_container: 3
2024-09-29 15:38:12,158:INFO:_display_container: 2
2024-09-29 15:38:12,158:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-29 15:38:12,158:INFO:create_model() successfully completed......................................
2024-09-29 15:38:12,223:INFO:SubProcess create_model() end ==================================
2024-09-29 15:38:12,223:INFO:Creating metrics dataframe
2024-09-29 15:38:12,225:INFO:Initializing Decision Tree Classifier
2024-09-29 15:38:12,225:INFO:Total runtime is 0.15292688608169555 minutes
2024-09-29 15:38:12,225:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:12,225:INFO:Initializing create_model()
2024-09-29 15:38:12,225:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:12,225:INFO:Checking exceptions
2024-09-29 15:38:12,225:INFO:Importing libraries
2024-09-29 15:38:12,225:INFO:Copying training dataset
2024-09-29 15:38:12,396:INFO:Defining folds
2024-09-29 15:38:12,396:INFO:Declaring metric variables
2024-09-29 15:38:12,396:INFO:Importing untrained model
2024-09-29 15:38:12,397:INFO:Decision Tree Classifier Imported successfully
2024-09-29 15:38:12,397:INFO:Starting cross validation
2024-09-29 15:38:12,399:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:38:13,859:INFO:Calculating mean and std
2024-09-29 15:38:13,859:INFO:Creating metrics dataframe
2024-09-29 15:38:13,861:INFO:Uploading results into container
2024-09-29 15:38:13,861:INFO:Uploading model into container now
2024-09-29 15:38:13,861:INFO:_master_model_container: 4
2024-09-29 15:38:13,862:INFO:_display_container: 2
2024-09-29 15:38:13,862:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-29 15:38:13,862:INFO:create_model() successfully completed......................................
2024-09-29 15:38:13,931:INFO:SubProcess create_model() end ==================================
2024-09-29 15:38:13,931:INFO:Creating metrics dataframe
2024-09-29 15:38:13,933:INFO:Initializing SVM - Linear Kernel
2024-09-29 15:38:13,933:INFO:Total runtime is 0.18139216899871827 minutes
2024-09-29 15:38:13,934:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:13,934:INFO:Initializing create_model()
2024-09-29 15:38:13,934:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:13,934:INFO:Checking exceptions
2024-09-29 15:38:13,934:INFO:Importing libraries
2024-09-29 15:38:13,934:INFO:Copying training dataset
2024-09-29 15:38:14,111:INFO:Defining folds
2024-09-29 15:38:14,112:INFO:Declaring metric variables
2024-09-29 15:38:14,112:INFO:Importing untrained model
2024-09-29 15:38:14,112:INFO:SVM - Linear Kernel Imported successfully
2024-09-29 15:38:14,112:INFO:Starting cross validation
2024-09-29 15:38:14,115:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:38:14,719:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:14,858:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:14,879:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:14,958:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:15,224:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:15,254:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:15,275:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:15,299:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:15,384:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:15,458:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:15,472:INFO:Calculating mean and std
2024-09-29 15:38:15,472:INFO:Creating metrics dataframe
2024-09-29 15:38:15,474:INFO:Uploading results into container
2024-09-29 15:38:15,474:INFO:Uploading model into container now
2024-09-29 15:38:15,474:INFO:_master_model_container: 5
2024-09-29 15:38:15,474:INFO:_display_container: 2
2024-09-29 15:38:15,475:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-29 15:38:15,475:INFO:create_model() successfully completed......................................
2024-09-29 15:38:15,540:INFO:SubProcess create_model() end ==================================
2024-09-29 15:38:15,540:INFO:Creating metrics dataframe
2024-09-29 15:38:15,542:INFO:Initializing Ridge Classifier
2024-09-29 15:38:15,543:INFO:Total runtime is 0.20823024511337282 minutes
2024-09-29 15:38:15,543:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:15,543:INFO:Initializing create_model()
2024-09-29 15:38:15,543:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:15,543:INFO:Checking exceptions
2024-09-29 15:38:15,543:INFO:Importing libraries
2024-09-29 15:38:15,543:INFO:Copying training dataset
2024-09-29 15:38:15,732:INFO:Defining folds
2024-09-29 15:38:15,732:INFO:Declaring metric variables
2024-09-29 15:38:15,732:INFO:Importing untrained model
2024-09-29 15:38:15,732:INFO:Ridge Classifier Imported successfully
2024-09-29 15:38:15,733:INFO:Starting cross validation
2024-09-29 15:38:15,735:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:38:16,072:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,174:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,219:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,293:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,492:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,534:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,550:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,633:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,650:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,743:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:16,760:INFO:Calculating mean and std
2024-09-29 15:38:16,760:INFO:Creating metrics dataframe
2024-09-29 15:38:16,762:INFO:Uploading results into container
2024-09-29 15:38:16,762:INFO:Uploading model into container now
2024-09-29 15:38:16,762:INFO:_master_model_container: 6
2024-09-29 15:38:16,763:INFO:_display_container: 2
2024-09-29 15:38:16,763:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-29 15:38:16,763:INFO:create_model() successfully completed......................................
2024-09-29 15:38:16,830:INFO:SubProcess create_model() end ==================================
2024-09-29 15:38:16,830:INFO:Creating metrics dataframe
2024-09-29 15:38:16,832:INFO:Initializing Random Forest Classifier
2024-09-29 15:38:16,832:INFO:Total runtime is 0.2297180493672689 minutes
2024-09-29 15:38:16,832:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:16,832:INFO:Initializing create_model()
2024-09-29 15:38:16,832:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:16,833:INFO:Checking exceptions
2024-09-29 15:38:16,833:INFO:Importing libraries
2024-09-29 15:38:16,833:INFO:Copying training dataset
2024-09-29 15:38:17,016:INFO:Defining folds
2024-09-29 15:38:17,016:INFO:Declaring metric variables
2024-09-29 15:38:17,016:INFO:Importing untrained model
2024-09-29 15:38:17,017:INFO:Random Forest Classifier Imported successfully
2024-09-29 15:38:17,017:INFO:Starting cross validation
2024-09-29 15:38:17,019:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:38:19,439:INFO:Calculating mean and std
2024-09-29 15:38:19,439:INFO:Creating metrics dataframe
2024-09-29 15:38:19,441:INFO:Uploading results into container
2024-09-29 15:38:19,441:INFO:Uploading model into container now
2024-09-29 15:38:19,442:INFO:_master_model_container: 7
2024-09-29 15:38:19,442:INFO:_display_container: 2
2024-09-29 15:38:19,442:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-29 15:38:19,442:INFO:create_model() successfully completed......................................
2024-09-29 15:38:19,506:INFO:SubProcess create_model() end ==================================
2024-09-29 15:38:19,506:INFO:Creating metrics dataframe
2024-09-29 15:38:19,508:INFO:Initializing Quadratic Discriminant Analysis
2024-09-29 15:38:19,508:INFO:Total runtime is 0.27430653969446817 minutes
2024-09-29 15:38:19,509:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:19,509:INFO:Initializing create_model()
2024-09-29 15:38:19,509:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:19,509:INFO:Checking exceptions
2024-09-29 15:38:19,509:INFO:Importing libraries
2024-09-29 15:38:19,509:INFO:Copying training dataset
2024-09-29 15:38:19,680:INFO:Defining folds
2024-09-29 15:38:19,680:INFO:Declaring metric variables
2024-09-29 15:38:19,680:INFO:Importing untrained model
2024-09-29 15:38:19,681:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-29 15:38:19,681:INFO:Starting cross validation
2024-09-29 15:38:19,683:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:38:20,103:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:20,168:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:20,240:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:20,334:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:20,505:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:20,664:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:20,722:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:20,865:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:21,218:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:21,325:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:21,417:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:21,422:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:38:21,514:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:21,637:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:21,845:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:21,916:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:21,950:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:22,047:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:22,207:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:22,241:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:22,257:INFO:Calculating mean and std
2024-09-29 15:38:22,257:INFO:Creating metrics dataframe
2024-09-29 15:38:22,259:INFO:Uploading results into container
2024-09-29 15:38:22,259:INFO:Uploading model into container now
2024-09-29 15:38:22,259:INFO:_master_model_container: 8
2024-09-29 15:38:22,259:INFO:_display_container: 2
2024-09-29 15:38:22,260:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-29 15:38:22,260:INFO:create_model() successfully completed......................................
2024-09-29 15:38:22,326:INFO:SubProcess create_model() end ==================================
2024-09-29 15:38:22,326:INFO:Creating metrics dataframe
2024-09-29 15:38:22,328:INFO:Initializing Ada Boost Classifier
2024-09-29 15:38:22,328:INFO:Total runtime is 0.3213203191757202 minutes
2024-09-29 15:38:22,328:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:22,328:INFO:Initializing create_model()
2024-09-29 15:38:22,328:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:22,328:INFO:Checking exceptions
2024-09-29 15:38:22,329:INFO:Importing libraries
2024-09-29 15:38:22,329:INFO:Copying training dataset
2024-09-29 15:38:22,512:INFO:Defining folds
2024-09-29 15:38:22,512:INFO:Declaring metric variables
2024-09-29 15:38:22,512:INFO:Importing untrained model
2024-09-29 15:38:22,513:INFO:Ada Boost Classifier Imported successfully
2024-09-29 15:38:22,513:INFO:Starting cross validation
2024-09-29 15:38:22,515:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:38:22,758:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:22,824:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:22,985:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:23,064:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:23,173:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:23,253:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:23,310:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:23,375:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:23,426:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:23,535:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:38:25,065:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,180:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,352:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,412:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,481:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,508:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,537:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,607:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,639:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,719:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:38:25,740:INFO:Calculating mean and std
2024-09-29 15:38:25,740:INFO:Creating metrics dataframe
2024-09-29 15:38:25,741:INFO:Uploading results into container
2024-09-29 15:38:25,741:INFO:Uploading model into container now
2024-09-29 15:38:25,742:INFO:_master_model_container: 9
2024-09-29 15:38:25,742:INFO:_display_container: 2
2024-09-29 15:38:25,742:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-29 15:38:25,742:INFO:create_model() successfully completed......................................
2024-09-29 15:38:25,801:INFO:SubProcess create_model() end ==================================
2024-09-29 15:38:25,801:INFO:Creating metrics dataframe
2024-09-29 15:38:25,804:INFO:Initializing Gradient Boosting Classifier
2024-09-29 15:38:25,804:INFO:Total runtime is 0.3792544841766357 minutes
2024-09-29 15:38:25,804:INFO:SubProcess create_model() called ==================================
2024-09-29 15:38:25,804:INFO:Initializing create_model()
2024-09-29 15:38:25,804:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:38:25,804:INFO:Checking exceptions
2024-09-29 15:38:25,804:INFO:Importing libraries
2024-09-29 15:38:25,804:INFO:Copying training dataset
2024-09-29 15:38:25,973:INFO:Defining folds
2024-09-29 15:38:25,973:INFO:Declaring metric variables
2024-09-29 15:38:25,973:INFO:Importing untrained model
2024-09-29 15:38:25,973:INFO:Gradient Boosting Classifier Imported successfully
2024-09-29 15:38:25,973:INFO:Starting cross validation
2024-09-29 15:38:25,975:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:39:05,166:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:05,187:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:05,466:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:05,483:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:05,550:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:05,557:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:05,646:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:05,681:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:05,739:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:06,088:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:06,098:INFO:Calculating mean and std
2024-09-29 15:39:06,098:INFO:Creating metrics dataframe
2024-09-29 15:39:06,100:INFO:Uploading results into container
2024-09-29 15:39:06,100:INFO:Uploading model into container now
2024-09-29 15:39:06,100:INFO:_master_model_container: 10
2024-09-29 15:39:06,100:INFO:_display_container: 2
2024-09-29 15:39:06,101:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-29 15:39:06,101:INFO:create_model() successfully completed......................................
2024-09-29 15:39:06,163:INFO:SubProcess create_model() end ==================================
2024-09-29 15:39:06,163:INFO:Creating metrics dataframe
2024-09-29 15:39:06,165:INFO:Initializing Linear Discriminant Analysis
2024-09-29 15:39:06,165:INFO:Total runtime is 1.051926600933075 minutes
2024-09-29 15:39:06,165:INFO:SubProcess create_model() called ==================================
2024-09-29 15:39:06,165:INFO:Initializing create_model()
2024-09-29 15:39:06,166:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:39:06,166:INFO:Checking exceptions
2024-09-29 15:39:06,166:INFO:Importing libraries
2024-09-29 15:39:06,166:INFO:Copying training dataset
2024-09-29 15:39:06,333:INFO:Defining folds
2024-09-29 15:39:06,333:INFO:Declaring metric variables
2024-09-29 15:39:06,333:INFO:Importing untrained model
2024-09-29 15:39:06,333:INFO:Linear Discriminant Analysis Imported successfully
2024-09-29 15:39:06,333:INFO:Starting cross validation
2024-09-29 15:39:06,336:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:39:07,660:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:07,795:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:08,037:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:08,149:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:08,337:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:08,398:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:08,444:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:08,464:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:08,566:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:08,583:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:39:08,596:INFO:Calculating mean and std
2024-09-29 15:39:08,596:INFO:Creating metrics dataframe
2024-09-29 15:39:08,598:INFO:Uploading results into container
2024-09-29 15:39:08,598:INFO:Uploading model into container now
2024-09-29 15:39:08,598:INFO:_master_model_container: 11
2024-09-29 15:39:08,598:INFO:_display_container: 2
2024-09-29 15:39:08,599:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-29 15:39:08,599:INFO:create_model() successfully completed......................................
2024-09-29 15:39:08,657:INFO:SubProcess create_model() end ==================================
2024-09-29 15:39:08,657:INFO:Creating metrics dataframe
2024-09-29 15:39:08,659:INFO:Initializing Extra Trees Classifier
2024-09-29 15:39:08,660:INFO:Total runtime is 1.0935208559036256 minutes
2024-09-29 15:39:08,660:INFO:SubProcess create_model() called ==================================
2024-09-29 15:39:08,660:INFO:Initializing create_model()
2024-09-29 15:39:08,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:39:08,660:INFO:Checking exceptions
2024-09-29 15:39:08,660:INFO:Importing libraries
2024-09-29 15:39:08,660:INFO:Copying training dataset
2024-09-29 15:39:08,825:INFO:Defining folds
2024-09-29 15:39:08,825:INFO:Declaring metric variables
2024-09-29 15:39:08,825:INFO:Importing untrained model
2024-09-29 15:39:08,825:INFO:Extra Trees Classifier Imported successfully
2024-09-29 15:39:08,826:INFO:Starting cross validation
2024-09-29 15:39:08,828:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:39:11,312:INFO:Calculating mean and std
2024-09-29 15:39:11,312:INFO:Creating metrics dataframe
2024-09-29 15:39:11,314:INFO:Uploading results into container
2024-09-29 15:39:11,314:INFO:Uploading model into container now
2024-09-29 15:39:11,314:INFO:_master_model_container: 12
2024-09-29 15:39:11,314:INFO:_display_container: 2
2024-09-29 15:39:11,315:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:39:11,315:INFO:create_model() successfully completed......................................
2024-09-29 15:39:11,380:INFO:SubProcess create_model() end ==================================
2024-09-29 15:39:11,380:INFO:Creating metrics dataframe
2024-09-29 15:39:11,383:INFO:Initializing Light Gradient Boosting Machine
2024-09-29 15:39:11,383:INFO:Total runtime is 1.1388890822728477 minutes
2024-09-29 15:39:11,383:INFO:SubProcess create_model() called ==================================
2024-09-29 15:39:11,383:INFO:Initializing create_model()
2024-09-29 15:39:11,383:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:39:11,383:INFO:Checking exceptions
2024-09-29 15:39:11,383:INFO:Importing libraries
2024-09-29 15:39:11,383:INFO:Copying training dataset
2024-09-29 15:39:11,550:INFO:Defining folds
2024-09-29 15:39:11,550:INFO:Declaring metric variables
2024-09-29 15:39:11,550:INFO:Importing untrained model
2024-09-29 15:39:11,550:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-29 15:39:11,550:INFO:Starting cross validation
2024-09-29 15:39:11,553:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:39:41,095:INFO:Calculating mean and std
2024-09-29 15:39:41,096:INFO:Creating metrics dataframe
2024-09-29 15:39:41,098:INFO:Uploading results into container
2024-09-29 15:39:41,099:INFO:Uploading model into container now
2024-09-29 15:39:41,099:INFO:_master_model_container: 13
2024-09-29 15:39:41,099:INFO:_display_container: 2
2024-09-29 15:39:41,100:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-29 15:39:41,100:INFO:create_model() successfully completed......................................
2024-09-29 15:39:41,204:INFO:SubProcess create_model() end ==================================
2024-09-29 15:39:41,204:INFO:Creating metrics dataframe
2024-09-29 15:39:41,208:INFO:Initializing Dummy Classifier
2024-09-29 15:39:41,209:INFO:Total runtime is 1.6359971324602764 minutes
2024-09-29 15:39:41,209:INFO:SubProcess create_model() called ==================================
2024-09-29 15:39:41,209:INFO:Initializing create_model()
2024-09-29 15:39:41,209:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021DBED5B650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:39:41,209:INFO:Checking exceptions
2024-09-29 15:39:41,209:INFO:Importing libraries
2024-09-29 15:39:41,210:INFO:Copying training dataset
2024-09-29 15:39:41,433:INFO:Defining folds
2024-09-29 15:39:41,433:INFO:Declaring metric variables
2024-09-29 15:39:41,433:INFO:Importing untrained model
2024-09-29 15:39:41,434:INFO:Dummy Classifier Imported successfully
2024-09-29 15:39:41,434:INFO:Starting cross validation
2024-09-29 15:39:41,436:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:39:41,830:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:41,942:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:42,014:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:42,129:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:42,171:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:42,198:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:42,230:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:42,359:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:42,375:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:42,417:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:39:42,429:INFO:Calculating mean and std
2024-09-29 15:39:42,429:INFO:Creating metrics dataframe
2024-09-29 15:39:42,431:INFO:Uploading results into container
2024-09-29 15:39:42,431:INFO:Uploading model into container now
2024-09-29 15:39:42,431:INFO:_master_model_container: 14
2024-09-29 15:39:42,431:INFO:_display_container: 2
2024-09-29 15:39:42,431:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-29 15:39:42,431:INFO:create_model() successfully completed......................................
2024-09-29 15:39:42,503:INFO:SubProcess create_model() end ==================================
2024-09-29 15:39:42,503:INFO:Creating metrics dataframe
2024-09-29 15:39:42,505:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning:

Styler.applymap has been deprecated. Use Styler.map instead.


2024-09-29 15:39:42,507:INFO:Initializing create_model()
2024-09-29 15:39:42,507:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:39:42,507:INFO:Checking exceptions
2024-09-29 15:39:42,507:INFO:Importing libraries
2024-09-29 15:39:42,507:INFO:Copying training dataset
2024-09-29 15:39:42,681:INFO:Defining folds
2024-09-29 15:39:42,681:INFO:Declaring metric variables
2024-09-29 15:39:42,682:INFO:Importing untrained model
2024-09-29 15:39:42,682:INFO:Declaring custom model
2024-09-29 15:39:42,682:INFO:Extra Trees Classifier Imported successfully
2024-09-29 15:39:42,684:INFO:Cross validation set to False
2024-09-29 15:39:42,685:INFO:Fitting Model
2024-09-29 15:39:43,040:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:39:43,040:INFO:create_model() successfully completed......................................
2024-09-29 15:39:43,119:INFO:_master_model_container: 14
2024-09-29 15:39:43,119:INFO:_display_container: 2
2024-09-29 15:39:43,119:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:39:43,119:INFO:compare_models() successfully completed......................................
2024-09-29 15:39:43,119:INFO:Initializing finalize_model()
2024-09-29 15:39:43,119:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-29 15:39:43,120:INFO:Finalizing ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:39:43,246:INFO:Initializing create_model()
2024-09-29 15:39:43,246:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:39:43,246:INFO:Checking exceptions
2024-09-29 15:39:43,247:INFO:Importing libraries
2024-09-29 15:39:43,247:INFO:Copying training dataset
2024-09-29 15:39:43,263:INFO:Defining folds
2024-09-29 15:39:43,263:INFO:Declaring metric variables
2024-09-29 15:39:43,263:INFO:Importing untrained model
2024-09-29 15:39:43,263:INFO:Declaring custom model
2024-09-29 15:39:43,264:INFO:Extra Trees Classifier Imported successfully
2024-09-29 15:39:43,266:INFO:Cross validation set to False
2024-09-29 15:39:43,266:INFO:Fitting Model
2024-09-29 15:39:43,688:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-29 15:39:43,688:INFO:create_model() successfully completed......................................
2024-09-29 15:39:43,760:INFO:_master_model_container: 14
2024-09-29 15:39:43,760:INFO:_display_container: 2
2024-09-29 15:39:43,768:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-29 15:39:43,769:INFO:finalize_model() successfully completed......................................
2024-09-29 15:39:43,842:INFO:Initializing predict_model()
2024-09-29 15:39:43,842:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021D84F247C0>)
2024-09-29 15:39:43,842:INFO:Checking exceptions
2024-09-29 15:39:43,842:INFO:Preloading libraries
2024-09-29 15:39:43,843:INFO:Set up data.
2024-09-29 15:39:43,990:INFO:Set up index.
2024-09-29 15:39:44,174:INFO:Initializing plot_model()
2024-09-29 15:39:44,175:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021DBECCFD10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-29 15:39:44,175:INFO:Checking exceptions
2024-09-29 15:39:44,253:INFO:Preloading libraries
2024-09-29 15:39:44,276:INFO:Copying training dataset
2024-09-29 15:39:44,276:INFO:Plot type: feature
2024-09-29 15:39:44,276:WARNING:No coef_ found. Trying feature_importances_
2024-09-29 15:39:44,740:INFO:Visual Rendered Successfully
2024-09-29 15:39:44,805:INFO:plot_model() successfully completed......................................
2024-09-29 15:52:22,525:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:52:22,525:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:52:22,525:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:52:22,526:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 15:52:24,522:INFO:PyCaret ClassificationExperiment
2024-09-29 15:52:24,522:INFO:Logging name: clf-default-name
2024-09-29 15:52:24,522:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-29 15:52:24,522:INFO:version 3.3.2
2024-09-29 15:52:24,522:INFO:Initializing setup()
2024-09-29 15:52:24,522:INFO:self.USI: 156d
2024-09-29 15:52:24,522:INFO:self._variable_keys: {'gpu_n_jobs_param', 'X_train', 'X', 'y_train', 'fold_shuffle_param', 'exp_id', 'fix_imbalance', 'X_test', '_available_plots', 'fold_generator', '_ml_usecase', 'n_jobs_param', 'data', 'fold_groups_param', 'logging_param', 'idx', 'y', 'target_param', 'pipeline', 'log_plots_param', 'USI', 'exp_name_log', 'memory', 'html_param', 'gpu_param', 'seed', 'is_multiclass', 'y_test'}
2024-09-29 15:52:24,522:INFO:Checking environment
2024-09-29 15:52:24,522:INFO:python_version: 3.11.6
2024-09-29 15:52:24,522:INFO:python_build: ('tags/v3.11.6:8b6ee5b', 'Oct  2 2023 14:57:12')
2024-09-29 15:52:24,522:INFO:machine: AMD64
2024-09-29 15:52:24,531:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-29 15:52:24,533:INFO:Memory: svmem(total=17113423872, available=8389308416, percent=51.0, used=8724115456, free=8389308416)
2024-09-29 15:52:24,533:INFO:Physical Core: 6
2024-09-29 15:52:24,533:INFO:Logical Core: 12
2024-09-29 15:52:24,533:INFO:Checking libraries
2024-09-29 15:52:24,533:INFO:System:
2024-09-29 15:52:24,533:INFO:    python: 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]
2024-09-29 15:52:24,533:INFO:executable: C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Scripts\python.exe
2024-09-29 15:52:24,533:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-29 15:52:24,533:INFO:PyCaret required dependencies:
2024-09-29 15:52:24,554:INFO:                 pip: 23.2.1
2024-09-29 15:52:24,554:INFO:          setuptools: 68.2.0
2024-09-29 15:52:24,554:INFO:             pycaret: 3.3.2
2024-09-29 15:52:24,554:INFO:             IPython: 8.27.0
2024-09-29 15:52:24,554:INFO:          ipywidgets: 8.1.5
2024-09-29 15:52:24,554:INFO:                tqdm: 4.66.5
2024-09-29 15:52:24,554:INFO:               numpy: 1.26.4
2024-09-29 15:52:24,554:INFO:              pandas: 2.1.4
2024-09-29 15:52:24,554:INFO:              jinja2: 3.1.4
2024-09-29 15:52:24,554:INFO:               scipy: 1.11.4
2024-09-29 15:52:24,554:INFO:              joblib: 1.3.2
2024-09-29 15:52:24,554:INFO:             sklearn: 1.4.2
2024-09-29 15:52:24,554:INFO:                pyod: 2.0.2
2024-09-29 15:52:24,554:INFO:            imblearn: 0.12.3
2024-09-29 15:52:24,554:INFO:   category_encoders: 2.6.3
2024-09-29 15:52:24,554:INFO:            lightgbm: 4.5.0
2024-09-29 15:52:24,554:INFO:               numba: 0.60.0
2024-09-29 15:52:24,554:INFO:            requests: 2.32.3
2024-09-29 15:52:24,554:INFO:          matplotlib: 3.7.5
2024-09-29 15:52:24,554:INFO:          scikitplot: 0.3.7
2024-09-29 15:52:24,555:INFO:         yellowbrick: 1.5
2024-09-29 15:52:24,555:INFO:              plotly: 5.24.1
2024-09-29 15:52:24,555:INFO:    plotly-resampler: Not installed
2024-09-29 15:52:24,555:INFO:             kaleido: 0.2.1
2024-09-29 15:52:24,555:INFO:           schemdraw: 0.15
2024-09-29 15:52:24,555:INFO:         statsmodels: 0.14.3
2024-09-29 15:52:24,555:INFO:              sktime: 0.26.0
2024-09-29 15:52:24,555:INFO:               tbats: 1.1.3
2024-09-29 15:52:24,555:INFO:            pmdarima: 2.0.4
2024-09-29 15:52:24,555:INFO:              psutil: 6.0.0
2024-09-29 15:52:24,555:INFO:          markupsafe: 2.1.5
2024-09-29 15:52:24,555:INFO:             pickle5: Not installed
2024-09-29 15:52:24,555:INFO:         cloudpickle: 3.0.0
2024-09-29 15:52:24,555:INFO:         deprecation: 2.1.0
2024-09-29 15:52:24,555:INFO:              xxhash: 3.5.0
2024-09-29 15:52:24,555:INFO:           wurlitzer: Not installed
2024-09-29 15:52:24,555:INFO:PyCaret optional dependencies:
2024-09-29 15:52:24,570:INFO:                shap: Not installed
2024-09-29 15:52:24,570:INFO:           interpret: Not installed
2024-09-29 15:52:24,570:INFO:                umap: Not installed
2024-09-29 15:52:24,570:INFO:     ydata_profiling: Not installed
2024-09-29 15:52:24,570:INFO:  explainerdashboard: Not installed
2024-09-29 15:52:24,570:INFO:             autoviz: Not installed
2024-09-29 15:52:24,570:INFO:           fairlearn: Not installed
2024-09-29 15:52:24,571:INFO:          deepchecks: Not installed
2024-09-29 15:52:24,571:INFO:             xgboost: Not installed
2024-09-29 15:52:24,571:INFO:            catboost: Not installed
2024-09-29 15:52:24,571:INFO:              kmodes: Not installed
2024-09-29 15:52:24,571:INFO:             mlxtend: Not installed
2024-09-29 15:52:24,571:INFO:       statsforecast: Not installed
2024-09-29 15:52:24,571:INFO:        tune_sklearn: Not installed
2024-09-29 15:52:24,571:INFO:                 ray: Not installed
2024-09-29 15:52:24,571:INFO:            hyperopt: Not installed
2024-09-29 15:52:24,571:INFO:              optuna: Not installed
2024-09-29 15:52:24,571:INFO:               skopt: Not installed
2024-09-29 15:52:24,571:INFO:              mlflow: 2.16.2
2024-09-29 15:52:24,571:INFO:              gradio: Not installed
2024-09-29 15:52:24,571:INFO:             fastapi: Not installed
2024-09-29 15:52:24,571:INFO:             uvicorn: Not installed
2024-09-29 15:52:24,571:INFO:              m2cgen: Not installed
2024-09-29 15:52:24,571:INFO:           evidently: Not installed
2024-09-29 15:52:24,571:INFO:               fugue: Not installed
2024-09-29 15:52:24,571:INFO:           streamlit: Not installed
2024-09-29 15:52:24,571:INFO:             prophet: Not installed
2024-09-29 15:52:24,571:INFO:None
2024-09-29 15:52:24,571:INFO:Set up data.
2024-09-29 15:52:24,776:INFO:Set up folding strategy.
2024-09-29 15:52:24,776:INFO:Set up train/test split.
2024-09-29 15:52:24,983:INFO:Set up index.
2024-09-29 15:52:24,987:INFO:Assigning column types.
2024-09-29 15:52:25,176:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-29 15:52:25,216:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-29 15:52:25,219:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:52:25,249:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,249:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,289:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-29 15:52:25,290:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:52:25,315:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,315:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,315:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-29 15:52:25,356:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:52:25,381:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,382:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,423:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 15:52:25,448:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,448:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,448:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-29 15:52:25,515:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,515:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,582:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,582:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:25,583:INFO:Preparing preprocessing pipeline...
2024-09-29 15:52:25,610:INFO:Set up simple imputation.
2024-09-29 15:52:26,069:INFO:Finished creating preprocessing pipeline.
2024-09-29 15:52:26,076:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-29 15:52:26,076:INFO:Creating final display dataframe.
2024-09-29 15:52:27,162:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape      (3156, 1001)
4        Transformed data shape      (3156, 1001)
5   Transformed train set shape      (2209, 1001)
6    Transformed test set shape       (947, 1001)
7              Numeric features              1000
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              156d
2024-09-29 15:52:27,232:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:27,232:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:27,299:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:27,299:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 15:52:27,301:INFO:setup() successfully completed in 2.78s...............
2024-09-29 15:52:27,301:INFO:Initializing compare_models()
2024-09-29 15:52:27,301:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-29 15:52:27,301:INFO:Checking exceptions
2024-09-29 15:52:27,482:INFO:Preparing display monitor
2024-09-29 15:52:27,485:INFO:Initializing Logistic Regression
2024-09-29 15:52:27,485:INFO:Total runtime is 0.0 minutes
2024-09-29 15:52:27,485:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:27,485:INFO:Initializing create_model()
2024-09-29 15:52:27,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:27,485:INFO:Checking exceptions
2024-09-29 15:52:27,486:INFO:Importing libraries
2024-09-29 15:52:27,486:INFO:Copying training dataset
2024-09-29 15:52:27,660:INFO:Defining folds
2024-09-29 15:52:27,660:INFO:Declaring metric variables
2024-09-29 15:52:27,660:INFO:Importing untrained model
2024-09-29 15:52:27,661:INFO:Logistic Regression Imported successfully
2024-09-29 15:52:27,661:INFO:Starting cross validation
2024-09-29 15:52:27,663:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:52:32,892:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,112:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,221:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,407:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,430:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,495:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,560:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,588:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,684:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,689:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:33,706:INFO:Calculating mean and std
2024-09-29 15:52:33,707:INFO:Creating metrics dataframe
2024-09-29 15:52:33,709:INFO:Uploading results into container
2024-09-29 15:52:33,710:INFO:Uploading model into container now
2024-09-29 15:52:33,710:INFO:_master_model_container: 1
2024-09-29 15:52:33,710:INFO:_display_container: 2
2024-09-29 15:52:33,710:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-29 15:52:33,710:INFO:create_model() successfully completed......................................
2024-09-29 15:52:33,791:INFO:SubProcess create_model() end ==================================
2024-09-29 15:52:33,791:INFO:Creating metrics dataframe
2024-09-29 15:52:33,793:INFO:Initializing K Neighbors Classifier
2024-09-29 15:52:33,793:INFO:Total runtime is 0.10513652165730794 minutes
2024-09-29 15:52:33,793:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:33,793:INFO:Initializing create_model()
2024-09-29 15:52:33,793:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:33,793:INFO:Checking exceptions
2024-09-29 15:52:33,793:INFO:Importing libraries
2024-09-29 15:52:33,793:INFO:Copying training dataset
2024-09-29 15:52:33,963:INFO:Defining folds
2024-09-29 15:52:33,964:INFO:Declaring metric variables
2024-09-29 15:52:33,964:INFO:Importing untrained model
2024-09-29 15:52:33,964:INFO:K Neighbors Classifier Imported successfully
2024-09-29 15:52:33,964:INFO:Starting cross validation
2024-09-29 15:52:33,966:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:52:35,803:INFO:Calculating mean and std
2024-09-29 15:52:35,803:INFO:Creating metrics dataframe
2024-09-29 15:52:35,805:INFO:Uploading results into container
2024-09-29 15:52:35,805:INFO:Uploading model into container now
2024-09-29 15:52:35,805:INFO:_master_model_container: 2
2024-09-29 15:52:35,805:INFO:_display_container: 2
2024-09-29 15:52:35,806:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-29 15:52:35,806:INFO:create_model() successfully completed......................................
2024-09-29 15:52:35,870:INFO:SubProcess create_model() end ==================================
2024-09-29 15:52:35,870:INFO:Creating metrics dataframe
2024-09-29 15:52:35,873:INFO:Initializing Naive Bayes
2024-09-29 15:52:35,873:INFO:Total runtime is 0.13980621496836343 minutes
2024-09-29 15:52:35,873:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:35,873:INFO:Initializing create_model()
2024-09-29 15:52:35,873:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:35,873:INFO:Checking exceptions
2024-09-29 15:52:35,873:INFO:Importing libraries
2024-09-29 15:52:35,873:INFO:Copying training dataset
2024-09-29 15:52:36,041:INFO:Defining folds
2024-09-29 15:52:36,041:INFO:Declaring metric variables
2024-09-29 15:52:36,041:INFO:Importing untrained model
2024-09-29 15:52:36,042:INFO:Naive Bayes Imported successfully
2024-09-29 15:52:36,042:INFO:Starting cross validation
2024-09-29 15:52:36,044:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:52:36,902:INFO:Calculating mean and std
2024-09-29 15:52:36,902:INFO:Creating metrics dataframe
2024-09-29 15:52:36,904:INFO:Uploading results into container
2024-09-29 15:52:36,904:INFO:Uploading model into container now
2024-09-29 15:52:36,905:INFO:_master_model_container: 3
2024-09-29 15:52:36,905:INFO:_display_container: 2
2024-09-29 15:52:36,905:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-29 15:52:36,905:INFO:create_model() successfully completed......................................
2024-09-29 15:52:36,969:INFO:SubProcess create_model() end ==================================
2024-09-29 15:52:36,969:INFO:Creating metrics dataframe
2024-09-29 15:52:36,971:INFO:Initializing Decision Tree Classifier
2024-09-29 15:52:36,971:INFO:Total runtime is 0.1580976088841756 minutes
2024-09-29 15:52:36,972:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:36,972:INFO:Initializing create_model()
2024-09-29 15:52:36,972:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:36,972:INFO:Checking exceptions
2024-09-29 15:52:36,972:INFO:Importing libraries
2024-09-29 15:52:36,972:INFO:Copying training dataset
2024-09-29 15:52:37,139:INFO:Defining folds
2024-09-29 15:52:37,139:INFO:Declaring metric variables
2024-09-29 15:52:37,139:INFO:Importing untrained model
2024-09-29 15:52:37,140:INFO:Decision Tree Classifier Imported successfully
2024-09-29 15:52:37,140:INFO:Starting cross validation
2024-09-29 15:52:37,142:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:52:38,518:INFO:Calculating mean and std
2024-09-29 15:52:38,518:INFO:Creating metrics dataframe
2024-09-29 15:52:38,520:INFO:Uploading results into container
2024-09-29 15:52:38,520:INFO:Uploading model into container now
2024-09-29 15:52:38,520:INFO:_master_model_container: 4
2024-09-29 15:52:38,520:INFO:_display_container: 2
2024-09-29 15:52:38,521:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-29 15:52:38,521:INFO:create_model() successfully completed......................................
2024-09-29 15:52:38,582:INFO:SubProcess create_model() end ==================================
2024-09-29 15:52:38,582:INFO:Creating metrics dataframe
2024-09-29 15:52:38,584:INFO:Initializing SVM - Linear Kernel
2024-09-29 15:52:38,585:INFO:Total runtime is 0.18500259717305498 minutes
2024-09-29 15:52:38,585:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:38,585:INFO:Initializing create_model()
2024-09-29 15:52:38,585:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:38,585:INFO:Checking exceptions
2024-09-29 15:52:38,585:INFO:Importing libraries
2024-09-29 15:52:38,585:INFO:Copying training dataset
2024-09-29 15:52:38,770:INFO:Defining folds
2024-09-29 15:52:38,770:INFO:Declaring metric variables
2024-09-29 15:52:38,770:INFO:Importing untrained model
2024-09-29 15:52:38,771:INFO:SVM - Linear Kernel Imported successfully
2024-09-29 15:52:38,771:INFO:Starting cross validation
2024-09-29 15:52:38,773:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:52:39,323:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,395:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,444:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,512:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,695:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,699:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,717:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,748:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,812:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,875:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:39,886:INFO:Calculating mean and std
2024-09-29 15:52:39,886:INFO:Creating metrics dataframe
2024-09-29 15:52:39,888:INFO:Uploading results into container
2024-09-29 15:52:39,888:INFO:Uploading model into container now
2024-09-29 15:52:39,888:INFO:_master_model_container: 5
2024-09-29 15:52:39,888:INFO:_display_container: 2
2024-09-29 15:52:39,889:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-29 15:52:39,889:INFO:create_model() successfully completed......................................
2024-09-29 15:52:39,953:INFO:SubProcess create_model() end ==================================
2024-09-29 15:52:39,953:INFO:Creating metrics dataframe
2024-09-29 15:52:39,955:INFO:Initializing Ridge Classifier
2024-09-29 15:52:39,955:INFO:Total runtime is 0.2078381021817525 minutes
2024-09-29 15:52:39,955:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:39,956:INFO:Initializing create_model()
2024-09-29 15:52:39,956:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:39,956:INFO:Checking exceptions
2024-09-29 15:52:39,956:INFO:Importing libraries
2024-09-29 15:52:39,956:INFO:Copying training dataset
2024-09-29 15:52:40,124:INFO:Defining folds
2024-09-29 15:52:40,124:INFO:Declaring metric variables
2024-09-29 15:52:40,124:INFO:Importing untrained model
2024-09-29 15:52:40,124:INFO:Ridge Classifier Imported successfully
2024-09-29 15:52:40,125:INFO:Starting cross validation
2024-09-29 15:52:40,127:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:52:40,476:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:40,527:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:40,612:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:40,744:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:40,778:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:40,864:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:40,878:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:40,950:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:40,977:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:41,059:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:41,075:INFO:Calculating mean and std
2024-09-29 15:52:41,075:INFO:Creating metrics dataframe
2024-09-29 15:52:41,077:INFO:Uploading results into container
2024-09-29 15:52:41,077:INFO:Uploading model into container now
2024-09-29 15:52:41,078:INFO:_master_model_container: 6
2024-09-29 15:52:41,078:INFO:_display_container: 2
2024-09-29 15:52:41,078:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-29 15:52:41,078:INFO:create_model() successfully completed......................................
2024-09-29 15:52:41,140:INFO:SubProcess create_model() end ==================================
2024-09-29 15:52:41,140:INFO:Creating metrics dataframe
2024-09-29 15:52:41,142:INFO:Initializing Random Forest Classifier
2024-09-29 15:52:41,142:INFO:Total runtime is 0.22761452992757158 minutes
2024-09-29 15:52:41,142:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:41,142:INFO:Initializing create_model()
2024-09-29 15:52:41,142:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:41,142:INFO:Checking exceptions
2024-09-29 15:52:41,142:INFO:Importing libraries
2024-09-29 15:52:41,142:INFO:Copying training dataset
2024-09-29 15:52:41,316:INFO:Defining folds
2024-09-29 15:52:41,316:INFO:Declaring metric variables
2024-09-29 15:52:41,316:INFO:Importing untrained model
2024-09-29 15:52:41,316:INFO:Random Forest Classifier Imported successfully
2024-09-29 15:52:41,316:INFO:Starting cross validation
2024-09-29 15:52:41,319:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:52:43,829:INFO:Calculating mean and std
2024-09-29 15:52:43,829:INFO:Creating metrics dataframe
2024-09-29 15:52:43,831:INFO:Uploading results into container
2024-09-29 15:52:43,832:INFO:Uploading model into container now
2024-09-29 15:52:43,832:INFO:_master_model_container: 7
2024-09-29 15:52:43,832:INFO:_display_container: 2
2024-09-29 15:52:43,832:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-29 15:52:43,832:INFO:create_model() successfully completed......................................
2024-09-29 15:52:43,901:INFO:SubProcess create_model() end ==================================
2024-09-29 15:52:43,901:INFO:Creating metrics dataframe
2024-09-29 15:52:43,904:INFO:Initializing Quadratic Discriminant Analysis
2024-09-29 15:52:43,904:INFO:Total runtime is 0.27364571094512935 minutes
2024-09-29 15:52:43,904:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:43,904:INFO:Initializing create_model()
2024-09-29 15:52:43,904:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:43,904:INFO:Checking exceptions
2024-09-29 15:52:43,904:INFO:Importing libraries
2024-09-29 15:52:43,905:INFO:Copying training dataset
2024-09-29 15:52:44,089:INFO:Defining folds
2024-09-29 15:52:44,089:INFO:Declaring metric variables
2024-09-29 15:52:44,089:INFO:Importing untrained model
2024-09-29 15:52:44,089:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-29 15:52:44,089:INFO:Starting cross validation
2024-09-29 15:52:44,092:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:52:44,559:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:44,674:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:44,803:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:44,932:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:45,094:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:45,260:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:45,467:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:45,949:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:45,980:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:46,307:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:46,323:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:46,324:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:46,382:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 15:52:46,492:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:46,688:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:46,698:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:46,847:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:46,958:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:46,964:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:47,058:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:47,069:INFO:Calculating mean and std
2024-09-29 15:52:47,069:INFO:Creating metrics dataframe
2024-09-29 15:52:47,071:INFO:Uploading results into container
2024-09-29 15:52:47,071:INFO:Uploading model into container now
2024-09-29 15:52:47,071:INFO:_master_model_container: 8
2024-09-29 15:52:47,072:INFO:_display_container: 2
2024-09-29 15:52:47,072:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-29 15:52:47,072:INFO:create_model() successfully completed......................................
2024-09-29 15:52:47,137:INFO:SubProcess create_model() end ==================================
2024-09-29 15:52:47,137:INFO:Creating metrics dataframe
2024-09-29 15:52:47,139:INFO:Initializing Ada Boost Classifier
2024-09-29 15:52:47,139:INFO:Total runtime is 0.3275750637054443 minutes
2024-09-29 15:52:47,139:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:47,140:INFO:Initializing create_model()
2024-09-29 15:52:47,140:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:47,140:INFO:Checking exceptions
2024-09-29 15:52:47,140:INFO:Importing libraries
2024-09-29 15:52:47,140:INFO:Copying training dataset
2024-09-29 15:52:47,308:INFO:Defining folds
2024-09-29 15:52:47,309:INFO:Declaring metric variables
2024-09-29 15:52:47,309:INFO:Importing untrained model
2024-09-29 15:52:47,309:INFO:Ada Boost Classifier Imported successfully
2024-09-29 15:52:47,309:INFO:Starting cross validation
2024-09-29 15:52:47,311:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:52:47,584:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:47,695:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:47,890:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:47,962:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:48,041:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:48,114:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:48,180:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:48,245:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:48,304:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:48,357:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 15:52:49,891:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:49,945:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:50,123:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:50,201:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:50,230:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:50,292:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:50,336:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:50,396:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:50,434:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:50,448:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:52:50,461:INFO:Calculating mean and std
2024-09-29 15:52:50,461:INFO:Creating metrics dataframe
2024-09-29 15:52:50,463:INFO:Uploading results into container
2024-09-29 15:52:50,463:INFO:Uploading model into container now
2024-09-29 15:52:50,463:INFO:_master_model_container: 9
2024-09-29 15:52:50,463:INFO:_display_container: 2
2024-09-29 15:52:50,464:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-29 15:52:50,464:INFO:create_model() successfully completed......................................
2024-09-29 15:52:50,534:INFO:SubProcess create_model() end ==================================
2024-09-29 15:52:50,534:INFO:Creating metrics dataframe
2024-09-29 15:52:50,536:INFO:Initializing Gradient Boosting Classifier
2024-09-29 15:52:50,536:INFO:Total runtime is 0.3841797232627868 minutes
2024-09-29 15:52:50,536:INFO:SubProcess create_model() called ==================================
2024-09-29 15:52:50,536:INFO:Initializing create_model()
2024-09-29 15:52:50,536:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:52:50,536:INFO:Checking exceptions
2024-09-29 15:52:50,536:INFO:Importing libraries
2024-09-29 15:52:50,537:INFO:Copying training dataset
2024-09-29 15:52:50,712:INFO:Defining folds
2024-09-29 15:52:50,712:INFO:Declaring metric variables
2024-09-29 15:52:50,712:INFO:Importing untrained model
2024-09-29 15:52:50,713:INFO:Gradient Boosting Classifier Imported successfully
2024-09-29 15:52:50,713:INFO:Starting cross validation
2024-09-29 15:52:50,715:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:53:30,241:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:30,487:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:30,659:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:31,102:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:31,119:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:31,201:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:31,218:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:31,244:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:31,276:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:31,355:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:31,371:INFO:Calculating mean and std
2024-09-29 15:53:31,371:INFO:Creating metrics dataframe
2024-09-29 15:53:31,373:INFO:Uploading results into container
2024-09-29 15:53:31,373:INFO:Uploading model into container now
2024-09-29 15:53:31,374:INFO:_master_model_container: 10
2024-09-29 15:53:31,374:INFO:_display_container: 2
2024-09-29 15:53:31,374:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-29 15:53:31,374:INFO:create_model() successfully completed......................................
2024-09-29 15:53:31,441:INFO:SubProcess create_model() end ==================================
2024-09-29 15:53:31,441:INFO:Creating metrics dataframe
2024-09-29 15:53:31,443:INFO:Initializing Linear Discriminant Analysis
2024-09-29 15:53:31,443:INFO:Total runtime is 1.0659724315007528 minutes
2024-09-29 15:53:31,443:INFO:SubProcess create_model() called ==================================
2024-09-29 15:53:31,443:INFO:Initializing create_model()
2024-09-29 15:53:31,443:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:53:31,443:INFO:Checking exceptions
2024-09-29 15:53:31,443:INFO:Importing libraries
2024-09-29 15:53:31,444:INFO:Copying training dataset
2024-09-29 15:53:31,623:INFO:Defining folds
2024-09-29 15:53:31,623:INFO:Declaring metric variables
2024-09-29 15:53:31,624:INFO:Importing untrained model
2024-09-29 15:53:31,624:INFO:Linear Discriminant Analysis Imported successfully
2024-09-29 15:53:31,624:INFO:Starting cross validation
2024-09-29 15:53:31,627:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:53:33,266:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:33,508:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:33,689:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:33,807:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:33,976:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:34,087:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:34,157:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:34,257:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:34,314:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:34,317:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 15:53:34,341:INFO:Calculating mean and std
2024-09-29 15:53:34,341:INFO:Creating metrics dataframe
2024-09-29 15:53:34,343:INFO:Uploading results into container
2024-09-29 15:53:34,343:INFO:Uploading model into container now
2024-09-29 15:53:34,344:INFO:_master_model_container: 11
2024-09-29 15:53:34,344:INFO:_display_container: 2
2024-09-29 15:53:34,344:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-29 15:53:34,344:INFO:create_model() successfully completed......................................
2024-09-29 15:53:34,431:INFO:SubProcess create_model() end ==================================
2024-09-29 15:53:34,431:INFO:Creating metrics dataframe
2024-09-29 15:53:34,433:INFO:Initializing Extra Trees Classifier
2024-09-29 15:53:34,433:INFO:Total runtime is 1.115804171562195 minutes
2024-09-29 15:53:34,433:INFO:SubProcess create_model() called ==================================
2024-09-29 15:53:34,434:INFO:Initializing create_model()
2024-09-29 15:53:34,434:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:53:34,434:INFO:Checking exceptions
2024-09-29 15:53:34,434:INFO:Importing libraries
2024-09-29 15:53:34,434:INFO:Copying training dataset
2024-09-29 15:53:34,666:INFO:Defining folds
2024-09-29 15:53:34,666:INFO:Declaring metric variables
2024-09-29 15:53:34,667:INFO:Importing untrained model
2024-09-29 15:53:34,667:INFO:Extra Trees Classifier Imported successfully
2024-09-29 15:53:34,667:INFO:Starting cross validation
2024-09-29 15:53:34,669:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:53:37,640:INFO:Calculating mean and std
2024-09-29 15:53:37,641:INFO:Creating metrics dataframe
2024-09-29 15:53:37,642:INFO:Uploading results into container
2024-09-29 15:53:37,642:INFO:Uploading model into container now
2024-09-29 15:53:37,642:INFO:_master_model_container: 12
2024-09-29 15:53:37,643:INFO:_display_container: 2
2024-09-29 15:53:37,643:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:53:37,643:INFO:create_model() successfully completed......................................
2024-09-29 15:53:37,719:INFO:SubProcess create_model() end ==================================
2024-09-29 15:53:37,719:INFO:Creating metrics dataframe
2024-09-29 15:53:37,721:INFO:Initializing Light Gradient Boosting Machine
2024-09-29 15:53:37,721:INFO:Total runtime is 1.1706099629402162 minutes
2024-09-29 15:53:37,722:INFO:SubProcess create_model() called ==================================
2024-09-29 15:53:37,722:INFO:Initializing create_model()
2024-09-29 15:53:37,722:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:53:37,722:INFO:Checking exceptions
2024-09-29 15:53:37,722:INFO:Importing libraries
2024-09-29 15:53:37,722:INFO:Copying training dataset
2024-09-29 15:53:37,904:INFO:Defining folds
2024-09-29 15:53:37,904:INFO:Declaring metric variables
2024-09-29 15:53:37,904:INFO:Importing untrained model
2024-09-29 15:53:37,905:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-29 15:53:37,905:INFO:Starting cross validation
2024-09-29 15:53:37,907:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:54:04,638:INFO:Calculating mean and std
2024-09-29 15:54:04,639:INFO:Creating metrics dataframe
2024-09-29 15:54:04,641:INFO:Uploading results into container
2024-09-29 15:54:04,641:INFO:Uploading model into container now
2024-09-29 15:54:04,641:INFO:_master_model_container: 13
2024-09-29 15:54:04,642:INFO:_display_container: 2
2024-09-29 15:54:04,642:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-29 15:54:04,642:INFO:create_model() successfully completed......................................
2024-09-29 15:54:04,732:INFO:SubProcess create_model() end ==================================
2024-09-29 15:54:04,732:INFO:Creating metrics dataframe
2024-09-29 15:54:04,735:INFO:Initializing Dummy Classifier
2024-09-29 15:54:04,735:INFO:Total runtime is 1.620831088225047 minutes
2024-09-29 15:54:04,735:INFO:SubProcess create_model() called ==================================
2024-09-29 15:54:04,736:INFO:Initializing create_model()
2024-09-29 15:54:04,736:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D3FB0E7650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:54:04,736:INFO:Checking exceptions
2024-09-29 15:54:04,736:INFO:Importing libraries
2024-09-29 15:54:04,736:INFO:Copying training dataset
2024-09-29 15:54:04,951:INFO:Defining folds
2024-09-29 15:54:04,951:INFO:Declaring metric variables
2024-09-29 15:54:04,951:INFO:Importing untrained model
2024-09-29 15:54:04,951:INFO:Dummy Classifier Imported successfully
2024-09-29 15:54:04,951:INFO:Starting cross validation
2024-09-29 15:54:04,954:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 15:54:05,236:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,330:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,474:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,509:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,680:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,734:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,823:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,860:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,908:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,936:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 15:54:05,947:INFO:Calculating mean and std
2024-09-29 15:54:05,948:INFO:Creating metrics dataframe
2024-09-29 15:54:05,949:INFO:Uploading results into container
2024-09-29 15:54:05,950:INFO:Uploading model into container now
2024-09-29 15:54:05,950:INFO:_master_model_container: 14
2024-09-29 15:54:05,950:INFO:_display_container: 2
2024-09-29 15:54:05,950:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-29 15:54:05,950:INFO:create_model() successfully completed......................................
2024-09-29 15:54:06,032:INFO:SubProcess create_model() end ==================================
2024-09-29 15:54:06,032:INFO:Creating metrics dataframe
2024-09-29 15:54:06,034:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning:

Styler.applymap has been deprecated. Use Styler.map instead.


2024-09-29 15:54:06,036:INFO:Initializing create_model()
2024-09-29 15:54:06,036:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:54:06,036:INFO:Checking exceptions
2024-09-29 15:54:06,037:INFO:Importing libraries
2024-09-29 15:54:06,037:INFO:Copying training dataset
2024-09-29 15:54:06,233:INFO:Defining folds
2024-09-29 15:54:06,233:INFO:Declaring metric variables
2024-09-29 15:54:06,233:INFO:Importing untrained model
2024-09-29 15:54:06,233:INFO:Declaring custom model
2024-09-29 15:54:06,234:INFO:Extra Trees Classifier Imported successfully
2024-09-29 15:54:06,236:INFO:Cross validation set to False
2024-09-29 15:54:06,236:INFO:Fitting Model
2024-09-29 15:54:06,600:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:54:06,600:INFO:create_model() successfully completed......................................
2024-09-29 15:54:06,710:INFO:_master_model_container: 14
2024-09-29 15:54:06,710:INFO:_display_container: 2
2024-09-29 15:54:06,710:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:54:06,710:INFO:compare_models() successfully completed......................................
2024-09-29 15:54:06,711:INFO:Initializing finalize_model()
2024-09-29 15:54:06,711:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-29 15:54:06,711:INFO:Finalizing ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 15:54:06,846:INFO:Initializing create_model()
2024-09-29 15:54:06,846:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 15:54:06,846:INFO:Checking exceptions
2024-09-29 15:54:06,847:INFO:Importing libraries
2024-09-29 15:54:06,848:INFO:Copying training dataset
2024-09-29 15:54:06,868:INFO:Defining folds
2024-09-29 15:54:06,868:INFO:Declaring metric variables
2024-09-29 15:54:06,868:INFO:Importing untrained model
2024-09-29 15:54:06,869:INFO:Declaring custom model
2024-09-29 15:54:06,869:INFO:Extra Trees Classifier Imported successfully
2024-09-29 15:54:06,872:INFO:Cross validation set to False
2024-09-29 15:54:06,872:INFO:Fitting Model
2024-09-29 15:54:07,378:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-29 15:54:07,379:INFO:create_model() successfully completed......................................
2024-09-29 15:54:07,454:INFO:_master_model_container: 14
2024-09-29 15:54:07,454:INFO:_display_container: 2
2024-09-29 15:54:07,462:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-29 15:54:07,462:INFO:finalize_model() successfully completed......................................
2024-09-29 15:54:07,538:INFO:Initializing predict_model()
2024-09-29 15:54:07,538:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002D3C2BA47C0>)
2024-09-29 15:54:07,538:INFO:Checking exceptions
2024-09-29 15:54:07,538:INFO:Preloading libraries
2024-09-29 15:54:07,538:INFO:Set up data.
2024-09-29 15:54:07,709:INFO:Set up index.
2024-09-29 15:54:07,916:INFO:Initializing plot_model()
2024-09-29 15:54:07,916:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D3FC94E450>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-29 15:54:07,916:INFO:Checking exceptions
2024-09-29 15:54:08,017:INFO:Preloading libraries
2024-09-29 15:54:08,043:INFO:Copying training dataset
2024-09-29 15:54:08,043:INFO:Plot type: feature
2024-09-29 15:54:08,044:WARNING:No coef_ found. Trying feature_importances_
2024-09-29 15:54:08,499:INFO:Visual Rendered Successfully
2024-09-29 15:54:08,587:INFO:plot_model() successfully completed......................................
2024-09-29 16:17:23,367:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 16:17:23,367:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 16:17:23,368:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 16:17:23,368:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-29 16:17:25,501:INFO:PyCaret ClassificationExperiment
2024-09-29 16:17:25,501:INFO:Logging name: clf-default-name
2024-09-29 16:17:25,501:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-29 16:17:25,501:INFO:version 3.3.2
2024-09-29 16:17:25,501:INFO:Initializing setup()
2024-09-29 16:17:25,501:INFO:self.USI: c8b6
2024-09-29 16:17:25,502:INFO:self._variable_keys: {'fold_generator', 'y_train', 'USI', '_ml_usecase', 'X_train', 'fix_imbalance', 'logging_param', 'log_plots_param', 'gpu_n_jobs_param', 'exp_id', 'target_param', 'fold_groups_param', 'X_test', 'fold_shuffle_param', 'gpu_param', 'X', 'y', 'html_param', '_available_plots', 'pipeline', 'exp_name_log', 'y_test', 'data', 'memory', 'seed', 'idx', 'is_multiclass', 'n_jobs_param'}
2024-09-29 16:17:25,502:INFO:Checking environment
2024-09-29 16:17:25,502:INFO:python_version: 3.11.6
2024-09-29 16:17:25,502:INFO:python_build: ('tags/v3.11.6:8b6ee5b', 'Oct  2 2023 14:57:12')
2024-09-29 16:17:25,502:INFO:machine: AMD64
2024-09-29 16:17:25,508:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-29 16:17:25,511:INFO:Memory: svmem(total=17113423872, available=7480307712, percent=56.3, used=9633116160, free=7480307712)
2024-09-29 16:17:25,511:INFO:Physical Core: 6
2024-09-29 16:17:25,511:INFO:Logical Core: 12
2024-09-29 16:17:25,511:INFO:Checking libraries
2024-09-29 16:17:25,511:INFO:System:
2024-09-29 16:17:25,511:INFO:    python: 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]
2024-09-29 16:17:25,511:INFO:executable: C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Scripts\python.exe
2024-09-29 16:17:25,511:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-29 16:17:25,511:INFO:PyCaret required dependencies:
2024-09-29 16:17:25,532:INFO:                 pip: 23.2.1
2024-09-29 16:17:25,532:INFO:          setuptools: 68.2.0
2024-09-29 16:17:25,532:INFO:             pycaret: 3.3.2
2024-09-29 16:17:25,532:INFO:             IPython: 8.27.0
2024-09-29 16:17:25,532:INFO:          ipywidgets: 8.1.5
2024-09-29 16:17:25,532:INFO:                tqdm: 4.66.5
2024-09-29 16:17:25,532:INFO:               numpy: 1.26.4
2024-09-29 16:17:25,532:INFO:              pandas: 2.1.4
2024-09-29 16:17:25,532:INFO:              jinja2: 3.1.4
2024-09-29 16:17:25,532:INFO:               scipy: 1.11.4
2024-09-29 16:17:25,532:INFO:              joblib: 1.3.2
2024-09-29 16:17:25,532:INFO:             sklearn: 1.4.2
2024-09-29 16:17:25,532:INFO:                pyod: 2.0.2
2024-09-29 16:17:25,532:INFO:            imblearn: 0.12.3
2024-09-29 16:17:25,532:INFO:   category_encoders: 2.6.3
2024-09-29 16:17:25,532:INFO:            lightgbm: 4.5.0
2024-09-29 16:17:25,532:INFO:               numba: 0.60.0
2024-09-29 16:17:25,532:INFO:            requests: 2.32.3
2024-09-29 16:17:25,532:INFO:          matplotlib: 3.7.5
2024-09-29 16:17:25,532:INFO:          scikitplot: 0.3.7
2024-09-29 16:17:25,532:INFO:         yellowbrick: 1.5
2024-09-29 16:17:25,532:INFO:              plotly: 5.24.1
2024-09-29 16:17:25,532:INFO:    plotly-resampler: Not installed
2024-09-29 16:17:25,532:INFO:             kaleido: 0.2.1
2024-09-29 16:17:25,532:INFO:           schemdraw: 0.15
2024-09-29 16:17:25,533:INFO:         statsmodels: 0.14.3
2024-09-29 16:17:25,533:INFO:              sktime: 0.26.0
2024-09-29 16:17:25,533:INFO:               tbats: 1.1.3
2024-09-29 16:17:25,533:INFO:            pmdarima: 2.0.4
2024-09-29 16:17:25,533:INFO:              psutil: 6.0.0
2024-09-29 16:17:25,533:INFO:          markupsafe: 2.1.5
2024-09-29 16:17:25,533:INFO:             pickle5: Not installed
2024-09-29 16:17:25,533:INFO:         cloudpickle: 3.0.0
2024-09-29 16:17:25,533:INFO:         deprecation: 2.1.0
2024-09-29 16:17:25,533:INFO:              xxhash: 3.5.0
2024-09-29 16:17:25,533:INFO:           wurlitzer: Not installed
2024-09-29 16:17:25,533:INFO:PyCaret optional dependencies:
2024-09-29 16:17:25,548:INFO:                shap: Not installed
2024-09-29 16:17:25,548:INFO:           interpret: Not installed
2024-09-29 16:17:25,548:INFO:                umap: Not installed
2024-09-29 16:17:25,548:INFO:     ydata_profiling: Not installed
2024-09-29 16:17:25,548:INFO:  explainerdashboard: Not installed
2024-09-29 16:17:25,548:INFO:             autoviz: Not installed
2024-09-29 16:17:25,548:INFO:           fairlearn: Not installed
2024-09-29 16:17:25,548:INFO:          deepchecks: Not installed
2024-09-29 16:17:25,548:INFO:             xgboost: Not installed
2024-09-29 16:17:25,548:INFO:            catboost: Not installed
2024-09-29 16:17:25,548:INFO:              kmodes: Not installed
2024-09-29 16:17:25,548:INFO:             mlxtend: Not installed
2024-09-29 16:17:25,549:INFO:       statsforecast: Not installed
2024-09-29 16:17:25,549:INFO:        tune_sklearn: Not installed
2024-09-29 16:17:25,549:INFO:                 ray: Not installed
2024-09-29 16:17:25,549:INFO:            hyperopt: Not installed
2024-09-29 16:17:25,549:INFO:              optuna: Not installed
2024-09-29 16:17:25,549:INFO:               skopt: Not installed
2024-09-29 16:17:25,549:INFO:              mlflow: 2.16.2
2024-09-29 16:17:25,549:INFO:              gradio: Not installed
2024-09-29 16:17:25,549:INFO:             fastapi: Not installed
2024-09-29 16:17:25,549:INFO:             uvicorn: Not installed
2024-09-29 16:17:25,549:INFO:              m2cgen: Not installed
2024-09-29 16:17:25,549:INFO:           evidently: Not installed
2024-09-29 16:17:25,549:INFO:               fugue: Not installed
2024-09-29 16:17:25,549:INFO:           streamlit: Not installed
2024-09-29 16:17:25,549:INFO:             prophet: Not installed
2024-09-29 16:17:25,549:INFO:None
2024-09-29 16:17:25,549:INFO:Set up data.
2024-09-29 16:17:25,753:INFO:Set up folding strategy.
2024-09-29 16:17:25,753:INFO:Set up train/test split.
2024-09-29 16:17:25,961:INFO:Set up index.
2024-09-29 16:17:25,965:INFO:Assigning column types.
2024-09-29 16:17:26,151:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-29 16:17:26,191:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-29 16:17:26,194:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 16:17:26,224:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,224:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,264:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-29 16:17:26,265:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 16:17:26,289:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,290:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,290:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-29 16:17:26,329:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 16:17:26,354:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,354:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,394:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-29 16:17:26,418:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,418:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,418:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-29 16:17:26,483:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,484:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,547:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,548:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:26,549:INFO:Preparing preprocessing pipeline...
2024-09-29 16:17:26,573:INFO:Set up simple imputation.
2024-09-29 16:17:27,024:INFO:Finished creating preprocessing pipeline.
2024-09-29 16:17:27,032:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-29 16:17:27,032:INFO:Creating final display dataframe.
2024-09-29 16:17:28,035:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape      (3156, 1001)
4        Transformed data shape      (3156, 1001)
5   Transformed train set shape      (2209, 1001)
6    Transformed test set shape       (947, 1001)
7              Numeric features              1000
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              c8b6
2024-09-29 16:17:28,102:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:28,102:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:28,166:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:28,166:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-29 16:17:28,167:INFO:setup() successfully completed in 2.67s...............
2024-09-29 16:17:28,167:INFO:Initializing compare_models()
2024-09-29 16:17:28,167:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-29 16:17:28,167:INFO:Checking exceptions
2024-09-29 16:17:28,330:INFO:Preparing display monitor
2024-09-29 16:17:28,333:INFO:Initializing Logistic Regression
2024-09-29 16:17:28,333:INFO:Total runtime is 0.0 minutes
2024-09-29 16:17:28,333:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:28,334:INFO:Initializing create_model()
2024-09-29 16:17:28,334:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:28,334:INFO:Checking exceptions
2024-09-29 16:17:28,334:INFO:Importing libraries
2024-09-29 16:17:28,334:INFO:Copying training dataset
2024-09-29 16:17:28,499:INFO:Defining folds
2024-09-29 16:17:28,499:INFO:Declaring metric variables
2024-09-29 16:17:28,499:INFO:Importing untrained model
2024-09-29 16:17:28,500:INFO:Logistic Regression Imported successfully
2024-09-29 16:17:28,500:INFO:Starting cross validation
2024-09-29 16:17:28,502:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:17:33,644:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:33,879:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:34,034:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:34,255:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:34,285:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:34,345:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:34,424:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:34,566:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:34,649:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:34,676:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:34,696:INFO:Calculating mean and std
2024-09-29 16:17:34,697:INFO:Creating metrics dataframe
2024-09-29 16:17:34,699:INFO:Uploading results into container
2024-09-29 16:17:34,700:INFO:Uploading model into container now
2024-09-29 16:17:34,701:INFO:_master_model_container: 1
2024-09-29 16:17:34,701:INFO:_display_container: 2
2024-09-29 16:17:34,701:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-29 16:17:34,701:INFO:create_model() successfully completed......................................
2024-09-29 16:17:34,778:INFO:SubProcess create_model() end ==================================
2024-09-29 16:17:34,778:INFO:Creating metrics dataframe
2024-09-29 16:17:34,781:INFO:Initializing K Neighbors Classifier
2024-09-29 16:17:34,781:INFO:Total runtime is 0.10746928850809732 minutes
2024-09-29 16:17:34,781:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:34,782:INFO:Initializing create_model()
2024-09-29 16:17:34,782:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:34,782:INFO:Checking exceptions
2024-09-29 16:17:34,782:INFO:Importing libraries
2024-09-29 16:17:34,782:INFO:Copying training dataset
2024-09-29 16:17:34,966:INFO:Defining folds
2024-09-29 16:17:34,966:INFO:Declaring metric variables
2024-09-29 16:17:34,967:INFO:Importing untrained model
2024-09-29 16:17:34,967:INFO:K Neighbors Classifier Imported successfully
2024-09-29 16:17:34,967:INFO:Starting cross validation
2024-09-29 16:17:34,971:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:17:37,192:INFO:Calculating mean and std
2024-09-29 16:17:37,193:INFO:Creating metrics dataframe
2024-09-29 16:17:37,195:INFO:Uploading results into container
2024-09-29 16:17:37,196:INFO:Uploading model into container now
2024-09-29 16:17:37,196:INFO:_master_model_container: 2
2024-09-29 16:17:37,196:INFO:_display_container: 2
2024-09-29 16:17:37,197:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-29 16:17:37,197:INFO:create_model() successfully completed......................................
2024-09-29 16:17:37,270:INFO:SubProcess create_model() end ==================================
2024-09-29 16:17:37,270:INFO:Creating metrics dataframe
2024-09-29 16:17:37,272:INFO:Initializing Naive Bayes
2024-09-29 16:17:37,272:INFO:Total runtime is 0.1489977518717448 minutes
2024-09-29 16:17:37,273:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:37,273:INFO:Initializing create_model()
2024-09-29 16:17:37,273:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:37,273:INFO:Checking exceptions
2024-09-29 16:17:37,273:INFO:Importing libraries
2024-09-29 16:17:37,273:INFO:Copying training dataset
2024-09-29 16:17:37,449:INFO:Defining folds
2024-09-29 16:17:37,449:INFO:Declaring metric variables
2024-09-29 16:17:37,449:INFO:Importing untrained model
2024-09-29 16:17:37,449:INFO:Naive Bayes Imported successfully
2024-09-29 16:17:37,450:INFO:Starting cross validation
2024-09-29 16:17:37,452:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:17:38,431:INFO:Calculating mean and std
2024-09-29 16:17:38,431:INFO:Creating metrics dataframe
2024-09-29 16:17:38,433:INFO:Uploading results into container
2024-09-29 16:17:38,433:INFO:Uploading model into container now
2024-09-29 16:17:38,433:INFO:_master_model_container: 3
2024-09-29 16:17:38,433:INFO:_display_container: 2
2024-09-29 16:17:38,434:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-29 16:17:38,434:INFO:create_model() successfully completed......................................
2024-09-29 16:17:38,500:INFO:SubProcess create_model() end ==================================
2024-09-29 16:17:38,500:INFO:Creating metrics dataframe
2024-09-29 16:17:38,503:INFO:Initializing Decision Tree Classifier
2024-09-29 16:17:38,503:INFO:Total runtime is 0.16951344013214112 minutes
2024-09-29 16:17:38,503:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:38,503:INFO:Initializing create_model()
2024-09-29 16:17:38,503:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:38,503:INFO:Checking exceptions
2024-09-29 16:17:38,503:INFO:Importing libraries
2024-09-29 16:17:38,503:INFO:Copying training dataset
2024-09-29 16:17:38,680:INFO:Defining folds
2024-09-29 16:17:38,680:INFO:Declaring metric variables
2024-09-29 16:17:38,680:INFO:Importing untrained model
2024-09-29 16:17:38,681:INFO:Decision Tree Classifier Imported successfully
2024-09-29 16:17:38,681:INFO:Starting cross validation
2024-09-29 16:17:38,683:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:17:40,169:INFO:Calculating mean and std
2024-09-29 16:17:40,169:INFO:Creating metrics dataframe
2024-09-29 16:17:40,171:INFO:Uploading results into container
2024-09-29 16:17:40,171:INFO:Uploading model into container now
2024-09-29 16:17:40,171:INFO:_master_model_container: 4
2024-09-29 16:17:40,171:INFO:_display_container: 2
2024-09-29 16:17:40,172:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-29 16:17:40,172:INFO:create_model() successfully completed......................................
2024-09-29 16:17:40,239:INFO:SubProcess create_model() end ==================================
2024-09-29 16:17:40,239:INFO:Creating metrics dataframe
2024-09-29 16:17:40,241:INFO:Initializing SVM - Linear Kernel
2024-09-29 16:17:40,241:INFO:Total runtime is 0.19846996466318767 minutes
2024-09-29 16:17:40,241:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:40,241:INFO:Initializing create_model()
2024-09-29 16:17:40,241:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:40,241:INFO:Checking exceptions
2024-09-29 16:17:40,241:INFO:Importing libraries
2024-09-29 16:17:40,241:INFO:Copying training dataset
2024-09-29 16:17:40,420:INFO:Defining folds
2024-09-29 16:17:40,420:INFO:Declaring metric variables
2024-09-29 16:17:40,420:INFO:Importing untrained model
2024-09-29 16:17:40,421:INFO:SVM - Linear Kernel Imported successfully
2024-09-29 16:17:40,421:INFO:Starting cross validation
2024-09-29 16:17:40,423:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:17:41,050:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,129:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,135:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,257:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,404:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,451:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,492:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,506:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,558:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,622:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:41,637:INFO:Calculating mean and std
2024-09-29 16:17:41,637:INFO:Creating metrics dataframe
2024-09-29 16:17:41,639:INFO:Uploading results into container
2024-09-29 16:17:41,639:INFO:Uploading model into container now
2024-09-29 16:17:41,639:INFO:_master_model_container: 5
2024-09-29 16:17:41,639:INFO:_display_container: 2
2024-09-29 16:17:41,640:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-29 16:17:41,640:INFO:create_model() successfully completed......................................
2024-09-29 16:17:41,707:INFO:SubProcess create_model() end ==================================
2024-09-29 16:17:41,707:INFO:Creating metrics dataframe
2024-09-29 16:17:41,709:INFO:Initializing Ridge Classifier
2024-09-29 16:17:41,709:INFO:Total runtime is 0.22293390035629274 minutes
2024-09-29 16:17:41,709:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:41,709:INFO:Initializing create_model()
2024-09-29 16:17:41,709:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:41,709:INFO:Checking exceptions
2024-09-29 16:17:41,710:INFO:Importing libraries
2024-09-29 16:17:41,710:INFO:Copying training dataset
2024-09-29 16:17:41,882:INFO:Defining folds
2024-09-29 16:17:41,882:INFO:Declaring metric variables
2024-09-29 16:17:41,882:INFO:Importing untrained model
2024-09-29 16:17:41,882:INFO:Ridge Classifier Imported successfully
2024-09-29 16:17:41,883:INFO:Starting cross validation
2024-09-29 16:17:41,885:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:17:42,250:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,316:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,380:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,525:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,555:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,671:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,698:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,728:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,831:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,874:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:42,887:INFO:Calculating mean and std
2024-09-29 16:17:42,887:INFO:Creating metrics dataframe
2024-09-29 16:17:42,889:INFO:Uploading results into container
2024-09-29 16:17:42,889:INFO:Uploading model into container now
2024-09-29 16:17:42,889:INFO:_master_model_container: 6
2024-09-29 16:17:42,890:INFO:_display_container: 2
2024-09-29 16:17:42,890:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-29 16:17:42,890:INFO:create_model() successfully completed......................................
2024-09-29 16:17:42,956:INFO:SubProcess create_model() end ==================================
2024-09-29 16:17:42,956:INFO:Creating metrics dataframe
2024-09-29 16:17:42,958:INFO:Initializing Random Forest Classifier
2024-09-29 16:17:42,958:INFO:Total runtime is 0.24376267592112225 minutes
2024-09-29 16:17:42,958:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:42,959:INFO:Initializing create_model()
2024-09-29 16:17:42,959:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:42,959:INFO:Checking exceptions
2024-09-29 16:17:42,959:INFO:Importing libraries
2024-09-29 16:17:42,959:INFO:Copying training dataset
2024-09-29 16:17:43,144:INFO:Defining folds
2024-09-29 16:17:43,144:INFO:Declaring metric variables
2024-09-29 16:17:43,144:INFO:Importing untrained model
2024-09-29 16:17:43,145:INFO:Random Forest Classifier Imported successfully
2024-09-29 16:17:43,145:INFO:Starting cross validation
2024-09-29 16:17:43,148:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:17:45,753:INFO:Calculating mean and std
2024-09-29 16:17:45,753:INFO:Creating metrics dataframe
2024-09-29 16:17:45,755:INFO:Uploading results into container
2024-09-29 16:17:45,755:INFO:Uploading model into container now
2024-09-29 16:17:45,756:INFO:_master_model_container: 7
2024-09-29 16:17:45,756:INFO:_display_container: 2
2024-09-29 16:17:45,756:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-29 16:17:45,756:INFO:create_model() successfully completed......................................
2024-09-29 16:17:45,824:INFO:SubProcess create_model() end ==================================
2024-09-29 16:17:45,824:INFO:Creating metrics dataframe
2024-09-29 16:17:45,827:INFO:Initializing Quadratic Discriminant Analysis
2024-09-29 16:17:45,827:INFO:Total runtime is 0.2915788769721985 minutes
2024-09-29 16:17:45,827:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:45,827:INFO:Initializing create_model()
2024-09-29 16:17:45,827:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:45,827:INFO:Checking exceptions
2024-09-29 16:17:45,827:INFO:Importing libraries
2024-09-29 16:17:45,827:INFO:Copying training dataset
2024-09-29 16:17:46,005:INFO:Defining folds
2024-09-29 16:17:46,005:INFO:Declaring metric variables
2024-09-29 16:17:46,005:INFO:Importing untrained model
2024-09-29 16:17:46,005:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-29 16:17:46,006:INFO:Starting cross validation
2024-09-29 16:17:46,008:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:17:46,431:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:46,573:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:46,650:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:46,746:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:46,902:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:47,056:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:47,179:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:47,336:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:47,799:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:47,829:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:47,957:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning:

Variables are collinear


2024-09-29 16:17:48,041:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:48,187:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:48,192:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:48,358:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:48,429:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:48,498:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:48,535:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:48,634:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:48,684:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:48,701:INFO:Calculating mean and std
2024-09-29 16:17:48,701:INFO:Creating metrics dataframe
2024-09-29 16:17:48,703:INFO:Uploading results into container
2024-09-29 16:17:48,703:INFO:Uploading model into container now
2024-09-29 16:17:48,704:INFO:_master_model_container: 8
2024-09-29 16:17:48,704:INFO:_display_container: 2
2024-09-29 16:17:48,704:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-29 16:17:48,704:INFO:create_model() successfully completed......................................
2024-09-29 16:17:48,769:INFO:SubProcess create_model() end ==================================
2024-09-29 16:17:48,770:INFO:Creating metrics dataframe
2024-09-29 16:17:48,772:INFO:Initializing Ada Boost Classifier
2024-09-29 16:17:48,772:INFO:Total runtime is 0.3406575918197632 minutes
2024-09-29 16:17:48,772:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:48,772:INFO:Initializing create_model()
2024-09-29 16:17:48,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:48,772:INFO:Checking exceptions
2024-09-29 16:17:48,772:INFO:Importing libraries
2024-09-29 16:17:48,772:INFO:Copying training dataset
2024-09-29 16:17:48,946:INFO:Defining folds
2024-09-29 16:17:48,946:INFO:Declaring metric variables
2024-09-29 16:17:48,946:INFO:Importing untrained model
2024-09-29 16:17:48,946:INFO:Ada Boost Classifier Imported successfully
2024-09-29 16:17:48,946:INFO:Starting cross validation
2024-09-29 16:17:48,948:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:17:49,199:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:49,260:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:49,444:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:49,503:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:49,603:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:49,668:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:49,751:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:49,825:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:49,890:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:49,936:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning:

The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.


2024-09-29 16:17:51,521:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:51,592:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:51,749:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:51,829:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:51,873:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:51,918:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:51,970:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:52,037:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:52,056:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:52,092:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:17:52,108:INFO:Calculating mean and std
2024-09-29 16:17:52,108:INFO:Creating metrics dataframe
2024-09-29 16:17:52,109:INFO:Uploading results into container
2024-09-29 16:17:52,109:INFO:Uploading model into container now
2024-09-29 16:17:52,109:INFO:_master_model_container: 9
2024-09-29 16:17:52,109:INFO:_display_container: 2
2024-09-29 16:17:52,110:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-29 16:17:52,110:INFO:create_model() successfully completed......................................
2024-09-29 16:17:52,178:INFO:SubProcess create_model() end ==================================
2024-09-29 16:17:52,178:INFO:Creating metrics dataframe
2024-09-29 16:17:52,180:INFO:Initializing Gradient Boosting Classifier
2024-09-29 16:17:52,180:INFO:Total runtime is 0.39745842218399047 minutes
2024-09-29 16:17:52,180:INFO:SubProcess create_model() called ==================================
2024-09-29 16:17:52,180:INFO:Initializing create_model()
2024-09-29 16:17:52,181:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:17:52,181:INFO:Checking exceptions
2024-09-29 16:17:52,181:INFO:Importing libraries
2024-09-29 16:17:52,181:INFO:Copying training dataset
2024-09-29 16:17:52,356:INFO:Defining folds
2024-09-29 16:17:52,356:INFO:Declaring metric variables
2024-09-29 16:17:52,356:INFO:Importing untrained model
2024-09-29 16:17:52,356:INFO:Gradient Boosting Classifier Imported successfully
2024-09-29 16:17:52,356:INFO:Starting cross validation
2024-09-29 16:17:52,359:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:18:33,207:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:33,536:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:33,791:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:33,893:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:33,908:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:33,952:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:33,984:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:34,049:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:34,148:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:34,191:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:34,211:INFO:Calculating mean and std
2024-09-29 16:18:34,211:INFO:Creating metrics dataframe
2024-09-29 16:18:34,213:INFO:Uploading results into container
2024-09-29 16:18:34,213:INFO:Uploading model into container now
2024-09-29 16:18:34,214:INFO:_master_model_container: 10
2024-09-29 16:18:34,214:INFO:_display_container: 2
2024-09-29 16:18:34,214:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-29 16:18:34,214:INFO:create_model() successfully completed......................................
2024-09-29 16:18:34,286:INFO:SubProcess create_model() end ==================================
2024-09-29 16:18:34,286:INFO:Creating metrics dataframe
2024-09-29 16:18:34,288:INFO:Initializing Linear Discriminant Analysis
2024-09-29 16:18:34,289:INFO:Total runtime is 1.0992701013882955 minutes
2024-09-29 16:18:34,289:INFO:SubProcess create_model() called ==================================
2024-09-29 16:18:34,289:INFO:Initializing create_model()
2024-09-29 16:18:34,289:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:18:34,289:INFO:Checking exceptions
2024-09-29 16:18:34,289:INFO:Importing libraries
2024-09-29 16:18:34,289:INFO:Copying training dataset
2024-09-29 16:18:34,466:INFO:Defining folds
2024-09-29 16:18:34,466:INFO:Declaring metric variables
2024-09-29 16:18:34,466:INFO:Importing untrained model
2024-09-29 16:18:34,466:INFO:Linear Discriminant Analysis Imported successfully
2024-09-29 16:18:34,467:INFO:Starting cross validation
2024-09-29 16:18:34,469:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:18:36,058:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:36,089:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:36,487:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:36,599:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:36,755:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:36,800:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:36,939:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:36,944:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:36,989:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:36,993:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning:

Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes



2024-09-29 16:18:37,011:INFO:Calculating mean and std
2024-09-29 16:18:37,011:INFO:Creating metrics dataframe
2024-09-29 16:18:37,013:INFO:Uploading results into container
2024-09-29 16:18:37,013:INFO:Uploading model into container now
2024-09-29 16:18:37,013:INFO:_master_model_container: 11
2024-09-29 16:18:37,013:INFO:_display_container: 2
2024-09-29 16:18:37,014:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-29 16:18:37,014:INFO:create_model() successfully completed......................................
2024-09-29 16:18:37,079:INFO:SubProcess create_model() end ==================================
2024-09-29 16:18:37,079:INFO:Creating metrics dataframe
2024-09-29 16:18:37,081:INFO:Initializing Extra Trees Classifier
2024-09-29 16:18:37,081:INFO:Total runtime is 1.1458019932111105 minutes
2024-09-29 16:18:37,081:INFO:SubProcess create_model() called ==================================
2024-09-29 16:18:37,081:INFO:Initializing create_model()
2024-09-29 16:18:37,081:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:18:37,081:INFO:Checking exceptions
2024-09-29 16:18:37,082:INFO:Importing libraries
2024-09-29 16:18:37,082:INFO:Copying training dataset
2024-09-29 16:18:37,252:INFO:Defining folds
2024-09-29 16:18:37,252:INFO:Declaring metric variables
2024-09-29 16:18:37,252:INFO:Importing untrained model
2024-09-29 16:18:37,252:INFO:Extra Trees Classifier Imported successfully
2024-09-29 16:18:37,253:INFO:Starting cross validation
2024-09-29 16:18:37,255:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:18:39,860:INFO:Calculating mean and std
2024-09-29 16:18:39,860:INFO:Creating metrics dataframe
2024-09-29 16:18:39,862:INFO:Uploading results into container
2024-09-29 16:18:39,862:INFO:Uploading model into container now
2024-09-29 16:18:39,862:INFO:_master_model_container: 12
2024-09-29 16:18:39,862:INFO:_display_container: 2
2024-09-29 16:18:39,863:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 16:18:39,863:INFO:create_model() successfully completed......................................
2024-09-29 16:18:39,924:INFO:SubProcess create_model() end ==================================
2024-09-29 16:18:39,924:INFO:Creating metrics dataframe
2024-09-29 16:18:39,926:INFO:Initializing Light Gradient Boosting Machine
2024-09-29 16:18:39,927:INFO:Total runtime is 1.193249555428823 minutes
2024-09-29 16:18:39,928:INFO:SubProcess create_model() called ==================================
2024-09-29 16:18:39,928:INFO:Initializing create_model()
2024-09-29 16:18:39,928:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:18:39,928:INFO:Checking exceptions
2024-09-29 16:18:39,928:INFO:Importing libraries
2024-09-29 16:18:39,928:INFO:Copying training dataset
2024-09-29 16:18:40,094:INFO:Defining folds
2024-09-29 16:18:40,094:INFO:Declaring metric variables
2024-09-29 16:18:40,094:INFO:Importing untrained model
2024-09-29 16:18:40,094:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-29 16:18:40,094:INFO:Starting cross validation
2024-09-29 16:18:40,097:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:19:05,569:INFO:Calculating mean and std
2024-09-29 16:19:05,570:INFO:Creating metrics dataframe
2024-09-29 16:19:05,572:INFO:Uploading results into container
2024-09-29 16:19:05,572:INFO:Uploading model into container now
2024-09-29 16:19:05,572:INFO:_master_model_container: 13
2024-09-29 16:19:05,572:INFO:_display_container: 2
2024-09-29 16:19:05,573:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-29 16:19:05,573:INFO:create_model() successfully completed......................................
2024-09-29 16:19:05,662:INFO:SubProcess create_model() end ==================================
2024-09-29 16:19:05,662:INFO:Creating metrics dataframe
2024-09-29 16:19:05,664:INFO:Initializing Dummy Classifier
2024-09-29 16:19:05,664:INFO:Total runtime is 1.6221877892812095 minutes
2024-09-29 16:19:05,664:INFO:SubProcess create_model() called ==================================
2024-09-29 16:19:05,665:INFO:Initializing create_model()
2024-09-29 16:19:05,665:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029D0CD41490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:19:05,665:INFO:Checking exceptions
2024-09-29 16:19:05,665:INFO:Importing libraries
2024-09-29 16:19:05,665:INFO:Copying training dataset
2024-09-29 16:19:05,839:INFO:Defining folds
2024-09-29 16:19:05,839:INFO:Declaring metric variables
2024-09-29 16:19:05,839:INFO:Importing untrained model
2024-09-29 16:19:05,839:INFO:Dummy Classifier Imported successfully
2024-09-29 16:19:05,839:INFO:Starting cross validation
2024-09-29 16:19:05,842:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-29 16:19:06,202:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,286:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,352:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,357:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,396:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,479:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,500:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,534:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,632:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,684:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning:

Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.


2024-09-29 16:19:06,699:INFO:Calculating mean and std
2024-09-29 16:19:06,699:INFO:Creating metrics dataframe
2024-09-29 16:19:06,701:INFO:Uploading results into container
2024-09-29 16:19:06,701:INFO:Uploading model into container now
2024-09-29 16:19:06,701:INFO:_master_model_container: 14
2024-09-29 16:19:06,701:INFO:_display_container: 2
2024-09-29 16:19:06,702:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-29 16:19:06,702:INFO:create_model() successfully completed......................................
2024-09-29 16:19:06,764:INFO:SubProcess create_model() end ==================================
2024-09-29 16:19:06,764:INFO:Creating metrics dataframe
2024-09-29 16:19:06,766:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning:

Styler.applymap has been deprecated. Use Styler.map instead.


2024-09-29 16:19:06,767:INFO:Initializing create_model()
2024-09-29 16:19:06,767:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:19:06,768:INFO:Checking exceptions
2024-09-29 16:19:06,768:INFO:Importing libraries
2024-09-29 16:19:06,768:INFO:Copying training dataset
2024-09-29 16:19:06,936:INFO:Defining folds
2024-09-29 16:19:06,936:INFO:Declaring metric variables
2024-09-29 16:19:06,936:INFO:Importing untrained model
2024-09-29 16:19:06,936:INFO:Declaring custom model
2024-09-29 16:19:06,936:INFO:Extra Trees Classifier Imported successfully
2024-09-29 16:19:06,939:INFO:Cross validation set to False
2024-09-29 16:19:06,939:INFO:Fitting Model
2024-09-29 16:19:07,271:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 16:19:07,271:INFO:create_model() successfully completed......................................
2024-09-29 16:19:07,347:INFO:_master_model_container: 14
2024-09-29 16:19:07,347:INFO:_display_container: 2
2024-09-29 16:19:07,347:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 16:19:07,347:INFO:compare_models() successfully completed......................................
2024-09-29 16:19:07,348:INFO:Initializing finalize_model()
2024-09-29 16:19:07,348:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-29 16:19:07,348:INFO:Finalizing ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-29 16:19:07,476:INFO:Initializing create_model()
2024-09-29 16:19:07,476:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-29 16:19:07,476:INFO:Checking exceptions
2024-09-29 16:19:07,476:INFO:Importing libraries
2024-09-29 16:19:07,476:INFO:Copying training dataset
2024-09-29 16:19:07,494:INFO:Defining folds
2024-09-29 16:19:07,494:INFO:Declaring metric variables
2024-09-29 16:19:07,494:INFO:Importing untrained model
2024-09-29 16:19:07,494:INFO:Declaring custom model
2024-09-29 16:19:07,495:INFO:Extra Trees Classifier Imported successfully
2024-09-29 16:19:07,497:INFO:Cross validation set to False
2024-09-29 16:19:07,498:INFO:Fitting Model
2024-09-29 16:19:07,958:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-29 16:19:07,959:INFO:create_model() successfully completed......................................
2024-09-29 16:19:08,026:INFO:_master_model_container: 14
2024-09-29 16:19:08,026:INFO:_display_container: 2
2024-09-29 16:19:08,034:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-29 16:19:08,034:INFO:finalize_model() successfully completed......................................
2024-09-29 16:19:08,110:INFO:Initializing predict_model()
2024-09-29 16:19:08,110:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029D52CD47C0>)
2024-09-29 16:19:08,110:INFO:Checking exceptions
2024-09-29 16:19:08,110:INFO:Preloading libraries
2024-09-29 16:19:08,110:INFO:Set up data.
2024-09-29 16:19:08,260:INFO:Set up index.
2024-09-29 16:19:08,463:INFO:Initializing plot_model()
2024-09-29 16:19:08,463:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029D0CC6C250>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-09-29 16:19:08,464:INFO:Checking exceptions
2024-09-29 16:19:08,553:INFO:Preloading libraries
2024-09-29 16:19:08,574:INFO:Copying training dataset
2024-09-29 16:19:08,575:INFO:Plot type: feature
2024-09-29 16:19:08,575:WARNING:No coef_ found. Trying feature_importances_
2024-09-29 16:19:08,977:INFO:Visual Rendered Successfully
2024-09-29 16:19:09,037:INFO:plot_model() successfully completed......................................
2024-09-30 09:58:01,485:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-30 09:58:01,486:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-30 09:58:01,486:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-30 09:58:01,486:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-30 09:58:06,160:INFO:PyCaret ClassificationExperiment
2024-09-30 09:58:06,160:INFO:Logging name: text_classification
2024-09-30 09:58:06,160:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-30 09:58:06,160:INFO:version 3.3.2
2024-09-30 09:58:06,160:INFO:Initializing setup()
2024-09-30 09:58:06,160:INFO:self.USI: 8c2d
2024-09-30 09:58:06,160:INFO:self._variable_keys: {'data', 'memory', 'gpu_param', 'X', 'exp_id', 'exp_name_log', 'y_test', 'X_train', 'target_param', 'idx', 'pipeline', 'X_test', 'gpu_n_jobs_param', '_ml_usecase', 'log_plots_param', 'html_param', 'fold_generator', 'y_train', 'seed', 'n_jobs_param', '_available_plots', 'is_multiclass', 'fold_groups_param', 'fix_imbalance', 'y', 'USI', 'fold_shuffle_param', 'logging_param'}
2024-09-30 09:58:06,160:INFO:Checking environment
2024-09-30 09:58:06,160:INFO:python_version: 3.11.6
2024-09-30 09:58:06,160:INFO:python_build: ('tags/v3.11.6:8b6ee5b', 'Oct  2 2023 14:57:12')
2024-09-30 09:58:06,160:INFO:machine: AMD64
2024-09-30 09:58:06,160:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-30 09:58:06,162:INFO:Memory: svmem(total=17113423872, available=8088039424, percent=52.7, used=9025384448, free=8088039424)
2024-09-30 09:58:06,162:INFO:Physical Core: 6
2024-09-30 09:58:06,163:INFO:Logical Core: 12
2024-09-30 09:58:06,163:INFO:Checking libraries
2024-09-30 09:58:06,163:INFO:System:
2024-09-30 09:58:06,163:INFO:    python: 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]
2024-09-30 09:58:06,163:INFO:executable: C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Scripts\python.exe
2024-09-30 09:58:06,163:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-30 09:58:06,163:INFO:PyCaret required dependencies:
2024-09-30 09:58:07,501:INFO:                 pip: 23.2.1
2024-09-30 09:58:07,501:INFO:          setuptools: 68.2.0
2024-09-30 09:58:07,501:INFO:             pycaret: 3.3.2
2024-09-30 09:58:07,501:INFO:             IPython: 8.27.0
2024-09-30 09:58:07,501:INFO:          ipywidgets: 8.1.5
2024-09-30 09:58:07,501:INFO:                tqdm: 4.66.5
2024-09-30 09:58:07,501:INFO:               numpy: 1.26.4
2024-09-30 09:58:07,501:INFO:              pandas: 2.1.4
2024-09-30 09:58:07,501:INFO:              jinja2: 3.1.4
2024-09-30 09:58:07,501:INFO:               scipy: 1.11.4
2024-09-30 09:58:07,501:INFO:              joblib: 1.3.2
2024-09-30 09:58:07,501:INFO:             sklearn: 1.4.2
2024-09-30 09:58:07,501:INFO:                pyod: 2.0.2
2024-09-30 09:58:07,501:INFO:            imblearn: 0.12.3
2024-09-30 09:58:07,501:INFO:   category_encoders: 2.6.3
2024-09-30 09:58:07,501:INFO:            lightgbm: 4.5.0
2024-09-30 09:58:07,501:INFO:               numba: 0.60.0
2024-09-30 09:58:07,501:INFO:            requests: 2.32.3
2024-09-30 09:58:07,501:INFO:          matplotlib: 3.7.5
2024-09-30 09:58:07,501:INFO:          scikitplot: 0.3.7
2024-09-30 09:58:07,502:INFO:         yellowbrick: 1.5
2024-09-30 09:58:07,502:INFO:              plotly: 5.24.1
2024-09-30 09:58:07,502:INFO:    plotly-resampler: Not installed
2024-09-30 09:58:07,502:INFO:             kaleido: 0.2.1
2024-09-30 09:58:07,502:INFO:           schemdraw: 0.15
2024-09-30 09:58:07,502:INFO:         statsmodels: 0.14.3
2024-09-30 09:58:07,502:INFO:              sktime: 0.26.0
2024-09-30 09:58:07,502:INFO:               tbats: 1.1.3
2024-09-30 09:58:07,502:INFO:            pmdarima: 2.0.4
2024-09-30 09:58:07,502:INFO:              psutil: 6.0.0
2024-09-30 09:58:07,502:INFO:          markupsafe: 2.1.5
2024-09-30 09:58:07,502:INFO:             pickle5: Not installed
2024-09-30 09:58:07,502:INFO:         cloudpickle: 3.0.0
2024-09-30 09:58:07,502:INFO:         deprecation: 2.1.0
2024-09-30 09:58:07,502:INFO:              xxhash: 3.5.0
2024-09-30 09:58:07,502:INFO:           wurlitzer: Not installed
2024-09-30 09:58:07,502:INFO:PyCaret optional dependencies:
2024-09-30 09:58:11,635:INFO:                shap: 0.44.1
2024-09-30 09:58:11,635:INFO:           interpret: 0.6.4
2024-09-30 09:58:11,635:INFO:                umap: 0.5.6
2024-09-30 09:58:11,635:INFO:     ydata_profiling: 4.10.0
2024-09-30 09:58:11,635:INFO:  explainerdashboard: 0.4.7
2024-09-30 09:58:11,635:INFO:             autoviz: Not installed
2024-09-30 09:58:11,635:INFO:           fairlearn: 0.7.0
2024-09-30 09:58:11,635:INFO:          deepchecks: Not installed
2024-09-30 09:58:11,635:INFO:             xgboost: Not installed
2024-09-30 09:58:11,635:INFO:            catboost: 1.2.7
2024-09-30 09:58:11,635:INFO:              kmodes: 0.12.2
2024-09-30 09:58:11,635:INFO:             mlxtend: 0.23.1
2024-09-30 09:58:11,635:INFO:       statsforecast: 1.5.0
2024-09-30 09:58:11,635:INFO:        tune_sklearn: Not installed
2024-09-30 09:58:11,635:INFO:                 ray: Not installed
2024-09-30 09:58:11,635:INFO:            hyperopt: 0.2.7
2024-09-30 09:58:11,635:INFO:              optuna: 4.0.0
2024-09-30 09:58:11,635:INFO:               skopt: 0.10.2
2024-09-30 09:58:11,635:INFO:              mlflow: 2.16.2
2024-09-30 09:58:11,635:INFO:              gradio: 4.44.0
2024-09-30 09:58:11,635:INFO:             fastapi: 0.115.0
2024-09-30 09:58:11,635:INFO:             uvicorn: 0.31.0
2024-09-30 09:58:11,635:INFO:              m2cgen: 0.10.0
2024-09-30 09:58:11,636:INFO:           evidently: 0.4.38
2024-09-30 09:58:11,636:INFO:               fugue: 0.8.7
2024-09-30 09:58:11,636:INFO:           streamlit: Not installed
2024-09-30 09:58:11,636:INFO:             prophet: Not installed
2024-09-30 09:58:11,636:INFO:None
2024-09-30 09:58:11,636:INFO:Set up data.
2024-09-30 09:58:11,859:INFO:Set up folding strategy.
2024-09-30 09:58:11,860:INFO:Set up train/test split.
2024-09-30 09:58:12,100:INFO:Set up index.
2024-09-30 09:58:12,105:INFO:Assigning column types.
2024-09-30 09:58:12,294:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-30 09:58:12,336:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-30 09:58:12,340:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-30 09:58:12,374:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 09:58:12,375:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 09:58:12,848:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-30 09:58:12,848:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-30 09:58:12,875:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 09:58:12,875:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 09:58:12,876:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-30 09:58:12,918:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-30 09:58:12,945:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 09:58:12,945:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 09:58:12,988:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-30 09:58:13,014:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 09:58:13,014:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 09:58:13,015:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-30 09:58:13,084:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 09:58:13,084:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 09:58:13,152:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 09:58:13,152:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 09:58:13,162:INFO:Preparing preprocessing pipeline...
2024-09-30 09:58:13,189:INFO:Set up simple imputation.
2024-09-30 09:58:13,760:INFO:Finished creating preprocessing pipeline.
2024-09-30 09:58:13,768:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-30 09:58:13,769:INFO:Creating final display dataframe.
2024-09-30 09:58:14,992:INFO:Setup _display_container:                     Description                Value
0                    Session id                  123
1                        Target               target
2                   Target type           Multiclass
3           Original data shape         (3156, 1001)
4        Transformed data shape         (3156, 1001)
5   Transformed train set shape         (2209, 1001)
6    Transformed test set shape          (947, 1001)
7              Numeric features                 1000
8                    Preprocess                 True
9               Imputation type               simple
10           Numeric imputation                 mean
11       Categorical imputation                 mode
12               Fold Generator      StratifiedKFold
13                  Fold Number                   10
14                     CPU Jobs                   -1
15                      Use GPU                False
16               Log Experiment         MlflowLogger
17              Experiment Name  text_classification
18                          USI                 8c2d
2024-09-30 09:58:15,067:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 09:58:15,068:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 09:58:15,141:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 09:58:15,141:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 09:58:15,142:INFO:Logging experiment in loggers
2024-09-30 09:58:15,330:INFO:SubProcess save_model() called ==================================
2024-09-30 09:58:15,353:INFO:Initializing save_model()
2024-09-30 09:58:15,353:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False), model_name=C:\Users\yuriy\AppData\Local\Temp\tmpxfrga3hc\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False), verbose=False, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-09-30 09:58:15,353:INFO:Adding model into prep_pipe
2024-09-30 09:58:15,353:WARNING:Only Model saved as it was a pipeline.
2024-09-30 09:58:15,367:INFO:C:\Users\yuriy\AppData\Local\Temp\tmpxfrga3hc\Transformation Pipeline.pkl saved in current working directory
2024-09-30 09:58:15,379:INFO:Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-30 09:58:15,380:INFO:save_model() successfully completed......................................
2024-09-30 09:58:15,502:INFO:SubProcess save_model() end ==================================
2024-09-30 09:58:18,333:INFO:setup() successfully completed in 8.98s...............
2024-09-30 09:58:18,333:INFO:Initializing compare_models()
2024-09-30 09:58:18,334:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-30 09:58:18,334:INFO:Checking exceptions
2024-09-30 09:58:18,531:INFO:Preparing display monitor
2024-09-30 09:58:18,685:INFO:Initializing Logistic Regression
2024-09-30 09:58:18,685:INFO:Total runtime is 0.0 minutes
2024-09-30 09:58:18,689:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:18,690:INFO:Initializing create_model()
2024-09-30 09:58:18,690:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:18,690:INFO:Checking exceptions
2024-09-30 09:58:18,690:INFO:Importing libraries
2024-09-30 09:58:18,690:INFO:Copying training dataset
2024-09-30 09:58:19,047:INFO:Defining folds
2024-09-30 09:58:19,047:INFO:Declaring metric variables
2024-09-30 09:58:19,051:INFO:Importing untrained model
2024-09-30 09:58:19,054:INFO:Logistic Regression Imported successfully
2024-09-30 09:58:19,061:INFO:Starting cross validation
2024-09-30 09:58:19,064:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:58:24,813:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,011:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,069:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,244:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,337:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,372:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,497:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,552:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,554:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,565:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:25,590:INFO:Calculating mean and std
2024-09-30 09:58:25,591:INFO:Creating metrics dataframe
2024-09-30 09:58:25,594:INFO:Uploading results into container
2024-09-30 09:58:25,594:INFO:Uploading model into container now
2024-09-30 09:58:25,594:INFO:_master_model_container: 1
2024-09-30 09:58:25,595:INFO:_display_container: 2
2024-09-30 09:58:25,595:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-30 09:58:25,595:INFO:create_model() successfully completed......................................
2024-09-30 09:58:25,709:INFO:SubProcess create_model() end ==================================
2024-09-30 09:58:25,709:INFO:Creating metrics dataframe
2024-09-30 09:58:25,714:INFO:Initializing K Neighbors Classifier
2024-09-30 09:58:25,715:INFO:Total runtime is 0.11716756025950113 minutes
2024-09-30 09:58:25,717:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:25,717:INFO:Initializing create_model()
2024-09-30 09:58:25,717:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:25,717:INFO:Checking exceptions
2024-09-30 09:58:25,717:INFO:Importing libraries
2024-09-30 09:58:25,718:INFO:Copying training dataset
2024-09-30 09:58:25,890:INFO:Defining folds
2024-09-30 09:58:25,890:INFO:Declaring metric variables
2024-09-30 09:58:25,893:INFO:Importing untrained model
2024-09-30 09:58:25,896:INFO:K Neighbors Classifier Imported successfully
2024-09-30 09:58:25,901:INFO:Starting cross validation
2024-09-30 09:58:25,903:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:58:30,811:INFO:Calculating mean and std
2024-09-30 09:58:30,813:INFO:Creating metrics dataframe
2024-09-30 09:58:30,815:INFO:Uploading results into container
2024-09-30 09:58:30,815:INFO:Uploading model into container now
2024-09-30 09:58:30,816:INFO:_master_model_container: 2
2024-09-30 09:58:30,816:INFO:_display_container: 2
2024-09-30 09:58:30,816:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-30 09:58:30,816:INFO:create_model() successfully completed......................................
2024-09-30 09:58:30,985:INFO:SubProcess create_model() end ==================================
2024-09-30 09:58:30,985:INFO:Creating metrics dataframe
2024-09-30 09:58:31,000:INFO:Initializing Naive Bayes
2024-09-30 09:58:31,000:INFO:Total runtime is 0.20525057315826417 minutes
2024-09-30 09:58:31,004:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:31,004:INFO:Initializing create_model()
2024-09-30 09:58:31,005:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:31,005:INFO:Checking exceptions
2024-09-30 09:58:31,005:INFO:Importing libraries
2024-09-30 09:58:31,005:INFO:Copying training dataset
2024-09-30 09:58:31,232:INFO:Defining folds
2024-09-30 09:58:31,232:INFO:Declaring metric variables
2024-09-30 09:58:31,236:INFO:Importing untrained model
2024-09-30 09:58:31,239:INFO:Naive Bayes Imported successfully
2024-09-30 09:58:31,245:INFO:Starting cross validation
2024-09-30 09:58:31,249:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:58:33,033:INFO:Calculating mean and std
2024-09-30 09:58:33,035:INFO:Creating metrics dataframe
2024-09-30 09:58:33,037:INFO:Uploading results into container
2024-09-30 09:58:33,038:INFO:Uploading model into container now
2024-09-30 09:58:33,038:INFO:_master_model_container: 3
2024-09-30 09:58:33,038:INFO:_display_container: 2
2024-09-30 09:58:33,039:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-30 09:58:33,039:INFO:create_model() successfully completed......................................
2024-09-30 09:58:33,213:INFO:SubProcess create_model() end ==================================
2024-09-30 09:58:33,213:INFO:Creating metrics dataframe
2024-09-30 09:58:33,226:INFO:Initializing Decision Tree Classifier
2024-09-30 09:58:33,226:INFO:Total runtime is 0.24234994649887087 minutes
2024-09-30 09:58:33,233:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:33,233:INFO:Initializing create_model()
2024-09-30 09:58:33,233:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:33,234:INFO:Checking exceptions
2024-09-30 09:58:33,234:INFO:Importing libraries
2024-09-30 09:58:33,234:INFO:Copying training dataset
2024-09-30 09:58:33,514:INFO:Defining folds
2024-09-30 09:58:33,514:INFO:Declaring metric variables
2024-09-30 09:58:33,518:INFO:Importing untrained model
2024-09-30 09:58:33,522:INFO:Decision Tree Classifier Imported successfully
2024-09-30 09:58:33,528:INFO:Starting cross validation
2024-09-30 09:58:33,530:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:58:36,166:INFO:Calculating mean and std
2024-09-30 09:58:36,167:INFO:Creating metrics dataframe
2024-09-30 09:58:36,169:INFO:Uploading results into container
2024-09-30 09:58:36,170:INFO:Uploading model into container now
2024-09-30 09:58:36,170:INFO:_master_model_container: 4
2024-09-30 09:58:36,170:INFO:_display_container: 2
2024-09-30 09:58:36,171:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-30 09:58:36,171:INFO:create_model() successfully completed......................................
2024-09-30 09:58:36,311:INFO:SubProcess create_model() end ==================================
2024-09-30 09:58:36,312:INFO:Creating metrics dataframe
2024-09-30 09:58:36,351:INFO:Initializing SVM - Linear Kernel
2024-09-30 09:58:36,351:INFO:Total runtime is 0.29444262584050496 minutes
2024-09-30 09:58:36,376:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:36,377:INFO:Initializing create_model()
2024-09-30 09:58:36,377:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:36,377:INFO:Checking exceptions
2024-09-30 09:58:36,377:INFO:Importing libraries
2024-09-30 09:58:36,377:INFO:Copying training dataset
2024-09-30 09:58:36,755:INFO:Defining folds
2024-09-30 09:58:36,755:INFO:Declaring metric variables
2024-09-30 09:58:36,760:INFO:Importing untrained model
2024-09-30 09:58:36,767:INFO:SVM - Linear Kernel Imported successfully
2024-09-30 09:58:36,774:INFO:Starting cross validation
2024-09-30 09:58:36,778:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:58:37,778:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:37,843:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:38,119:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:38,167:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:38,181:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:38,341:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:38,383:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:38,416:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:38,420:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:38,449:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:38,522:INFO:Calculating mean and std
2024-09-30 09:58:38,523:INFO:Creating metrics dataframe
2024-09-30 09:58:38,542:INFO:Uploading results into container
2024-09-30 09:58:38,561:INFO:Uploading model into container now
2024-09-30 09:58:38,561:INFO:_master_model_container: 5
2024-09-30 09:58:38,561:INFO:_display_container: 2
2024-09-30 09:58:38,563:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-30 09:58:38,564:INFO:create_model() successfully completed......................................
2024-09-30 09:58:38,775:INFO:SubProcess create_model() end ==================================
2024-09-30 09:58:38,775:INFO:Creating metrics dataframe
2024-09-30 09:58:38,782:INFO:Initializing Ridge Classifier
2024-09-30 09:58:38,783:INFO:Total runtime is 0.3349716742833455 minutes
2024-09-30 09:58:38,787:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:38,787:INFO:Initializing create_model()
2024-09-30 09:58:38,787:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:38,787:INFO:Checking exceptions
2024-09-30 09:58:38,788:INFO:Importing libraries
2024-09-30 09:58:38,788:INFO:Copying training dataset
2024-09-30 09:58:38,971:INFO:Defining folds
2024-09-30 09:58:38,971:INFO:Declaring metric variables
2024-09-30 09:58:38,975:INFO:Importing untrained model
2024-09-30 09:58:38,977:INFO:Ridge Classifier Imported successfully
2024-09-30 09:58:38,983:INFO:Starting cross validation
2024-09-30 09:58:38,985:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:58:39,511:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:39,581:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:39,610:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:39,801:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:39,801:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:39,867:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:39,899:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:40,019:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:40,041:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:40,054:INFO:Calculating mean and std
2024-09-30 09:58:40,055:INFO:Creating metrics dataframe
2024-09-30 09:58:40,056:INFO:Uploading results into container
2024-09-30 09:58:40,057:INFO:Uploading model into container now
2024-09-30 09:58:40,057:INFO:_master_model_container: 6
2024-09-30 09:58:40,057:INFO:_display_container: 2
2024-09-30 09:58:40,058:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-30 09:58:40,058:INFO:create_model() successfully completed......................................
2024-09-30 09:58:40,151:INFO:SubProcess create_model() end ==================================
2024-09-30 09:58:40,152:INFO:Creating metrics dataframe
2024-09-30 09:58:40,158:INFO:Initializing Random Forest Classifier
2024-09-30 09:58:40,158:INFO:Total runtime is 0.35789856910705564 minutes
2024-09-30 09:58:40,160:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:40,161:INFO:Initializing create_model()
2024-09-30 09:58:40,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:40,161:INFO:Checking exceptions
2024-09-30 09:58:40,161:INFO:Importing libraries
2024-09-30 09:58:40,161:INFO:Copying training dataset
2024-09-30 09:58:40,336:INFO:Defining folds
2024-09-30 09:58:40,336:INFO:Declaring metric variables
2024-09-30 09:58:40,339:INFO:Importing untrained model
2024-09-30 09:58:40,342:INFO:Random Forest Classifier Imported successfully
2024-09-30 09:58:40,348:INFO:Starting cross validation
2024-09-30 09:58:40,349:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:58:42,760:INFO:Calculating mean and std
2024-09-30 09:58:42,761:INFO:Creating metrics dataframe
2024-09-30 09:58:42,763:INFO:Uploading results into container
2024-09-30 09:58:42,763:INFO:Uploading model into container now
2024-09-30 09:58:42,763:INFO:_master_model_container: 7
2024-09-30 09:58:42,764:INFO:_display_container: 2
2024-09-30 09:58:42,764:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-30 09:58:42,764:INFO:create_model() successfully completed......................................
2024-09-30 09:58:42,856:INFO:SubProcess create_model() end ==================================
2024-09-30 09:58:42,856:INFO:Creating metrics dataframe
2024-09-30 09:58:42,862:INFO:Initializing Quadratic Discriminant Analysis
2024-09-30 09:58:42,862:INFO:Total runtime is 0.4029542446136474 minutes
2024-09-30 09:58:42,865:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:42,865:INFO:Initializing create_model()
2024-09-30 09:58:42,865:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:42,865:INFO:Checking exceptions
2024-09-30 09:58:42,865:INFO:Importing libraries
2024-09-30 09:58:42,865:INFO:Copying training dataset
2024-09-30 09:58:43,037:INFO:Defining folds
2024-09-30 09:58:43,037:INFO:Declaring metric variables
2024-09-30 09:58:43,039:INFO:Importing untrained model
2024-09-30 09:58:43,042:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-30 09:58:43,048:INFO:Starting cross validation
2024-09-30 09:58:43,050:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:58:43,490:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:43,522:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:43,602:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:43,786:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:43,837:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:43,949:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:44,058:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:44,166:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:44,492:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:44,541:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:44,571:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:44,589:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 09:58:44,659:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:44,745:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:44,802:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:44,856:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:44,953:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:44,971:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:45,098:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:45,143:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:45,155:INFO:Calculating mean and std
2024-09-30 09:58:45,156:INFO:Creating metrics dataframe
2024-09-30 09:58:45,157:INFO:Uploading results into container
2024-09-30 09:58:45,157:INFO:Uploading model into container now
2024-09-30 09:58:45,158:INFO:_master_model_container: 8
2024-09-30 09:58:45,158:INFO:_display_container: 2
2024-09-30 09:58:45,158:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-30 09:58:45,158:INFO:create_model() successfully completed......................................
2024-09-30 09:58:45,250:INFO:SubProcess create_model() end ==================================
2024-09-30 09:58:45,250:INFO:Creating metrics dataframe
2024-09-30 09:58:45,256:INFO:Initializing Ada Boost Classifier
2024-09-30 09:58:45,257:INFO:Total runtime is 0.44286590019861855 minutes
2024-09-30 09:58:45,259:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:45,259:INFO:Initializing create_model()
2024-09-30 09:58:45,259:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:45,260:INFO:Checking exceptions
2024-09-30 09:58:45,260:INFO:Importing libraries
2024-09-30 09:58:45,260:INFO:Copying training dataset
2024-09-30 09:58:45,434:INFO:Defining folds
2024-09-30 09:58:45,435:INFO:Declaring metric variables
2024-09-30 09:58:45,437:INFO:Importing untrained model
2024-09-30 09:58:45,440:INFO:Ada Boost Classifier Imported successfully
2024-09-30 09:58:45,447:INFO:Starting cross validation
2024-09-30 09:58:45,449:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:58:45,690:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:45,735:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:45,870:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:45,949:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:46,010:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:46,109:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:46,138:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:46,203:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:46,274:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:46,335:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 09:58:47,804:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:47,827:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:47,949:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:48,060:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:48,077:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:48,132:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:48,178:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:48,201:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:48,274:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:48,306:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:58:48,316:INFO:Calculating mean and std
2024-09-30 09:58:48,317:INFO:Creating metrics dataframe
2024-09-30 09:58:48,318:INFO:Uploading results into container
2024-09-30 09:58:48,319:INFO:Uploading model into container now
2024-09-30 09:58:48,319:INFO:_master_model_container: 9
2024-09-30 09:58:48,319:INFO:_display_container: 2
2024-09-30 09:58:48,319:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-30 09:58:48,319:INFO:create_model() successfully completed......................................
2024-09-30 09:58:48,410:INFO:SubProcess create_model() end ==================================
2024-09-30 09:58:48,410:INFO:Creating metrics dataframe
2024-09-30 09:58:48,418:INFO:Initializing Gradient Boosting Classifier
2024-09-30 09:58:48,418:INFO:Total runtime is 0.4955629388491312 minutes
2024-09-30 09:58:48,420:INFO:SubProcess create_model() called ==================================
2024-09-30 09:58:48,421:INFO:Initializing create_model()
2024-09-30 09:58:48,421:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:58:48,421:INFO:Checking exceptions
2024-09-30 09:58:48,421:INFO:Importing libraries
2024-09-30 09:58:48,421:INFO:Copying training dataset
2024-09-30 09:58:48,593:INFO:Defining folds
2024-09-30 09:58:48,593:INFO:Declaring metric variables
2024-09-30 09:58:48,596:INFO:Importing untrained model
2024-09-30 09:58:48,599:INFO:Gradient Boosting Classifier Imported successfully
2024-09-30 09:58:48,604:INFO:Starting cross validation
2024-09-30 09:58:48,606:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:59:26,687:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,113:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,198:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,453:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,503:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,528:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,532:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,632:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,781:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,816:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:27,831:INFO:Calculating mean and std
2024-09-30 09:59:27,832:INFO:Creating metrics dataframe
2024-09-30 09:59:27,833:INFO:Uploading results into container
2024-09-30 09:59:27,833:INFO:Uploading model into container now
2024-09-30 09:59:27,834:INFO:_master_model_container: 10
2024-09-30 09:59:27,834:INFO:_display_container: 2
2024-09-30 09:59:27,834:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-30 09:59:27,834:INFO:create_model() successfully completed......................................
2024-09-30 09:59:27,927:INFO:SubProcess create_model() end ==================================
2024-09-30 09:59:27,927:INFO:Creating metrics dataframe
2024-09-30 09:59:27,934:INFO:Initializing Linear Discriminant Analysis
2024-09-30 09:59:27,934:INFO:Total runtime is 1.1541568716367085 minutes
2024-09-30 09:59:27,936:INFO:SubProcess create_model() called ==================================
2024-09-30 09:59:27,937:INFO:Initializing create_model()
2024-09-30 09:59:27,937:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:59:27,937:INFO:Checking exceptions
2024-09-30 09:59:27,937:INFO:Importing libraries
2024-09-30 09:59:27,937:INFO:Copying training dataset
2024-09-30 09:59:28,111:INFO:Defining folds
2024-09-30 09:59:28,112:INFO:Declaring metric variables
2024-09-30 09:59:28,115:INFO:Importing untrained model
2024-09-30 09:59:28,118:INFO:Linear Discriminant Analysis Imported successfully
2024-09-30 09:59:28,123:INFO:Starting cross validation
2024-09-30 09:59:28,126:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:59:29,716:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:29,794:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:30,068:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:30,221:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:30,266:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:30,278:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:30,319:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:30,393:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:30,427:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:30,501:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 09:59:30,514:INFO:Calculating mean and std
2024-09-30 09:59:30,515:INFO:Creating metrics dataframe
2024-09-30 09:59:30,516:INFO:Uploading results into container
2024-09-30 09:59:30,516:INFO:Uploading model into container now
2024-09-30 09:59:30,517:INFO:_master_model_container: 11
2024-09-30 09:59:30,517:INFO:_display_container: 2
2024-09-30 09:59:30,517:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-30 09:59:30,517:INFO:create_model() successfully completed......................................
2024-09-30 09:59:30,609:INFO:SubProcess create_model() end ==================================
2024-09-30 09:59:30,609:INFO:Creating metrics dataframe
2024-09-30 09:59:30,616:INFO:Initializing Extra Trees Classifier
2024-09-30 09:59:30,616:INFO:Total runtime is 1.1988542556762696 minutes
2024-09-30 09:59:30,619:INFO:SubProcess create_model() called ==================================
2024-09-30 09:59:30,620:INFO:Initializing create_model()
2024-09-30 09:59:30,620:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:59:30,620:INFO:Checking exceptions
2024-09-30 09:59:30,620:INFO:Importing libraries
2024-09-30 09:59:30,620:INFO:Copying training dataset
2024-09-30 09:59:30,797:INFO:Defining folds
2024-09-30 09:59:30,798:INFO:Declaring metric variables
2024-09-30 09:59:30,800:INFO:Importing untrained model
2024-09-30 09:59:30,803:INFO:Extra Trees Classifier Imported successfully
2024-09-30 09:59:30,809:INFO:Starting cross validation
2024-09-30 09:59:30,811:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:59:33,308:INFO:Calculating mean and std
2024-09-30 09:59:33,309:INFO:Creating metrics dataframe
2024-09-30 09:59:33,310:INFO:Uploading results into container
2024-09-30 09:59:33,311:INFO:Uploading model into container now
2024-09-30 09:59:33,311:INFO:_master_model_container: 12
2024-09-30 09:59:33,311:INFO:_display_container: 2
2024-09-30 09:59:33,312:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-30 09:59:33,312:INFO:create_model() successfully completed......................................
2024-09-30 09:59:33,406:INFO:SubProcess create_model() end ==================================
2024-09-30 09:59:33,406:INFO:Creating metrics dataframe
2024-09-30 09:59:33,414:INFO:Initializing Light Gradient Boosting Machine
2024-09-30 09:59:33,414:INFO:Total runtime is 1.2454973459243774 minutes
2024-09-30 09:59:33,417:INFO:SubProcess create_model() called ==================================
2024-09-30 09:59:33,417:INFO:Initializing create_model()
2024-09-30 09:59:33,417:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:59:33,417:INFO:Checking exceptions
2024-09-30 09:59:33,417:INFO:Importing libraries
2024-09-30 09:59:33,417:INFO:Copying training dataset
2024-09-30 09:59:33,591:INFO:Defining folds
2024-09-30 09:59:33,591:INFO:Declaring metric variables
2024-09-30 09:59:33,593:INFO:Importing untrained model
2024-09-30 09:59:33,596:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-30 09:59:33,602:INFO:Starting cross validation
2024-09-30 09:59:33,605:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 09:59:56,216:INFO:Calculating mean and std
2024-09-30 09:59:56,217:INFO:Creating metrics dataframe
2024-09-30 09:59:56,219:INFO:Uploading results into container
2024-09-30 09:59:56,220:INFO:Uploading model into container now
2024-09-30 09:59:56,220:INFO:_master_model_container: 13
2024-09-30 09:59:56,220:INFO:_display_container: 2
2024-09-30 09:59:56,221:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-30 09:59:56,221:INFO:create_model() successfully completed......................................
2024-09-30 09:59:56,338:INFO:SubProcess create_model() end ==================================
2024-09-30 09:59:56,338:INFO:Creating metrics dataframe
2024-09-30 09:59:56,346:INFO:Initializing CatBoost Classifier
2024-09-30 09:59:56,346:INFO:Total runtime is 1.6276951074600219 minutes
2024-09-30 09:59:56,348:INFO:SubProcess create_model() called ==================================
2024-09-30 09:59:56,348:INFO:Initializing create_model()
2024-09-30 09:59:56,348:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 09:59:56,349:INFO:Checking exceptions
2024-09-30 09:59:56,349:INFO:Importing libraries
2024-09-30 09:59:56,349:INFO:Copying training dataset
2024-09-30 09:59:56,519:INFO:Defining folds
2024-09-30 09:59:56,519:INFO:Declaring metric variables
2024-09-30 09:59:56,522:INFO:Importing untrained model
2024-09-30 09:59:56,525:INFO:CatBoost Classifier Imported successfully
2024-09-30 09:59:56,530:INFO:Starting cross validation
2024-09-30 09:59:56,533:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 10:14:55,246:INFO:Calculating mean and std
2024-09-30 10:14:55,249:INFO:Creating metrics dataframe
2024-09-30 10:14:55,252:INFO:Uploading results into container
2024-09-30 10:14:55,252:INFO:Uploading model into container now
2024-09-30 10:14:55,253:INFO:_master_model_container: 14
2024-09-30 10:14:55,253:INFO:_display_container: 2
2024-09-30 10:14:55,253:INFO:<catboost.core.CatBoostClassifier object at 0x00000210DE3984D0>
2024-09-30 10:14:55,253:INFO:create_model() successfully completed......................................
2024-09-30 10:14:55,395:INFO:SubProcess create_model() end ==================================
2024-09-30 10:14:55,395:INFO:Creating metrics dataframe
2024-09-30 10:14:55,404:INFO:Initializing Dummy Classifier
2024-09-30 10:14:55,404:INFO:Total runtime is 16.61199826002121 minutes
2024-09-30 10:14:55,407:INFO:SubProcess create_model() called ==================================
2024-09-30 10:14:55,407:INFO:Initializing create_model()
2024-09-30 10:14:55,408:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E1A89A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 10:14:55,408:INFO:Checking exceptions
2024-09-30 10:14:55,408:INFO:Importing libraries
2024-09-30 10:14:55,408:INFO:Copying training dataset
2024-09-30 10:14:55,613:INFO:Defining folds
2024-09-30 10:14:55,613:INFO:Declaring metric variables
2024-09-30 10:14:55,616:INFO:Importing untrained model
2024-09-30 10:14:55,619:INFO:Dummy Classifier Imported successfully
2024-09-30 10:14:55,625:INFO:Starting cross validation
2024-09-30 10:14:55,627:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 10:14:55,996:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:55,996:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:56,069:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:56,096:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:56,184:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:56,236:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:56,321:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:56,387:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:56,436:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:56,468:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 10:14:56,479:INFO:Calculating mean and std
2024-09-30 10:14:56,480:INFO:Creating metrics dataframe
2024-09-30 10:14:56,481:INFO:Uploading results into container
2024-09-30 10:14:56,482:INFO:Uploading model into container now
2024-09-30 10:14:56,482:INFO:_master_model_container: 15
2024-09-30 10:14:56,482:INFO:_display_container: 2
2024-09-30 10:14:56,482:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-30 10:14:56,483:INFO:create_model() successfully completed......................................
2024-09-30 10:14:56,590:INFO:SubProcess create_model() end ==================================
2024-09-30 10:14:56,591:INFO:Creating metrics dataframe
2024-09-30 10:14:56,626:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-30 10:14:56,635:INFO:Initializing create_model()
2024-09-30 10:14:56,635:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 10:14:56,635:INFO:Checking exceptions
2024-09-30 10:14:56,638:INFO:Importing libraries
2024-09-30 10:14:56,638:INFO:Copying training dataset
2024-09-30 10:14:56,829:INFO:Defining folds
2024-09-30 10:14:56,829:INFO:Declaring metric variables
2024-09-30 10:14:56,829:INFO:Importing untrained model
2024-09-30 10:14:56,829:INFO:Declaring custom model
2024-09-30 10:14:56,830:INFO:Extra Trees Classifier Imported successfully
2024-09-30 10:14:56,832:INFO:Cross validation set to False
2024-09-30 10:14:56,832:INFO:Fitting Model
2024-09-30 10:14:57,166:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-30 10:14:57,167:INFO:create_model() successfully completed......................................
2024-09-30 10:14:57,264:INFO:Creating Dashboard logs
2024-09-30 10:14:57,267:INFO:Model: Extra Trees Classifier
2024-09-30 10:14:57,329:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-09-30 10:14:57,455:INFO:Initializing predict_model()
2024-09-30 10:14:57,455:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000210DF82BA60>)
2024-09-30 10:14:57,455:INFO:Checking exceptions
2024-09-30 10:14:57,455:INFO:Preloading libraries
2024-09-30 10:14:58,422:INFO:SubProcess plot_model() called ==================================
2024-09-30 10:14:58,423:INFO:Initializing plot_model()
2024-09-30 10:14:58,423:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), plot=auc, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmptkz8uqm5, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 10:14:58,423:INFO:Checking exceptions
2024-09-30 10:14:58,509:INFO:Preloading libraries
2024-09-30 10:14:58,526:INFO:Copying training dataset
2024-09-30 10:14:58,526:INFO:Plot type: auc
2024-09-30 10:14:59,682:INFO:Fitting Model
2024-09-30 10:14:59,691:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-30 10:14:59,691:INFO:Scoring test/hold-out set
2024-09-30 10:14:59,903:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmptkz8uqm5\AUC.png'
2024-09-30 10:15:00,123:INFO:Visual Rendered Successfully
2024-09-30 10:15:00,210:INFO:plot_model() successfully completed......................................
2024-09-30 10:15:00,218:INFO:Initializing plot_model()
2024-09-30 10:15:00,218:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), plot=confusion_matrix, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmptkz8uqm5, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 10:15:00,218:INFO:Checking exceptions
2024-09-30 10:15:00,295:INFO:Preloading libraries
2024-09-30 10:15:00,311:INFO:Copying training dataset
2024-09-30 10:15:00,311:INFO:Plot type: confusion_matrix
2024-09-30 10:15:01,496:INFO:Fitting Model
2024-09-30 10:15:01,496:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-30 10:15:01,496:INFO:Scoring test/hold-out set
2024-09-30 10:15:01,579:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmptkz8uqm5\Confusion Matrix.png'
2024-09-30 10:15:01,703:INFO:Visual Rendered Successfully
2024-09-30 10:15:01,796:INFO:plot_model() successfully completed......................................
2024-09-30 10:15:01,804:INFO:Initializing plot_model()
2024-09-30 10:15:01,804:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), plot=feature, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmptkz8uqm5, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 10:15:01,805:INFO:Checking exceptions
2024-09-30 10:15:01,882:INFO:Preloading libraries
2024-09-30 10:15:01,899:INFO:Copying training dataset
2024-09-30 10:15:01,899:INFO:Plot type: feature
2024-09-30 10:15:01,899:WARNING:No coef_ found. Trying feature_importances_
2024-09-30 10:15:02,233:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmptkz8uqm5\Feature Importance.png'
2024-09-30 10:15:02,359:INFO:Visual Rendered Successfully
2024-09-30 10:15:02,445:INFO:plot_model() successfully completed......................................
2024-09-30 10:15:02,453:INFO:SubProcess plot_model() end ==================================
2024-09-30 10:15:06,040:INFO:Creating Dashboard logs
2024-09-30 10:15:06,042:INFO:Model: Ridge Classifier
2024-09-30 10:15:06,087:INFO:Logged params: {'alpha': 1.0, 'class_weight': None, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2024-09-30 10:15:06,323:INFO:Creating Dashboard logs
2024-09-30 10:15:06,326:INFO:Model: SVM - Linear Kernel
2024-09-30 10:15:06,365:INFO:Logged params: {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.001, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-09-30 10:15:06,611:INFO:Creating Dashboard logs
2024-09-30 10:15:06,613:INFO:Model: Logistic Regression
2024-09-30 10:15:06,651:INFO:Logged params: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 123, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
2024-09-30 10:15:06,889:INFO:Creating Dashboard logs
2024-09-30 10:15:06,892:INFO:Model: Light Gradient Boosting Machine
2024-09-30 10:15:06,932:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-09-30 10:15:07,181:INFO:Creating Dashboard logs
2024-09-30 10:15:07,184:INFO:Model: CatBoost Classifier
2024-09-30 10:15:07,260:WARNING:Couldn't get params for model. Exception:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\loggers\dashboard_logger.py", line 78, in log_model
    params = params.get_all_params()
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\catboost\core.py", line 3504, in get_all_params
    raise CatBoostError("There is no trained model to use get_all_params(). Use fit() to train model. Then use this method.")
_catboost.CatBoostError: There is no trained model to use get_all_params(). Use fit() to train model. Then use this method.

2024-09-30 10:15:07,260:INFO:Logged params: {}
2024-09-30 10:15:07,480:INFO:Creating Dashboard logs
2024-09-30 10:15:07,483:INFO:Model: Random Forest Classifier
2024-09-30 10:15:07,522:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-09-30 10:15:07,754:INFO:Creating Dashboard logs
2024-09-30 10:15:07,757:INFO:Model: Gradient Boosting Classifier
2024-09-30 10:15:07,796:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-09-30 10:15:08,041:INFO:Creating Dashboard logs
2024-09-30 10:15:08,044:INFO:Model: Linear Discriminant Analysis
2024-09-30 10:15:08,082:INFO:Logged params: {'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}
2024-09-30 10:15:08,308:INFO:Creating Dashboard logs
2024-09-30 10:15:08,311:INFO:Model: Naive Bayes
2024-09-30 10:15:08,350:INFO:Logged params: {'priors': None, 'var_smoothing': 1e-09}
2024-09-30 10:15:08,586:INFO:Creating Dashboard logs
2024-09-30 10:15:08,588:INFO:Model: K Neighbors Classifier
2024-09-30 10:15:08,662:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2024-09-30 10:15:08,903:INFO:Creating Dashboard logs
2024-09-30 10:15:08,906:INFO:Model: Decision Tree Classifier
2024-09-30 10:15:08,946:INFO:Logged params: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 123, 'splitter': 'best'}
2024-09-30 10:15:09,179:INFO:Creating Dashboard logs
2024-09-30 10:15:09,182:INFO:Model: Ada Boost Classifier
2024-09-30 10:15:09,222:INFO:Logged params: {'algorithm': 'SAMME.R', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': 123}
2024-09-30 10:15:09,462:INFO:Creating Dashboard logs
2024-09-30 10:15:09,464:INFO:Model: Quadratic Discriminant Analysis
2024-09-30 10:15:09,505:INFO:Logged params: {'priors': None, 'reg_param': 0.0, 'store_covariance': False, 'tol': 0.0001}
2024-09-30 10:15:09,733:INFO:Creating Dashboard logs
2024-09-30 10:15:09,736:INFO:Model: Dummy Classifier
2024-09-30 10:15:09,777:INFO:Logged params: {'constant': None, 'random_state': 123, 'strategy': 'prior'}
2024-09-30 10:15:10,020:INFO:_master_model_container: 15
2024-09-30 10:15:10,021:INFO:_display_container: 2
2024-09-30 10:15:10,021:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-30 10:15:10,021:INFO:compare_models() successfully completed......................................
2024-09-30 10:15:10,022:INFO:Initializing finalize_model()
2024-09-30 10:15:10,022:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-30 10:15:10,022:INFO:Finalizing ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-30 10:15:10,140:INFO:Initializing create_model()
2024-09-30 10:15:10,140:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 10:15:10,140:INFO:Checking exceptions
2024-09-30 10:15:10,141:INFO:Importing libraries
2024-09-30 10:15:10,141:INFO:Copying training dataset
2024-09-30 10:15:10,157:INFO:Defining folds
2024-09-30 10:15:10,157:INFO:Declaring metric variables
2024-09-30 10:15:10,157:INFO:Importing untrained model
2024-09-30 10:15:10,157:INFO:Declaring custom model
2024-09-30 10:15:10,158:INFO:Extra Trees Classifier Imported successfully
2024-09-30 10:15:10,160:INFO:Cross validation set to False
2024-09-30 10:15:10,160:INFO:Fitting Model
2024-09-30 10:15:10,536:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-30 10:15:10,536:INFO:create_model() successfully completed......................................
2024-09-30 10:15:10,635:INFO:Creating Dashboard logs
2024-09-30 10:15:10,636:INFO:Model: Extra Trees Classifier
2024-09-30 10:15:10,673:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-09-30 10:15:10,743:INFO:SubProcess plot_model() called ==================================
2024-09-30 10:15:10,750:INFO:Initializing plot_model()
2024-09-30 10:15:10,750:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=auc, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmp8chfva9e, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 10:15:10,750:INFO:Checking exceptions
2024-09-30 10:15:10,831:INFO:Preloading libraries
2024-09-30 10:15:10,854:INFO:Copying training dataset
2024-09-30 10:15:10,854:INFO:Plot type: auc
2024-09-30 10:15:12,064:INFO:Fitting Model
2024-09-30 10:15:12,064:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-30 10:15:12,064:INFO:Scoring test/hold-out set
2024-09-30 10:15:12,143:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmp8chfva9e\AUC.png'
2024-09-30 10:15:12,345:INFO:Visual Rendered Successfully
2024-09-30 10:15:12,434:INFO:plot_model() successfully completed......................................
2024-09-30 10:15:12,456:INFO:Initializing plot_model()
2024-09-30 10:15:12,456:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=confusion_matrix, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmp8chfva9e, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 10:15:12,456:INFO:Checking exceptions
2024-09-30 10:15:12,535:INFO:Preloading libraries
2024-09-30 10:15:12,556:INFO:Copying training dataset
2024-09-30 10:15:12,556:INFO:Plot type: confusion_matrix
2024-09-30 10:15:13,765:INFO:Fitting Model
2024-09-30 10:15:13,766:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-30 10:15:13,766:INFO:Scoring test/hold-out set
2024-09-30 10:15:13,845:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmp8chfva9e\Confusion Matrix.png'
2024-09-30 10:15:13,965:INFO:Visual Rendered Successfully
2024-09-30 10:15:14,054:INFO:plot_model() successfully completed......................................
2024-09-30 10:15:14,070:INFO:Initializing plot_model()
2024-09-30 10:15:14,070:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=feature, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmp8chfva9e, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 10:15:14,070:INFO:Checking exceptions
2024-09-30 10:15:14,151:INFO:Preloading libraries
2024-09-30 10:15:14,173:INFO:Copying training dataset
2024-09-30 10:15:14,173:INFO:Plot type: feature
2024-09-30 10:15:14,174:WARNING:No coef_ found. Trying feature_importances_
2024-09-30 10:15:14,486:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmp8chfva9e\Feature Importance.png'
2024-09-30 10:15:14,616:INFO:Visual Rendered Successfully
2024-09-30 10:15:14,709:INFO:plot_model() successfully completed......................................
2024-09-30 10:15:14,719:INFO:SubProcess plot_model() end ==================================
2024-09-30 10:15:14,880:INFO:_master_model_container: 15
2024-09-30 10:15:14,880:INFO:_display_container: 2
2024-09-30 10:15:14,887:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-30 10:15:14,887:INFO:finalize_model() successfully completed......................................
2024-09-30 10:15:14,989:INFO:Initializing predict_model()
2024-09-30 10:15:14,989:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210D69E2C90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000210DE211620>)
2024-09-30 10:15:14,989:INFO:Checking exceptions
2024-09-30 10:15:14,989:INFO:Preloading libraries
2024-09-30 10:15:14,990:INFO:Set up data.
2024-09-30 10:15:15,145:INFO:Set up index.
2024-09-30 12:57:10,908:INFO:PyCaret ClassificationExperiment
2024-09-30 12:57:10,909:INFO:Logging name: text_classification
2024-09-30 12:57:10,909:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-30 12:57:10,909:INFO:version 3.3.2
2024-09-30 12:57:10,909:INFO:Initializing setup()
2024-09-30 12:57:10,909:INFO:self.USI: 97a4
2024-09-30 12:57:10,909:INFO:self._variable_keys: {'data', 'memory', 'gpu_param', 'X', 'exp_id', 'exp_name_log', 'y_test', 'X_train', 'target_param', 'idx', 'pipeline', 'X_test', 'gpu_n_jobs_param', '_ml_usecase', 'log_plots_param', 'html_param', 'fold_generator', 'y_train', 'seed', 'n_jobs_param', '_available_plots', 'is_multiclass', 'fold_groups_param', 'fix_imbalance', 'y', 'USI', 'fold_shuffle_param', 'logging_param'}
2024-09-30 12:57:10,909:INFO:Checking environment
2024-09-30 12:57:10,909:INFO:python_version: 3.11.6
2024-09-30 12:57:10,909:INFO:python_build: ('tags/v3.11.6:8b6ee5b', 'Oct  2 2023 14:57:12')
2024-09-30 12:57:10,909:INFO:machine: AMD64
2024-09-30 12:57:10,909:INFO:platform: Windows-10-10.0.19045-SP0
2024-09-30 12:57:10,911:INFO:Memory: svmem(total=17113423872, available=7780298752, percent=54.5, used=9333125120, free=7780298752)
2024-09-30 12:57:10,911:INFO:Physical Core: 6
2024-09-30 12:57:10,912:INFO:Logical Core: 12
2024-09-30 12:57:10,912:INFO:Checking libraries
2024-09-30 12:57:10,912:INFO:System:
2024-09-30 12:57:10,912:INFO:    python: 3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]
2024-09-30 12:57:10,912:INFO:executable: C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Scripts\python.exe
2024-09-30 12:57:10,912:INFO:   machine: Windows-10-10.0.19045-SP0
2024-09-30 12:57:10,912:INFO:PyCaret required dependencies:
2024-09-30 12:57:10,912:INFO:                 pip: 23.2.1
2024-09-30 12:57:10,912:INFO:          setuptools: 68.2.0
2024-09-30 12:57:10,912:INFO:             pycaret: 3.3.2
2024-09-30 12:57:10,912:INFO:             IPython: 8.27.0
2024-09-30 12:57:10,912:INFO:          ipywidgets: 8.1.5
2024-09-30 12:57:10,912:INFO:                tqdm: 4.66.5
2024-09-30 12:57:10,912:INFO:               numpy: 1.26.4
2024-09-30 12:57:10,912:INFO:              pandas: 2.1.4
2024-09-30 12:57:10,912:INFO:              jinja2: 3.1.4
2024-09-30 12:57:10,912:INFO:               scipy: 1.11.4
2024-09-30 12:57:10,912:INFO:              joblib: 1.3.2
2024-09-30 12:57:10,912:INFO:             sklearn: 1.4.2
2024-09-30 12:57:10,912:INFO:                pyod: 2.0.2
2024-09-30 12:57:10,912:INFO:            imblearn: 0.12.3
2024-09-30 12:57:10,912:INFO:   category_encoders: 2.6.3
2024-09-30 12:57:10,912:INFO:            lightgbm: 4.5.0
2024-09-30 12:57:10,913:INFO:               numba: 0.60.0
2024-09-30 12:57:10,913:INFO:            requests: 2.32.3
2024-09-30 12:57:10,913:INFO:          matplotlib: 3.7.5
2024-09-30 12:57:10,913:INFO:          scikitplot: 0.3.7
2024-09-30 12:57:10,913:INFO:         yellowbrick: 1.5
2024-09-30 12:57:10,913:INFO:              plotly: 5.24.1
2024-09-30 12:57:10,913:INFO:    plotly-resampler: Not installed
2024-09-30 12:57:10,913:INFO:             kaleido: 0.2.1
2024-09-30 12:57:10,913:INFO:           schemdraw: 0.15
2024-09-30 12:57:10,913:INFO:         statsmodels: 0.14.3
2024-09-30 12:57:10,913:INFO:              sktime: 0.26.0
2024-09-30 12:57:10,913:INFO:               tbats: 1.1.3
2024-09-30 12:57:10,913:INFO:            pmdarima: 2.0.4
2024-09-30 12:57:10,913:INFO:              psutil: 6.0.0
2024-09-30 12:57:10,913:INFO:          markupsafe: 2.1.5
2024-09-30 12:57:10,913:INFO:             pickle5: Not installed
2024-09-30 12:57:10,913:INFO:         cloudpickle: 3.0.0
2024-09-30 12:57:10,913:INFO:         deprecation: 2.1.0
2024-09-30 12:57:10,913:INFO:              xxhash: 3.5.0
2024-09-30 12:57:10,913:INFO:           wurlitzer: Not installed
2024-09-30 12:57:10,913:INFO:PyCaret optional dependencies:
2024-09-30 12:57:10,913:INFO:                shap: 0.44.1
2024-09-30 12:57:10,913:INFO:           interpret: 0.6.4
2024-09-30 12:57:10,913:INFO:                umap: 0.5.6
2024-09-30 12:57:10,913:INFO:     ydata_profiling: 4.10.0
2024-09-30 12:57:10,914:INFO:  explainerdashboard: 0.4.7
2024-09-30 12:57:10,914:INFO:             autoviz: Not installed
2024-09-30 12:57:10,914:INFO:           fairlearn: 0.7.0
2024-09-30 12:57:10,914:INFO:          deepchecks: Not installed
2024-09-30 12:57:10,914:INFO:             xgboost: Not installed
2024-09-30 12:57:10,914:INFO:            catboost: 1.2.7
2024-09-30 12:57:10,914:INFO:              kmodes: 0.12.2
2024-09-30 12:57:10,914:INFO:             mlxtend: 0.23.1
2024-09-30 12:57:10,914:INFO:       statsforecast: 1.5.0
2024-09-30 12:57:10,914:INFO:        tune_sklearn: Not installed
2024-09-30 12:57:10,914:INFO:                 ray: Not installed
2024-09-30 12:57:10,914:INFO:            hyperopt: 0.2.7
2024-09-30 12:57:10,914:INFO:              optuna: 4.0.0
2024-09-30 12:57:10,914:INFO:               skopt: 0.10.2
2024-09-30 12:57:10,914:INFO:              mlflow: 2.16.2
2024-09-30 12:57:10,914:INFO:              gradio: 4.44.0
2024-09-30 12:57:10,914:INFO:             fastapi: 0.115.0
2024-09-30 12:57:10,914:INFO:             uvicorn: 0.31.0
2024-09-30 12:57:10,914:INFO:              m2cgen: 0.10.0
2024-09-30 12:57:10,914:INFO:           evidently: 0.4.38
2024-09-30 12:57:10,914:INFO:               fugue: 0.8.7
2024-09-30 12:57:10,914:INFO:           streamlit: Not installed
2024-09-30 12:57:10,914:INFO:             prophet: Not installed
2024-09-30 12:57:10,914:INFO:None
2024-09-30 12:57:10,914:INFO:Set up data.
2024-09-30 12:57:11,120:INFO:Set up folding strategy.
2024-09-30 12:57:11,120:INFO:Set up train/test split.
2024-09-30 12:57:11,247:INFO:Set up index.
2024-09-30 12:57:11,249:INFO:Assigning column types.
2024-09-30 12:57:11,587:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-30 12:57:11,628:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-30 12:57:11,629:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-30 12:57:11,655:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 12:57:11,655:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 12:57:11,696:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-30 12:57:11,697:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-30 12:57:11,722:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 12:57:11,723:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 12:57:11,723:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-30 12:57:11,766:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-30 12:57:11,791:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 12:57:11,792:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 12:57:11,832:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-30 12:57:11,858:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 12:57:11,859:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 12:57:11,859:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-30 12:57:11,926:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 12:57:11,926:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 12:57:11,994:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 12:57:11,994:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 12:57:11,995:INFO:Preparing preprocessing pipeline...
2024-09-30 12:57:12,017:INFO:Set up simple imputation.
2024-09-30 12:57:12,466:INFO:Finished creating preprocessing pipeline.
2024-09-30 12:57:12,472:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-30 12:57:12,472:INFO:Creating final display dataframe.
2024-09-30 12:57:13,534:INFO:Setup _display_container:                     Description                Value
0                    Session id                  123
1                        Target               target
2                   Target type           Multiclass
3           Original data shape         (3156, 1001)
4        Transformed data shape         (3156, 1001)
5   Transformed train set shape         (2209, 1001)
6    Transformed test set shape          (947, 1001)
7              Numeric features                 1000
8                    Preprocess                 True
9               Imputation type               simple
10           Numeric imputation                 mean
11       Categorical imputation                 mode
12               Fold Generator      StratifiedKFold
13                  Fold Number                   10
14                     CPU Jobs                   -1
15                      Use GPU                False
16               Log Experiment         MlflowLogger
17              Experiment Name  text_classification
18                          USI                 97a4
2024-09-30 12:57:13,611:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 12:57:13,612:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 12:57:13,682:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-30 12:57:13,683:INFO:Soft dependency imported: catboost: 1.2.7
2024-09-30 12:57:13,685:INFO:Logging experiment in loggers
2024-09-30 12:57:13,756:INFO:SubProcess save_model() called ==================================
2024-09-30 12:57:13,768:INFO:Initializing save_model()
2024-09-30 12:57:13,769:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False), model_name=C:\Users\yuriy\AppData\Local\Temp\tmpmt13e92n\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False), verbose=False, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-09-30 12:57:13,769:INFO:Adding model into prep_pipe
2024-09-30 12:57:13,769:WARNING:Only Model saved as it was a pipeline.
2024-09-30 12:57:13,776:INFO:C:\Users\yuriy\AppData\Local\Temp\tmpmt13e92n\Transformation Pipeline.pkl saved in current working directory
2024-09-30 12:57:13,783:INFO:Pipeline(memory=FastMemory(location=C:\Users\yuriy\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-09-30 12:57:13,783:INFO:save_model() successfully completed......................................
2024-09-30 12:57:13,911:INFO:SubProcess save_model() end ==================================
2024-09-30 12:57:15,984:INFO:setup() successfully completed in 2.78s...............
2024-09-30 12:57:15,997:INFO:Initializing compare_models()
2024-09-30 12:57:15,997:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-09-30 12:57:15,997:INFO:Checking exceptions
2024-09-30 12:57:16,099:INFO:Preparing display monitor
2024-09-30 12:57:16,118:INFO:Initializing Logistic Regression
2024-09-30 12:57:16,118:INFO:Total runtime is 0.0 minutes
2024-09-30 12:57:16,121:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:16,122:INFO:Initializing create_model()
2024-09-30 12:57:16,122:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:16,122:INFO:Checking exceptions
2024-09-30 12:57:16,122:INFO:Importing libraries
2024-09-30 12:57:16,122:INFO:Copying training dataset
2024-09-30 12:57:16,304:INFO:Defining folds
2024-09-30 12:57:16,304:INFO:Declaring metric variables
2024-09-30 12:57:16,307:INFO:Importing untrained model
2024-09-30 12:57:16,310:INFO:Logistic Regression Imported successfully
2024-09-30 12:57:16,316:INFO:Starting cross validation
2024-09-30 12:57:16,320:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:57:22,134:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,287:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,316:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,344:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,379:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,506:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,574:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,580:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,657:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,732:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:22,748:INFO:Calculating mean and std
2024-09-30 12:57:22,750:INFO:Creating metrics dataframe
2024-09-30 12:57:22,752:INFO:Uploading results into container
2024-09-30 12:57:22,752:INFO:Uploading model into container now
2024-09-30 12:57:22,753:INFO:_master_model_container: 1
2024-09-30 12:57:22,753:INFO:_display_container: 2
2024-09-30 12:57:22,753:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-30 12:57:22,753:INFO:create_model() successfully completed......................................
2024-09-30 12:57:22,883:INFO:SubProcess create_model() end ==================================
2024-09-30 12:57:22,883:INFO:Creating metrics dataframe
2024-09-30 12:57:22,888:INFO:Initializing K Neighbors Classifier
2024-09-30 12:57:22,888:INFO:Total runtime is 0.11283312638600668 minutes
2024-09-30 12:57:22,891:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:22,891:INFO:Initializing create_model()
2024-09-30 12:57:22,892:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:22,892:INFO:Checking exceptions
2024-09-30 12:57:22,892:INFO:Importing libraries
2024-09-30 12:57:22,892:INFO:Copying training dataset
2024-09-30 12:57:23,071:INFO:Defining folds
2024-09-30 12:57:23,072:INFO:Declaring metric variables
2024-09-30 12:57:23,074:INFO:Importing untrained model
2024-09-30 12:57:23,078:INFO:K Neighbors Classifier Imported successfully
2024-09-30 12:57:23,085:INFO:Starting cross validation
2024-09-30 12:57:23,088:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:57:26,318:INFO:Calculating mean and std
2024-09-30 12:57:26,319:INFO:Creating metrics dataframe
2024-09-30 12:57:26,320:INFO:Uploading results into container
2024-09-30 12:57:26,321:INFO:Uploading model into container now
2024-09-30 12:57:26,321:INFO:_master_model_container: 2
2024-09-30 12:57:26,321:INFO:_display_container: 2
2024-09-30 12:57:26,321:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-30 12:57:26,322:INFO:create_model() successfully completed......................................
2024-09-30 12:57:26,449:INFO:SubProcess create_model() end ==================================
2024-09-30 12:57:26,449:INFO:Creating metrics dataframe
2024-09-30 12:57:26,455:INFO:Initializing Naive Bayes
2024-09-30 12:57:26,455:INFO:Total runtime is 0.17227644522984822 minutes
2024-09-30 12:57:26,457:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:26,457:INFO:Initializing create_model()
2024-09-30 12:57:26,457:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:26,458:INFO:Checking exceptions
2024-09-30 12:57:26,458:INFO:Importing libraries
2024-09-30 12:57:26,458:INFO:Copying training dataset
2024-09-30 12:57:26,641:INFO:Defining folds
2024-09-30 12:57:26,641:INFO:Declaring metric variables
2024-09-30 12:57:26,643:INFO:Importing untrained model
2024-09-30 12:57:26,646:INFO:Naive Bayes Imported successfully
2024-09-30 12:57:26,651:INFO:Starting cross validation
2024-09-30 12:57:26,654:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:57:27,535:INFO:Calculating mean and std
2024-09-30 12:57:27,536:INFO:Creating metrics dataframe
2024-09-30 12:57:27,537:INFO:Uploading results into container
2024-09-30 12:57:27,537:INFO:Uploading model into container now
2024-09-30 12:57:27,538:INFO:_master_model_container: 3
2024-09-30 12:57:27,538:INFO:_display_container: 2
2024-09-30 12:57:27,538:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-30 12:57:27,538:INFO:create_model() successfully completed......................................
2024-09-30 12:57:27,653:INFO:SubProcess create_model() end ==================================
2024-09-30 12:57:27,653:INFO:Creating metrics dataframe
2024-09-30 12:57:27,658:INFO:Initializing Decision Tree Classifier
2024-09-30 12:57:27,659:INFO:Total runtime is 0.1923590342203776 minutes
2024-09-30 12:57:27,661:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:27,661:INFO:Initializing create_model()
2024-09-30 12:57:27,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:27,662:INFO:Checking exceptions
2024-09-30 12:57:27,662:INFO:Importing libraries
2024-09-30 12:57:27,662:INFO:Copying training dataset
2024-09-30 12:57:27,827:INFO:Defining folds
2024-09-30 12:57:27,827:INFO:Declaring metric variables
2024-09-30 12:57:27,830:INFO:Importing untrained model
2024-09-30 12:57:27,833:INFO:Decision Tree Classifier Imported successfully
2024-09-30 12:57:27,838:INFO:Starting cross validation
2024-09-30 12:57:27,840:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:57:29,315:INFO:Calculating mean and std
2024-09-30 12:57:29,316:INFO:Creating metrics dataframe
2024-09-30 12:57:29,317:INFO:Uploading results into container
2024-09-30 12:57:29,318:INFO:Uploading model into container now
2024-09-30 12:57:29,318:INFO:_master_model_container: 4
2024-09-30 12:57:29,318:INFO:_display_container: 2
2024-09-30 12:57:29,318:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-30 12:57:29,318:INFO:create_model() successfully completed......................................
2024-09-30 12:57:29,438:INFO:SubProcess create_model() end ==================================
2024-09-30 12:57:29,438:INFO:Creating metrics dataframe
2024-09-30 12:57:29,444:INFO:Initializing SVM - Linear Kernel
2024-09-30 12:57:29,444:INFO:Total runtime is 0.2221058408419291 minutes
2024-09-30 12:57:29,447:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:29,448:INFO:Initializing create_model()
2024-09-30 12:57:29,448:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:29,448:INFO:Checking exceptions
2024-09-30 12:57:29,448:INFO:Importing libraries
2024-09-30 12:57:29,448:INFO:Copying training dataset
2024-09-30 12:57:29,619:INFO:Defining folds
2024-09-30 12:57:29,620:INFO:Declaring metric variables
2024-09-30 12:57:29,622:INFO:Importing untrained model
2024-09-30 12:57:29,625:INFO:SVM - Linear Kernel Imported successfully
2024-09-30 12:57:29,630:INFO:Starting cross validation
2024-09-30 12:57:29,632:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:57:30,204:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,239:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,329:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,338:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,553:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,571:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,589:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,597:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,670:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,719:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:30,732:INFO:Calculating mean and std
2024-09-30 12:57:30,733:INFO:Creating metrics dataframe
2024-09-30 12:57:30,735:INFO:Uploading results into container
2024-09-30 12:57:30,735:INFO:Uploading model into container now
2024-09-30 12:57:30,735:INFO:_master_model_container: 5
2024-09-30 12:57:30,735:INFO:_display_container: 2
2024-09-30 12:57:30,736:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-30 12:57:30,736:INFO:create_model() successfully completed......................................
2024-09-30 12:57:30,854:INFO:SubProcess create_model() end ==================================
2024-09-30 12:57:30,854:INFO:Creating metrics dataframe
2024-09-30 12:57:30,860:INFO:Initializing Ridge Classifier
2024-09-30 12:57:30,860:INFO:Total runtime is 0.24569869438807168 minutes
2024-09-30 12:57:30,863:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:30,864:INFO:Initializing create_model()
2024-09-30 12:57:30,864:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:30,864:INFO:Checking exceptions
2024-09-30 12:57:30,864:INFO:Importing libraries
2024-09-30 12:57:30,864:INFO:Copying training dataset
2024-09-30 12:57:31,049:INFO:Defining folds
2024-09-30 12:57:31,049:INFO:Declaring metric variables
2024-09-30 12:57:31,051:INFO:Importing untrained model
2024-09-30 12:57:31,054:INFO:Ridge Classifier Imported successfully
2024-09-30 12:57:31,059:INFO:Starting cross validation
2024-09-30 12:57:31,062:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:57:31,424:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:31,491:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:31,543:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:31,633:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:31,772:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:31,818:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:31,824:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:31,917:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:31,918:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:32,007:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:32,025:INFO:Calculating mean and std
2024-09-30 12:57:32,026:INFO:Creating metrics dataframe
2024-09-30 12:57:32,027:INFO:Uploading results into container
2024-09-30 12:57:32,027:INFO:Uploading model into container now
2024-09-30 12:57:32,028:INFO:_master_model_container: 6
2024-09-30 12:57:32,028:INFO:_display_container: 2
2024-09-30 12:57:32,028:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-30 12:57:32,028:INFO:create_model() successfully completed......................................
2024-09-30 12:57:32,143:INFO:SubProcess create_model() end ==================================
2024-09-30 12:57:32,143:INFO:Creating metrics dataframe
2024-09-30 12:57:32,151:INFO:Initializing Random Forest Classifier
2024-09-30 12:57:32,151:INFO:Total runtime is 0.26721272071202595 minutes
2024-09-30 12:57:32,154:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:32,154:INFO:Initializing create_model()
2024-09-30 12:57:32,154:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:32,155:INFO:Checking exceptions
2024-09-30 12:57:32,155:INFO:Importing libraries
2024-09-30 12:57:32,155:INFO:Copying training dataset
2024-09-30 12:57:32,342:INFO:Defining folds
2024-09-30 12:57:32,342:INFO:Declaring metric variables
2024-09-30 12:57:32,345:INFO:Importing untrained model
2024-09-30 12:57:32,347:INFO:Random Forest Classifier Imported successfully
2024-09-30 12:57:32,353:INFO:Starting cross validation
2024-09-30 12:57:32,355:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:57:34,678:INFO:Calculating mean and std
2024-09-30 12:57:34,679:INFO:Creating metrics dataframe
2024-09-30 12:57:34,680:INFO:Uploading results into container
2024-09-30 12:57:34,681:INFO:Uploading model into container now
2024-09-30 12:57:34,681:INFO:_master_model_container: 7
2024-09-30 12:57:34,681:INFO:_display_container: 2
2024-09-30 12:57:34,681:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-30 12:57:34,682:INFO:create_model() successfully completed......................................
2024-09-30 12:57:34,806:INFO:SubProcess create_model() end ==================================
2024-09-30 12:57:34,807:INFO:Creating metrics dataframe
2024-09-30 12:57:34,814:INFO:Initializing Quadratic Discriminant Analysis
2024-09-30 12:57:34,814:INFO:Total runtime is 0.31159324645996095 minutes
2024-09-30 12:57:34,817:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:34,817:INFO:Initializing create_model()
2024-09-30 12:57:34,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:34,817:INFO:Checking exceptions
2024-09-30 12:57:34,817:INFO:Importing libraries
2024-09-30 12:57:34,817:INFO:Copying training dataset
2024-09-30 12:57:34,998:INFO:Defining folds
2024-09-30 12:57:34,998:INFO:Declaring metric variables
2024-09-30 12:57:35,000:INFO:Importing untrained model
2024-09-30 12:57:35,003:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-30 12:57:35,009:INFO:Starting cross validation
2024-09-30 12:57:35,012:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:57:35,529:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:35,589:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:35,620:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:35,655:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:35,832:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:35,924:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:36,024:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:36,119:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:36,499:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:36,576:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:36,583:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-30 12:57:36,641:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:36,657:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:36,679:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:36,841:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:36,898:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:36,962:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:36,978:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:37,160:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:37,175:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:37,191:INFO:Calculating mean and std
2024-09-30 12:57:37,192:INFO:Creating metrics dataframe
2024-09-30 12:57:37,193:INFO:Uploading results into container
2024-09-30 12:57:37,194:INFO:Uploading model into container now
2024-09-30 12:57:37,194:INFO:_master_model_container: 8
2024-09-30 12:57:37,194:INFO:_display_container: 2
2024-09-30 12:57:37,194:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-30 12:57:37,194:INFO:create_model() successfully completed......................................
2024-09-30 12:57:37,327:INFO:SubProcess create_model() end ==================================
2024-09-30 12:57:37,327:INFO:Creating metrics dataframe
2024-09-30 12:57:37,334:INFO:Initializing Ada Boost Classifier
2024-09-30 12:57:37,334:INFO:Total runtime is 0.35359419584274293 minutes
2024-09-30 12:57:37,336:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:37,337:INFO:Initializing create_model()
2024-09-30 12:57:37,337:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:37,337:INFO:Checking exceptions
2024-09-30 12:57:37,337:INFO:Importing libraries
2024-09-30 12:57:37,337:INFO:Copying training dataset
2024-09-30 12:57:37,499:INFO:Defining folds
2024-09-30 12:57:37,499:INFO:Declaring metric variables
2024-09-30 12:57:37,502:INFO:Importing untrained model
2024-09-30 12:57:37,505:INFO:Ada Boost Classifier Imported successfully
2024-09-30 12:57:37,511:INFO:Starting cross validation
2024-09-30 12:57:37,514:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:57:37,768:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:37,914:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:37,954:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:37,995:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:38,133:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:38,182:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:38,250:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:38,338:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:38,362:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:38,446:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-30 12:57:40,093:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,151:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,215:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,258:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,436:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,454:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,465:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,499:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,586:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,601:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:57:40,612:INFO:Calculating mean and std
2024-09-30 12:57:40,613:INFO:Creating metrics dataframe
2024-09-30 12:57:40,615:INFO:Uploading results into container
2024-09-30 12:57:40,615:INFO:Uploading model into container now
2024-09-30 12:57:40,615:INFO:_master_model_container: 9
2024-09-30 12:57:40,615:INFO:_display_container: 2
2024-09-30 12:57:40,616:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-30 12:57:40,616:INFO:create_model() successfully completed......................................
2024-09-30 12:57:40,741:INFO:SubProcess create_model() end ==================================
2024-09-30 12:57:40,741:INFO:Creating metrics dataframe
2024-09-30 12:57:40,749:INFO:Initializing Gradient Boosting Classifier
2024-09-30 12:57:40,749:INFO:Total runtime is 0.41051042874654137 minutes
2024-09-30 12:57:40,752:INFO:SubProcess create_model() called ==================================
2024-09-30 12:57:40,753:INFO:Initializing create_model()
2024-09-30 12:57:40,753:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:57:40,753:INFO:Checking exceptions
2024-09-30 12:57:40,753:INFO:Importing libraries
2024-09-30 12:57:40,753:INFO:Copying training dataset
2024-09-30 12:57:40,925:INFO:Defining folds
2024-09-30 12:57:40,926:INFO:Declaring metric variables
2024-09-30 12:57:40,928:INFO:Importing untrained model
2024-09-30 12:57:40,931:INFO:Gradient Boosting Classifier Imported successfully
2024-09-30 12:57:40,937:INFO:Starting cross validation
2024-09-30 12:57:40,939:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:58:18,801:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,017:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,187:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,208:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,240:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,327:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,339:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,382:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,529:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,585:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:19,597:INFO:Calculating mean and std
2024-09-30 12:58:19,598:INFO:Creating metrics dataframe
2024-09-30 12:58:19,599:INFO:Uploading results into container
2024-09-30 12:58:19,600:INFO:Uploading model into container now
2024-09-30 12:58:19,600:INFO:_master_model_container: 10
2024-09-30 12:58:19,600:INFO:_display_container: 2
2024-09-30 12:58:19,600:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-30 12:58:19,600:INFO:create_model() successfully completed......................................
2024-09-30 12:58:19,716:INFO:SubProcess create_model() end ==================================
2024-09-30 12:58:19,716:INFO:Creating metrics dataframe
2024-09-30 12:58:19,723:INFO:Initializing Linear Discriminant Analysis
2024-09-30 12:58:19,723:INFO:Total runtime is 1.060087780157725 minutes
2024-09-30 12:58:19,726:INFO:SubProcess create_model() called ==================================
2024-09-30 12:58:19,726:INFO:Initializing create_model()
2024-09-30 12:58:19,726:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:58:19,726:INFO:Checking exceptions
2024-09-30 12:58:19,726:INFO:Importing libraries
2024-09-30 12:58:19,727:INFO:Copying training dataset
2024-09-30 12:58:19,892:INFO:Defining folds
2024-09-30 12:58:19,893:INFO:Declaring metric variables
2024-09-30 12:58:19,895:INFO:Importing untrained model
2024-09-30 12:58:19,898:INFO:Linear Discriminant Analysis Imported successfully
2024-09-30 12:58:19,903:INFO:Starting cross validation
2024-09-30 12:58:19,905:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:58:21,190:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,354:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,563:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,649:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,708:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,746:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,809:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,914:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,947:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,957:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-30 12:58:21,968:INFO:Calculating mean and std
2024-09-30 12:58:21,969:INFO:Creating metrics dataframe
2024-09-30 12:58:21,970:INFO:Uploading results into container
2024-09-30 12:58:21,970:INFO:Uploading model into container now
2024-09-30 12:58:21,971:INFO:_master_model_container: 11
2024-09-30 12:58:21,971:INFO:_display_container: 2
2024-09-30 12:58:21,971:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-30 12:58:21,971:INFO:create_model() successfully completed......................................
2024-09-30 12:58:22,087:INFO:SubProcess create_model() end ==================================
2024-09-30 12:58:22,087:INFO:Creating metrics dataframe
2024-09-30 12:58:22,095:INFO:Initializing Extra Trees Classifier
2024-09-30 12:58:22,095:INFO:Total runtime is 1.0996132969856263 minutes
2024-09-30 12:58:22,098:INFO:SubProcess create_model() called ==================================
2024-09-30 12:58:22,098:INFO:Initializing create_model()
2024-09-30 12:58:22,098:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:58:22,098:INFO:Checking exceptions
2024-09-30 12:58:22,098:INFO:Importing libraries
2024-09-30 12:58:22,099:INFO:Copying training dataset
2024-09-30 12:58:22,268:INFO:Defining folds
2024-09-30 12:58:22,268:INFO:Declaring metric variables
2024-09-30 12:58:22,271:INFO:Importing untrained model
2024-09-30 12:58:22,274:INFO:Extra Trees Classifier Imported successfully
2024-09-30 12:58:22,279:INFO:Starting cross validation
2024-09-30 12:58:22,281:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:58:24,708:INFO:Calculating mean and std
2024-09-30 12:58:24,709:INFO:Creating metrics dataframe
2024-09-30 12:58:24,711:INFO:Uploading results into container
2024-09-30 12:58:24,711:INFO:Uploading model into container now
2024-09-30 12:58:24,711:INFO:_master_model_container: 12
2024-09-30 12:58:24,711:INFO:_display_container: 2
2024-09-30 12:58:24,712:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-30 12:58:24,712:INFO:create_model() successfully completed......................................
2024-09-30 12:58:24,830:INFO:SubProcess create_model() end ==================================
2024-09-30 12:58:24,830:INFO:Creating metrics dataframe
2024-09-30 12:58:24,837:INFO:Initializing Light Gradient Boosting Machine
2024-09-30 12:58:24,838:INFO:Total runtime is 1.1453335404396057 minutes
2024-09-30 12:58:24,840:INFO:SubProcess create_model() called ==================================
2024-09-30 12:58:24,840:INFO:Initializing create_model()
2024-09-30 12:58:24,840:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:58:24,840:INFO:Checking exceptions
2024-09-30 12:58:24,840:INFO:Importing libraries
2024-09-30 12:58:24,841:INFO:Copying training dataset
2024-09-30 12:58:25,003:INFO:Defining folds
2024-09-30 12:58:25,003:INFO:Declaring metric variables
2024-09-30 12:58:25,006:INFO:Importing untrained model
2024-09-30 12:58:25,009:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-30 12:58:25,014:INFO:Starting cross validation
2024-09-30 12:58:25,017:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 12:58:47,021:INFO:Calculating mean and std
2024-09-30 12:58:47,022:INFO:Creating metrics dataframe
2024-09-30 12:58:47,025:INFO:Uploading results into container
2024-09-30 12:58:47,025:INFO:Uploading model into container now
2024-09-30 12:58:47,026:INFO:_master_model_container: 13
2024-09-30 12:58:47,026:INFO:_display_container: 2
2024-09-30 12:58:47,026:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-30 12:58:47,027:INFO:create_model() successfully completed......................................
2024-09-30 12:58:47,204:INFO:SubProcess create_model() end ==================================
2024-09-30 12:58:47,204:INFO:Creating metrics dataframe
2024-09-30 12:58:47,213:INFO:Initializing CatBoost Classifier
2024-09-30 12:58:47,213:INFO:Total runtime is 1.5182478427886963 minutes
2024-09-30 12:58:47,216:INFO:SubProcess create_model() called ==================================
2024-09-30 12:58:47,216:INFO:Initializing create_model()
2024-09-30 12:58:47,216:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 12:58:47,216:INFO:Checking exceptions
2024-09-30 12:58:47,216:INFO:Importing libraries
2024-09-30 12:58:47,216:INFO:Copying training dataset
2024-09-30 12:58:47,396:INFO:Defining folds
2024-09-30 12:58:47,396:INFO:Declaring metric variables
2024-09-30 12:58:47,399:INFO:Importing untrained model
2024-09-30 12:58:47,401:INFO:CatBoost Classifier Imported successfully
2024-09-30 12:58:47,406:INFO:Starting cross validation
2024-09-30 12:58:47,409:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 13:14:27,977:INFO:Calculating mean and std
2024-09-30 13:14:27,980:INFO:Creating metrics dataframe
2024-09-30 13:14:27,985:INFO:Uploading results into container
2024-09-30 13:14:27,986:INFO:Uploading model into container now
2024-09-30 13:14:27,987:INFO:_master_model_container: 14
2024-09-30 13:14:27,987:INFO:_display_container: 2
2024-09-30 13:14:27,987:INFO:<catboost.core.CatBoostClassifier object at 0x00000210DE485DD0>
2024-09-30 13:14:27,987:INFO:create_model() successfully completed......................................
2024-09-30 13:14:28,214:INFO:SubProcess create_model() end ==================================
2024-09-30 13:14:28,215:INFO:Creating metrics dataframe
2024-09-30 13:14:28,227:INFO:Initializing Dummy Classifier
2024-09-30 13:14:28,228:INFO:Total runtime is 17.20184150139491 minutes
2024-09-30 13:14:28,231:INFO:SubProcess create_model() called ==================================
2024-09-30 13:14:28,231:INFO:Initializing create_model()
2024-09-30 13:14:28,231:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000210E0CCDB50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 13:14:28,231:INFO:Checking exceptions
2024-09-30 13:14:28,232:INFO:Importing libraries
2024-09-30 13:14:28,232:INFO:Copying training dataset
2024-09-30 13:14:28,478:INFO:Defining folds
2024-09-30 13:14:28,479:INFO:Declaring metric variables
2024-09-30 13:14:28,481:INFO:Importing untrained model
2024-09-30 13:14:28,485:INFO:Dummy Classifier Imported successfully
2024-09-30 13:14:28,491:INFO:Starting cross validation
2024-09-30 13:14:28,494:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-30 13:14:28,874:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:28,961:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:29,035:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:29,092:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:29,219:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:29,280:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:29,296:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:29,325:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:29,387:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:29,429:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-30 13:14:29,437:INFO:Calculating mean and std
2024-09-30 13:14:29,438:INFO:Creating metrics dataframe
2024-09-30 13:14:29,439:INFO:Uploading results into container
2024-09-30 13:14:29,440:INFO:Uploading model into container now
2024-09-30 13:14:29,440:INFO:_master_model_container: 15
2024-09-30 13:14:29,440:INFO:_display_container: 2
2024-09-30 13:14:29,440:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-30 13:14:29,441:INFO:create_model() successfully completed......................................
2024-09-30 13:14:29,561:INFO:SubProcess create_model() end ==================================
2024-09-30 13:14:29,562:INFO:Creating metrics dataframe
2024-09-30 13:14:29,584:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-09-30 13:14:29,591:INFO:Initializing create_model()
2024-09-30 13:14:29,591:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 13:14:29,591:INFO:Checking exceptions
2024-09-30 13:14:29,593:INFO:Importing libraries
2024-09-30 13:14:29,593:INFO:Copying training dataset
2024-09-30 13:14:29,776:INFO:Defining folds
2024-09-30 13:14:29,776:INFO:Declaring metric variables
2024-09-30 13:14:29,776:INFO:Importing untrained model
2024-09-30 13:14:29,776:INFO:Declaring custom model
2024-09-30 13:14:29,777:INFO:Extra Trees Classifier Imported successfully
2024-09-30 13:14:29,779:INFO:Cross validation set to False
2024-09-30 13:14:29,779:INFO:Fitting Model
2024-09-30 13:14:30,251:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-30 13:14:30,251:INFO:create_model() successfully completed......................................
2024-09-30 13:14:30,375:INFO:Creating Dashboard logs
2024-09-30 13:14:30,379:INFO:Model: Extra Trees Classifier
2024-09-30 13:14:30,425:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-09-30 13:14:30,587:INFO:Initializing predict_model()
2024-09-30 13:14:30,587:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000210E0CB6DE0>)
2024-09-30 13:14:30,587:INFO:Checking exceptions
2024-09-30 13:14:30,587:INFO:Preloading libraries
2024-09-30 13:14:31,549:INFO:SubProcess plot_model() called ==================================
2024-09-30 13:14:31,549:INFO:Initializing plot_model()
2024-09-30 13:14:31,549:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), plot=auc, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmpovkpja1o, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 13:14:31,549:INFO:Checking exceptions
2024-09-30 13:14:31,636:INFO:Preloading libraries
2024-09-30 13:14:31,671:INFO:Copying training dataset
2024-09-30 13:14:31,671:INFO:Plot type: auc
2024-09-30 13:14:32,899:INFO:Fitting Model
2024-09-30 13:14:32,900:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-30 13:14:32,900:INFO:Scoring test/hold-out set
2024-09-30 13:14:32,994:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmpovkpja1o\AUC.png'
2024-09-30 13:14:33,237:INFO:Visual Rendered Successfully
2024-09-30 13:14:33,348:INFO:plot_model() successfully completed......................................
2024-09-30 13:14:33,367:INFO:Initializing plot_model()
2024-09-30 13:14:33,367:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), plot=confusion_matrix, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmpovkpja1o, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 13:14:33,367:INFO:Checking exceptions
2024-09-30 13:14:33,444:INFO:Preloading libraries
2024-09-30 13:14:33,466:INFO:Copying training dataset
2024-09-30 13:14:33,466:INFO:Plot type: confusion_matrix
2024-09-30 13:14:34,651:INFO:Fitting Model
2024-09-30 13:14:34,651:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-30 13:14:34,652:INFO:Scoring test/hold-out set
2024-09-30 13:14:34,851:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmpovkpja1o\Confusion Matrix.png'
2024-09-30 13:14:35,005:INFO:Visual Rendered Successfully
2024-09-30 13:14:35,117:INFO:plot_model() successfully completed......................................
2024-09-30 13:14:35,129:INFO:Initializing plot_model()
2024-09-30 13:14:35,129:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), plot=feature, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmpovkpja1o, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 13:14:35,129:INFO:Checking exceptions
2024-09-30 13:14:35,229:INFO:Preloading libraries
2024-09-30 13:14:35,246:INFO:Copying training dataset
2024-09-30 13:14:35,246:INFO:Plot type: feature
2024-09-30 13:14:35,247:WARNING:No coef_ found. Trying feature_importances_
2024-09-30 13:14:35,569:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmpovkpja1o\Feature Importance.png'
2024-09-30 13:14:35,696:INFO:Visual Rendered Successfully
2024-09-30 13:14:35,808:INFO:plot_model() successfully completed......................................
2024-09-30 13:14:35,817:INFO:SubProcess plot_model() end ==================================
2024-09-30 13:14:36,026:INFO:Creating Dashboard logs
2024-09-30 13:14:36,029:INFO:Model: Ridge Classifier
2024-09-30 13:14:36,075:INFO:Logged params: {'alpha': 1.0, 'class_weight': None, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2024-09-30 13:14:36,345:INFO:Creating Dashboard logs
2024-09-30 13:14:36,347:INFO:Model: SVM - Linear Kernel
2024-09-30 13:14:36,389:INFO:Logged params: {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.001, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'power_t': 0.5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-09-30 13:14:36,732:INFO:Creating Dashboard logs
2024-09-30 13:14:36,735:INFO:Model: Logistic Regression
2024-09-30 13:14:36,776:INFO:Logged params: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 123, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}
2024-09-30 13:14:37,051:INFO:Creating Dashboard logs
2024-09-30 13:14:37,053:INFO:Model: Light Gradient Boosting Machine
2024-09-30 13:14:37,102:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-09-30 13:14:37,369:INFO:Creating Dashboard logs
2024-09-30 13:14:37,372:INFO:Model: CatBoost Classifier
2024-09-30 13:14:37,414:WARNING:Couldn't get params for model. Exception:
Traceback (most recent call last):
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\pycaret\loggers\dashboard_logger.py", line 78, in log_model
    params = params.get_all_params()
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\catboost\core.py", line 3504, in get_all_params
    raise CatBoostError("There is no trained model to use get_all_params(). Use fit() to train model. Then use this method.")
_catboost.CatBoostError: There is no trained model to use get_all_params(). Use fit() to train model. Then use this method.

2024-09-30 13:14:37,414:INFO:Logged params: {}
2024-09-30 13:14:37,661:INFO:Creating Dashboard logs
2024-09-30 13:14:37,664:INFO:Model: Random Forest Classifier
2024-09-30 13:14:37,707:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-09-30 13:14:37,974:INFO:Creating Dashboard logs
2024-09-30 13:14:37,977:INFO:Model: Gradient Boosting Classifier
2024-09-30 13:14:38,015:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-09-30 13:14:38,296:INFO:Creating Dashboard logs
2024-09-30 13:14:38,299:INFO:Model: Linear Discriminant Analysis
2024-09-30 13:14:38,341:INFO:Logged params: {'covariance_estimator': None, 'n_components': None, 'priors': None, 'shrinkage': None, 'solver': 'svd', 'store_covariance': False, 'tol': 0.0001}
2024-09-30 13:14:38,629:INFO:Creating Dashboard logs
2024-09-30 13:14:38,631:INFO:Model: Naive Bayes
2024-09-30 13:14:38,668:INFO:Logged params: {'priors': None, 'var_smoothing': 1e-09}
2024-09-30 13:14:38,916:INFO:Creating Dashboard logs
2024-09-30 13:14:38,919:INFO:Model: K Neighbors Classifier
2024-09-30 13:14:38,961:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2024-09-30 13:14:39,260:INFO:Creating Dashboard logs
2024-09-30 13:14:39,262:INFO:Model: Decision Tree Classifier
2024-09-30 13:14:39,306:INFO:Logged params: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 123, 'splitter': 'best'}
2024-09-30 13:14:39,565:INFO:Creating Dashboard logs
2024-09-30 13:14:39,568:INFO:Model: Ada Boost Classifier
2024-09-30 13:14:39,604:INFO:Logged params: {'algorithm': 'SAMME.R', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 50, 'random_state': 123}
2024-09-30 13:14:39,866:INFO:Creating Dashboard logs
2024-09-30 13:14:39,868:INFO:Model: Quadratic Discriminant Analysis
2024-09-30 13:14:39,915:INFO:Logged params: {'priors': None, 'reg_param': 0.0, 'store_covariance': False, 'tol': 0.0001}
2024-09-30 13:14:40,181:INFO:Creating Dashboard logs
2024-09-30 13:14:40,184:INFO:Model: Dummy Classifier
2024-09-30 13:14:40,235:INFO:Logged params: {'constant': None, 'random_state': 123, 'strategy': 'prior'}
2024-09-30 13:14:40,504:INFO:_master_model_container: 15
2024-09-30 13:14:40,504:INFO:_display_container: 2
2024-09-30 13:14:40,504:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-30 13:14:40,504:INFO:compare_models() successfully completed......................................
2024-09-30 13:14:40,858:INFO:Initializing finalize_model()
2024-09-30 13:14:40,858:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-30 13:14:40,858:INFO:Finalizing ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-30 13:14:41,012:INFO:Initializing create_model()
2024-09-30 13:14:41,012:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-30 13:14:41,012:INFO:Checking exceptions
2024-09-30 13:14:41,013:INFO:Importing libraries
2024-09-30 13:14:41,014:INFO:Copying training dataset
2024-09-30 13:14:41,030:INFO:Defining folds
2024-09-30 13:14:41,030:INFO:Declaring metric variables
2024-09-30 13:14:41,030:INFO:Importing untrained model
2024-09-30 13:14:41,030:INFO:Declaring custom model
2024-09-30 13:14:41,031:INFO:Extra Trees Classifier Imported successfully
2024-09-30 13:14:41,033:INFO:Cross validation set to False
2024-09-30 13:14:41,033:INFO:Fitting Model
2024-09-30 13:14:41,432:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-30 13:14:41,432:INFO:create_model() successfully completed......................................
2024-09-30 13:14:41,553:INFO:Creating Dashboard logs
2024-09-30 13:14:41,553:INFO:Model: Extra Trees Classifier
2024-09-30 13:14:41,592:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-09-30 13:14:41,662:INFO:SubProcess plot_model() called ==================================
2024-09-30 13:14:41,670:INFO:Initializing plot_model()
2024-09-30 13:14:41,670:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=auc, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmp93yg87x9, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 13:14:41,670:INFO:Checking exceptions
2024-09-30 13:14:41,748:INFO:Preloading libraries
2024-09-30 13:14:41,773:INFO:Copying training dataset
2024-09-30 13:14:41,773:INFO:Plot type: auc
2024-09-30 13:14:43,014:INFO:Fitting Model
2024-09-30 13:14:43,015:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-30 13:14:43,015:INFO:Scoring test/hold-out set
2024-09-30 13:14:43,095:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmp93yg87x9\AUC.png'
2024-09-30 13:14:43,292:INFO:Visual Rendered Successfully
2024-09-30 13:14:43,409:INFO:plot_model() successfully completed......................................
2024-09-30 13:14:43,451:INFO:Initializing plot_model()
2024-09-30 13:14:43,451:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=confusion_matrix, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmp93yg87x9, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 13:14:43,451:INFO:Checking exceptions
2024-09-30 13:14:43,538:INFO:Preloading libraries
2024-09-30 13:14:43,559:INFO:Copying training dataset
2024-09-30 13:14:43,559:INFO:Plot type: confusion_matrix
2024-09-30 13:14:44,758:INFO:Fitting Model
2024-09-30 13:14:44,759:WARNING:C:\Users\yuriy\PycharmProjects\dmlm_task1\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names
  warnings.warn(

2024-09-30 13:14:44,759:INFO:Scoring test/hold-out set
2024-09-30 13:14:44,842:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmp93yg87x9\Confusion Matrix.png'
2024-09-30 13:14:44,963:INFO:Visual Rendered Successfully
2024-09-30 13:14:45,080:INFO:plot_model() successfully completed......................................
2024-09-30 13:14:45,097:INFO:Initializing plot_model()
2024-09-30 13:14:45,097:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), plot=feature, scale=1, save=C:\Users\yuriy\AppData\Local\Temp\tmp93yg87x9, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=False, display=None, display_format=None)
2024-09-30 13:14:45,097:INFO:Checking exceptions
2024-09-30 13:14:45,197:INFO:Preloading libraries
2024-09-30 13:14:45,217:INFO:Copying training dataset
2024-09-30 13:14:45,218:INFO:Plot type: feature
2024-09-30 13:14:45,218:WARNING:No coef_ found. Trying feature_importances_
2024-09-30 13:14:45,526:INFO:Saving 'C:\Users\yuriy\AppData\Local\Temp\tmp93yg87x9\Feature Importance.png'
2024-09-30 13:14:45,649:INFO:Visual Rendered Successfully
2024-09-30 13:14:45,761:INFO:plot_model() successfully completed......................................
2024-09-30 13:14:45,771:INFO:SubProcess plot_model() end ==================================
2024-09-30 13:14:45,961:INFO:_master_model_container: 15
2024-09-30 13:14:45,961:INFO:_display_container: 2
2024-09-30 13:14:45,968:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False)
2024-09-30 13:14:45,968:INFO:finalize_model() successfully completed......................................
2024-09-30 13:14:46,190:INFO:Initializing predict_model()
2024-09-30 13:14:46,190:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210E118AF50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['0', '1', '2', '3', '4', '5', '6',
                                             '7', '8', '9', '10', '11', '12',
                                             '13', '14', '15', '16', '17', '18',
                                             '19', '20', '21', '22', '23', '24',
                                             '25', '26', '27', '28', '29', ...],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,...
                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                                      class_weight=None, criterion='gini',
                                      max_depth=None, max_features='sqrt',
                                      max_leaf_nodes=None, max_samples=None,
                                      min_impurity_decrease=0.0,
                                      min_samples_leaf=1, min_samples_split=2,
                                      min_weight_fraction_leaf=0.0,
                                      monotonic_cst=None, n_estimators=100,
                                      n_jobs=-1, oob_score=False,
                                      random_state=123, verbose=0,
                                      warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000210E0A16C00>)
2024-09-30 13:14:46,191:INFO:Checking exceptions
2024-09-30 13:14:46,191:INFO:Preloading libraries
2024-09-30 13:14:46,193:INFO:Set up data.
2024-09-30 13:14:46,354:INFO:Set up index.
